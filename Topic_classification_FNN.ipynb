{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [COM6513] Assignment: Topic Classification with a Feedforward Network\n",
    "\n",
    "\n",
    "### Instructor: Nikos Aletras\n",
    "\n",
    "\n",
    "The goal of this assignment is to develop a Feedforward neural network for topic classification. \n",
    "\n",
    "\n",
    "\n",
    "For that purpose, you will implement:\n",
    "\n",
    "- Text processing methods for transforming raw text data into input vectors for your network  (**1 mark**)\n",
    "\n",
    "\n",
    "- A Feedforward network consisting of:\n",
    "    - **One-hot** input layer mapping words into an **Embedding weight matrix** (**1 mark**)\n",
    "    - **One hidden layer** computing the mean embedding vector of all words in input followed by a **ReLU activation function** (**1 mark**)\n",
    "    - **Output layer** with a **softmax** activation. (**1 mark**)\n",
    "\n",
    "\n",
    "- The Stochastic Gradient Descent (SGD) algorithm with **back-propagation** to learn the weights of your Neural network. Your algorithm should:\n",
    "    - Use (and minimise) the **Categorical Cross-entropy loss** function (**1 mark**)\n",
    "    - Perform a **Forward pass** to compute intermediate outputs (**2 marks**)\n",
    "    - Perform a **Backward pass** to compute gradients and update all sets of weights (**3 marks**)\n",
    "    - Implement and use **Dropout** after each hidden layer for regularisation (**1 marks**)\n",
    "\n",
    "\n",
    "\n",
    "- Discuss how did you choose hyperparameters? You can tune the learning rate (hint: choose small values), embedding size {e.g. 50, 300, 500} and the dropout rate {e.g. 0.2, 0.5}. Please use tables or graphs to show training and validation performance for each hyperparameter combination  (**2 marks**). \n",
    "\n",
    "\n",
    "\n",
    "- After training a model, plot the learning process (i.e. training and validation loss in each epoch) using a line plot and report accuracy. Does your model overfit, underfit or is about right? (**1 mark**).\n",
    "\n",
    "\n",
    "\n",
    "- Re-train your network by using pre-trained embeddings ([GloVe](https://nlp.stanford.edu/projects/glove/)) trained on large corpora. Instead of randomly initialising the embedding weights matrix, you should initialise it with the pre-trained weights. During training, you should not update them (i.e. weight freezing) and backprop should stop before computing gradients for updating embedding weights. Report results by performing hyperparameter tuning and plotting the learning process. Do you get better performance? (**1 marks**).\n",
    "\n",
    "\n",
    "\n",
    "- Extend you Feedforward network by adding more hidden layers (e.g. one more or two). How does it affect the performance? Note: You need to repeat hyperparameter tuning, but the number of combinations grows exponentially. Therefore, you need to choose a subset of all possible combinations (**3 marks**)\n",
    "\n",
    "\n",
    "- Provide well documented and commented code describing all of your choices. In general, you are free to make decisions about text processing (e.g. punctuation, numbers, vocabulary size) and hyperparameter values. We expect to see justifications and discussion for all of your choices. You must provide detailed explanations of your implementation, provide a detailed analysis of the results (e.g. why a model performs better than other models etc.) including error analyses (e.g. examples and discussion/analysis of missclasifications etc.)  (**10 marks**). \n",
    "\n",
    "\n",
    "\n",
    "- Provide efficient solutions by using Numpy arrays when possible. Executing the whole notebook with your code should not take more than 10 minutes on any standard computer (e.g. Intel Core i5 CPU, 8 or 16GB RAM) excluding hyperparameter tuning runs and loading the pretrained vectors. You can find tips in Lab 1 (**2 marks**). \n",
    "\n",
    "\n",
    "\n",
    "### Data \n",
    "\n",
    "The data you will use for the task is a subset of the [AG News Corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) and you can find it in the `./data_topic` folder in CSV format:\n",
    "\n",
    "- `data_topic/train.csv`: contains 2,400 news articles, 800 for each class to be used for training.\n",
    "- `data_topic/dev.csv`: contains 150 news articles, 50 for each class to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_topic/test.csv`: contains 900 news articles, 300 for each class to be used for testing.\n",
    "\n",
    "Class 1: Politics, Class 2: Sports, Class 3: Economy\n",
    "\n",
    "### Pre-trained Embeddings\n",
    "\n",
    "You can download pre-trained GloVe embeddings trained on Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download) from [here](http://nlp.stanford.edu/data/glove.840B.300d.zip). No need to unzip, the file is large.\n",
    "\n",
    "### Save Memory\n",
    "\n",
    "To save RAM, when you finish each experiment you can delete the weights of your network using `del W` followed by Python's garbage collector `gc.collect()`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "\n",
    "You **must** submit a Jupyter Notebook file (assignment_yourusername.ipynb) and an exported PDF version (you can do it from Jupyter: `File->Download as->PDF via Latex`, you need to have a Latex distribution installed e.g. MikTex or MacTex and pandoc). If you are unable to export the pdf via Latex, you can print the notebook web page to a pdf file from your browser (e.g. on Firefox: File->Print->Save to PDF).\n",
    "\n",
    "\n",
    "You are advised to follow the code structure given in this notebook by completing all given funtions. You can also write any auxilliary/helper functions (and arguments for the functions) that you might need but note that you can provide a full solution without any such functions. Similarly, you can just use only the packages imported below but you are free to use any functionality from the [Python Standard Library](https://docs.python.org/3/library/index.html), NumPy, SciPy (excluding built-in softmax funtcions) and Pandas. You are **not allowed to use any third-party library** such as Scikit-learn (apart from metric functions already provided), NLTK, Spacy, Keras, Pytorch etc.. You should mention if you've used Windows to write and test your code because we mostly use Unix based machines for marking (e.g. Ubuntu, MacOS). \n",
    "\n",
    "There is no single correct answer on what your accuracy should be, but correct implementations usually achieve F1-scores around 80\\% or higher. The quality of the analysis of the results and discussion is as important as the implementation and accuracy of your models. Please be brief and consice in your discussion and analyses. \n",
    "\n",
    "This assignment will be marked out of 30. It is worth 30\\% of your final grade in the module.\n",
    "\n",
    "The deadline for this assignment is **23:59 on Mon, 26 Apr 2023** and it needs to be submitted via Blackboard. Standard departmental penalties for lateness will be applied. We use a range of strategies to **detect [unfair means](https://www.sheffield.ac.uk/ssid/unfair-means/index)**, including Turnitin which helps detect plagiarism. Use of unfair means would result in getting a failing grade.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:00:18.625532Z",
     "start_time": "2020-04-02T15:00:17.377733Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "from time import localtime, strftime\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "import zipfile\n",
    "import gc\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Raw texts into training and development data\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:39.748484Z",
     "start_time": "2020-04-02T14:26:39.727404Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read Training Data and store it in a dataframe \n",
    "train_df = pd.read_csv('/Users/abolivi/Masters_Doc_Sem2/NLP_Lab/assignment/data_topic/train.csv',\n",
    "                       names = ['Class','News_Articles'])\n",
    "x_raw = train_df['News_Articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:39.753874Z",
     "start_time": "2020-04-02T14:26:39.749647Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read Development Data and store it in a dataframe \n",
    "dev_df = pd.read_csv('/Users/abolivi/Masters_Doc_Sem2/NLP_Lab/assignment/data_topic/dev.csv',\n",
    "                     names = ['Class','News_Articles'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Test Data and store it in a dataframe \n",
    "test_df = pd.read_csv('/Users/abolivi/Masters_Doc_Sem2/NLP_Lab/assignment/data_topic/test.csv',\n",
    "                      names = ['Class','News_Articles'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input representations\n",
    "\n",
    "\n",
    "To train your Feedforward network, you first need to obtain input representations given a vocabulary. One-hot encoding requires large memory capacity. Therefore, we will instead represent documents as lists of vocabulary indices (each word corresponds to a vocabulary index). \n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of words. You should: \n",
    "- tokenise all texts into a list of unigrams (tip: you can re-use the functions from Assignment 1) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- remove unigrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of the top-N most frequent unigrams in the entire corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:40.851926Z",
     "start_time": "2020-04-02T14:26:40.847500Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i', 'if',\n",
    "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what',\n",
    "              'but', 'not', 'there', 'no', 'does', 'not', 'so', 've', 'their',\n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- a list of all extracted features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', \n",
    "               stop_words=[], vocab=set()):\n",
    "# Uses re pattern to extract matching substrings from a given string, and \n",
    "# returns a list of those substrings\n",
    "    tokens = re.findall(token_pattern, x_raw.lower())\n",
    "\n",
    "# Removes the stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Creates ngrams using a loop and store them in the the list\n",
    "    ngrams = []\n",
    "    for n in range(ngram_range[0], ngram_range[1]+1):\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            ngram = ' '.join(tokens[i:i+n])\n",
    "            if not vocab or ngram in vocab:\n",
    "                ngrams.append(ngram)\n",
    "\n",
    "\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reuters', 'venezuelans', 'turned', 'out', 'early', 'large', 'numbers', 'sunday', 'vote', 'historic', 'referendum', 'either', 'remove', 'left', 'wing', 'president', 'hugo', 'chavez', 'office', 'give', 'him', 'new', 'mandate', 'govern', 'next', 'two', 'years']\n"
     ]
    }
   ],
   "source": [
    "ngrams = extract_ngrams(x_raw[0],ngram_range=(1,1),stop_words=stop_words)\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary of n-grams\n",
    "\n",
    "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:42.563876Z",
     "start_time": "2020-04-02T14:26:42.557967Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', \n",
    "              min_df=0, keep_topN=0, \n",
    "              stop_words=[]):\n",
    "    # Initialized an empty dictionary which will be used to store document frequency of n-grams\n",
    "    df = {}\n",
    "    vocab=set()\n",
    "    ngram_counts=0\n",
    "    #vocab returns all the unique ngrams and df contains document freq of each n-grams across all the articles\n",
    "    for article in X_raw:\n",
    "        ngrams=extract_ngrams(article,ngram_range, token_pattern, stop_words)\n",
    "        ngrams=set(ngrams)\n",
    "        for ngram in ngrams:\n",
    "            vocab.add(ngram)\n",
    "            if ngram in df:\n",
    "                df[ngram]+=1\n",
    "            else:\n",
    "                df[ngram]=1\n",
    "                \n",
    "    #sorts the dictionary by its values in descending order, which keeps the items with a value greater than min_df\n",
    "    df = {k:v for k,v in sorted(df.items(), key=lambda item: item[1], reverse= True) if v > min_df}\n",
    "    #Checks if keep_topN is true and keeps only top keep_topN items from the df dictionary\n",
    "    if keep_topN:\n",
    "        df= dict(list(df.items())[:keep_topN])\n",
    "    \n",
    "    #removes any n-grams from a list vocab that are not present in a dictionary df. \n",
    "    vocab_copy=list(vocab)\n",
    "    for ngram in vocab_copy:\n",
    "        if ngram not in df:\n",
    "            vocab.remove(ngram)\n",
    "            \n",
    "            \n",
    "    return vocab, df, ngram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:43.577997Z",
     "start_time": "2020-04-02T14:26:43.478950Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: \n",
      " ['players', 'collect', 'yasser', 'sardinia', 'returning', 'months', 'rangers', 'lows', 'tests', 'strength', 'already', 'seen', 'sweep', 'missed', 'since', 'hopes', 'target', 'insurers', 'facilities', 'case', 'cost', 'gerhard', 'massacre', 'march', 'yet', 'teammates', 'kobe', 'playing', 'first', 'bush']\n",
      "\n",
      " Document Frequency \n",
      " {'reuters': 631, 'said': 432, 'tuesday': 413, 'wednesday': 344, 'new': 325, 'after': 295, 'ap': 275, 'athens': 245, 'monday': 221, 'first': 210, 'two': 187, 'york': 187, 'over': 186, 'us': 184, 'olympic': 183, 'inc': 169, 'more': 157, 'year': 151, 'oil': 150, 'prices': 149, 'company': 147, 'world': 146, 'than': 140, 'aug': 140, 'about': 133, 'had': 133, 'one': 133, 'united': 133, 'sunday': 130, 'out': 130, 'into': 126, 'against': 124, 'up': 124, 'second': 119, 'last': 117, 'president': 111, 'stocks': 111, 'gold': 110, 'team': 109, 'when': 106, 'three': 105, 'night': 105, 'time': 104, 'yesterday': 104, 'games': 104, 'olympics': 102, 'states': 101, 'greece': 101, 'off': 100, 'iraq': 99, 'washington': 99, 'percent': 99, 'home': 92, 'day': 91, 'google': 91, 'public': 89, 'record': 86, 'week': 85, 'men': 85, 'government': 84, 'win': 81, 'american': 80, 'won': 80, 'years': 79, 'all': 79, 'billion': 79, 'shares': 77, 'city': 76, 'offering': 76, 'officials': 75, 'would': 75, 'today': 75, 'final': 75, 'afp': 74, 'gt': 74, 'people': 73, 'lt': 73, 'medal': 73, 'corp': 72, 'sales': 72, 'country': 71, 'back': 71, 'four': 71, 'high': 68, 'investor': 68, 'com': 68, 'minister': 67, 'reported': 67, 'month': 67, 'initial': 67, 'profit': 67, 'ticker': 66, 'al': 65, 'million': 65, 'top': 64, 'before': 64, 'china': 64, 'him': 63, 'national': 63, 'najaf': 63, 'victory': 63, 'end': 62, 'third': 62, 'target': 62, 'troops': 61, 'michael': 61, 'largest': 61, 'game': 60, 'left': 59, 'group': 59, 'price': 59, 'international': 59, 'http': 59, 'hit': 58, 'eight': 58, 'state': 58, 'expected': 58, 'plans': 58, 'href': 58, 'quickinfo': 58, 'aspx': 58, 'www': 58, 'fullquote': 58, 'most': 57, 'july': 56, 'london': 55, 'season': 55, 'while': 54, 'down': 54, 'least': 53, 'iraqi': 53, 'some': 53, 'may': 53, 'bank': 53, 'quarter': 53, 'john': 52, 'just': 52, 'round': 52, 'rose': 52, 'since': 51, 'between': 51, 'other': 51, 'crude': 51, 'another': 51, 'stock': 51, 'earnings': 51, 'market': 51, 'during': 50, 'run': 50, 'women': 50, 'former': 50, 'quot': 50, 'five': 50, 'prime': 49, 'set': 49, 'higher': 49, 'costs': 49, 'investors': 49, 'police': 48, 'australia': 48, 'half': 48, 'south': 47, 'biggest': 47, 'quarterly': 47, 'court': 46, 'fell': 46, 'chicago': 46, 'san': 46, 'phelps': 45, 'near': 45, 'days': 45, 'financial': 45, 'next': 44, 'hugo': 44, 'made': 44, 'chavez': 43, 'israeli': 43, 'west': 43, 'hurricane': 42, 'because': 42, 'british': 42, 'leader': 42, 'according': 42, 'street': 42, 'business': 42, 'plan': 41, 'earlier': 41, 'news': 41, 'search': 41, 'wall': 41, 'exchange': 41, 'strong': 41, 'federal': 40, 'where': 40, 'paul': 40, 'nearly': 40, 'basketball': 40, 'loss': 40, 'major': 40, 'champion': 40, 'charley': 39, 'cut': 39, 'economy': 39, 'killed': 39, 'shot': 39, 'co': 39, 'announced': 38, 'giant': 38, 'early': 37, 'cleric': 37, 'through': 37, 'only': 37, 'official': 37, 'start': 37, 'report': 37, 'england': 37, 'bloomberg': 37, 'right': 37, 'talks': 36, 'bush': 36, 'mark': 36, 'even': 36, 'commission': 36, 'securities': 36, 'put': 36, 'took': 35, 'lead': 35, 'under': 35, 'freestyle': 35, 'man': 35, 'meter': 35, 'canadian': 35, 'fourth': 35, 'economic': 35, 'region': 34, 'long': 34, 'militia': 34, 'homes': 34, 'medals': 34, 'lost': 34, 'agreed': 34, 'now': 34, 'six': 34, 'based': 34, 'consumer': 34, 'ahead': 34, 'demand': 34, 'research': 34, 'profile': 34, 'share': 34, 'seven': 33, 'service': 33, 'military': 33, 'nation': 33, 'sadr': 33, 'venezuela': 33, 'results': 33, 'fighting': 33, 'peace': 33, 'coach': 33, 'months': 33, 'posted': 33, 'referendum': 32, 'including': 32, 'florida': 32, 'baghdad': 32, 'war': 32, 'murder': 32, 'close': 32, 'players': 32, 'maker': 32, 'western': 31, 'despite': 31, 'says': 31, 'old': 31, 'leading': 31, 'competition': 31, 'part': 31, 'francisco': 31, 'still': 31, 'buy': 31, 'league': 31, 'amid': 30, 'forces': 30, 'army': 30, 'holy': 30, 'press': 30, 'nations': 30, 'beat': 30, 'political': 30, 'department': 30, 'helped': 30, 'data': 30, 'lower': 30, 'per': 30, 'quote': 30, 'race': 29, 'radical': 29, 'go': 29, 'capital': 29, 'trial': 29, 'europe': 29, 'championship': 29, 'inflation': 29, 'trading': 29, 'estimates': 29, 'greek': 29, 'winning': 29, 'vote': 28, 'decision': 28, 'whether': 28, 'history': 28, 'say': 28, 'many': 28, 'past': 28, 'make': 28, 'tournament': 28, 'security': 27, 'thousands': 27, 'north': 27, 'way': 27, 'help': 27, 'reports': 27, 'being': 27, 'play': 27, 'kenteris': 27, 'party': 27, 'making': 27, 'late': 27, 'drop': 27, 'shi': 27, 'much': 27, 'dollar': 27, 'sox': 27, 'here': 26, 'should': 26, 'election': 26, 'authorities': 26, 'held': 26, 'conference': 26, 'internet': 26, 'growth': 26, 'global': 26, 'services': 26, 'ite': 26, 'showed': 26, 'web': 26, 'career': 26, 'israel': 25, 'killing': 25, 'saturday': 25, 'running': 25, 'general': 25, 'behind': 25, 'russian': 25, 'sports': 25, 'industry': 25, 'led': 25, 'missed': 25, 'big': 25, 'summer': 25, 'take': 25, 'sprinters': 25, 'get': 25, 'contract': 25, 'tennis': 25, 'inning': 25, 'retailer': 25, 'foreign': 24, 'darfur': 24, 'key': 24, 'air': 24, 'japan': 24, 'thursday': 24, 'ian': 24, 'recall': 24, 'also': 24, 'august': 24, 'good': 24, 'opposition': 24, 'again': 24, 'around': 24, 'charged': 24, 'open': 24, 'manager': 24, 'double': 24, 'weeks': 24, 'drugs': 24, 'player': 24, 'cup': 24, 'little': 24, 'energy': 24, 'regulators': 24, 'case': 24, 'how': 24, 'engine': 24, 'title': 24, 'network': 24, 'korea': 23, 'pay': 23, 'leaders': 23, 'ago': 23, 'later': 23, 'canada': 23, 'interest': 23, 'los': 23, 'following': 23, 'americans': 23, 'cash': 23, 'pool': 23, 'companies': 23, 'number': 23, 'food': 23, 'beijing': 23, 'gaza': 23, 'firm': 23, 'atlanta': 23, 'began': 22, 'place': 22, 'sudan': 22, 'soldiers': 22, 'shrine': 22, 'thorpe': 22, 'center': 22, 'singh': 22, 'shiite': 22, 'cost': 22, 'committee': 22, 'ever': 22, 'angeles': 22, 'heavy': 22, 'thanou': 22, 'bid': 22, 'britain': 22, 'taking': 22, 'life': 22, 'toronto': 22, 'latest': 22, 'sharply': 22, 'head': 22, 'labor': 22, 'republic': 22, 'net': 22, 'deal': 22, 'slashed': 22, 'hamm': 22, 'store': 22, 'palestinian': 21, 'rebels': 21, 'democratic': 21, 'drug': 21, 'across': 21, 'tokyo': 21, 'afghanistan': 21, 'hours': 21, 'found': 21, 'field': 21, 'white': 21, 'militants': 21, 'such': 21, 'australian': 21, 'morning': 21, 'face': 21, 'ended': 21, 'germany': 21, 'hospital': 21, 'materials': 21, 'xinhuanet': 21, 'roger': 21, 'range': 21, 'boston': 21, 'red': 21, 'nuclear': 20, 'efforts': 20, 'friday': 20, 'due': 20, 'fresh': 20, 'both': 20, 'venezuelan': 20, 'starting': 20, 'relay': 20, 'pressure': 20, 'dropped': 20, 'straight': 20, 'released': 20, 'terror': 20, 'told': 20, 'agreement': 20, 'senior': 20, 'silver': 20, 'andy': 20, 'selling': 20, 'gymnastics': 20, 'office': 19, 'water': 19, 'central': 19, 'militiamen': 19, 'bomb': 19, 'india': 19, 'called': 19, 'saying': 19, 'caracas': 19, 'like': 19, 'money': 19, 'jerusalem': 19, 'accused': 19, 'america': 19, 'burundi': 19, 'defense': 19, 'soaring': 19, 'test': 19, 'kostas': 19, 'break': 19, 'delegation': 19, 'pakistan': 19, 'drove': 19, 'pga': 19, 'jones': 19, 'outlook': 19, 'fears': 19, 'sell': 19, 'injury': 19, 'filed': 19, 'approval': 19, 'fall': 19, 'sharon': 19, 'ariel': 19, 'construction': 19, 'low': 19, 'chief': 19, 'olympia': 19, 'awaited': 19, 'barrel': 19, 'funds': 19, 'halliburton': 19, 'used': 18, 'attack': 18, 'visit': 18, 'kerry': 18, 'release': 18, 'nine': 18, 'rival': 18, 'moqtada': 18, 'asked': 18, 'finally': 18, 'opening': 18, 'match': 18, 'caused': 18, 'rates': 18, 'russia': 18, 'charges': 18, 'try': 18, 'children': 18, 'vijay': 18, 'return': 18, 'operator': 18, 'less': 18, 'miss': 18, 'leave': 18, 'action': 18, 'fifth': 18, 'gains': 18, 'sources': 18, 'event': 18, 'june': 18, 'become': 18, 'individual': 18, 'housing': 18, 'soccer': 18, 'future': 18, 'pulled': 18, 'club': 18, 'philadelphia': 18, 'send': 17, 'presidential': 17, 'agency': 17, 'possible': 17, 'short': 17, 'any': 17, 'keep': 17, 'power': 17, 'france': 17, 'failed': 17, 'calif': 17, 'going': 17, 'hundreds': 17, 'impact': 17, 'katerina': 17, 'rule': 17, 'own': 17, 'same': 17, 'captain': 17, 'texas': 17, 'got': 17, 'claims': 17, 'blue': 17, 'boost': 17, 'suspects': 17, 'showing': 17, 'investment': 17, 'tony': 17, 'highs': 17, 'euro': 17, 'need': 17, 'card': 17, 'golf': 17, 'fund': 17, 'ipo': 17, 'didn': 16, 'injured': 16, 'democracy': 16, 'term': 16, 'continue': 16, 'violence': 16, 'french': 16, 'full': 16, 'came': 16, 'residents': 16, 'then': 16, 'st': 16, 'battle': 16, 'members': 16, 'holding': 16, 'southern': 16, 'worries': 16, 'refugees': 16, 'camp': 16, 'rise': 16, 'begin': 16, 'went': 16, 'trying': 16, 'private': 16, 'became': 16, 'points': 16, 'star': 16, 'chain': 16, 'tax': 16, 'might': 16, 'bill': 16, 'european': 16, 'approved': 16, 'work': 16, 'host': 16, 'continued': 16, 'yukos': 16, 'road': 16, 'investigation': 16, 'runs': 16, 'beating': 16, 'meters': 16, 'innings': 16, 'stadium': 16, 'performance': 16, 'credit': 16, 'baseball': 16, 'depot': 16, 'check': 16, 'death': 15, 'fire': 15, 'edwards': 15, 'avoid': 15, 'meeting': 15, 'toward': 15, 'japanese': 15, 'move': 15, 'pushed': 15, 'fans': 15, 'getting': 15, 'urged': 15, 'reserve': 15, 'congolese': 15, 'weekend': 15, 'likely': 15, 'survived': 15, 'appeared': 15, 'islamic': 15, 'away': 15, 'rights': 15, 'without': 15, 'uprising': 15, 'well': 15, 'appeal': 15, 'workers': 15, 'surged': 15, 'ancient': 15, 'recent': 15, 'executive': 15, 'green': 15, 'eased': 15, 'real': 15, 'phone': 15, 'trade': 15, 'baltimore': 15, 'tests': 15, 'sold': 15, 'williams': 15, 'forecast': 15, 'giants': 15, 'mutual': 15, 'operating': 15, 'amp': 15, 'give': 14, 'better': 14, 'ossetia': 14, 'campaign': 14, 'giving': 14, 'course': 14, 'concern': 14, 'fla': 14, 'reach': 14, 'insurance': 14, 'free': 14, 'qualifying': 14, 'van': 14, 'hour': 14, 'family': 14, 'given': 14, 'rico': 14, 'puerto': 14, 'opened': 14, 'province': 14, 'storm': 14, 'asia': 14, 'increase': 14, 'sent': 14, 'missing': 14, 'fraud': 14, 'car': 14, 'decided': 14, 'rising': 14, 'lowe': 14, 'bronze': 14, 'africa': 14, 'union': 14, 'meet': 14, 'conflict': 14, 'boosted': 14, 'backed': 14, 'pitched': 14, 'gasoline': 14, 'czech': 14, 'federer': 14, 'cardinals': 14, 'italian': 14, 'blair': 14, 'alleged': 14, 'concerns': 14, 'far': 14, 'yet': 14, 'potential': 14, 'cleveland': 14, 'indians': 14, 'doping': 14, 'practice': 14, 'rebounded': 14, 'debt': 14, 'nortel': 14, 'mortgage': 14, 'turned': 13, 'wing': 13, 'sydney': 13, 'swimming': 13, 'georgia': 13, 'un': 13, 'sixth': 13, 'poor': 13, 'punta': 13, 'look': 13, 'gorda': 13, 'terrorism': 13, 'cp': 13, 'intelligence': 13, 'television': 13, 'haven': 13, 'charge': 13, 'republican': 13, 'silvio': 13, 'berlusconi': 13, 'until': 13, 'allen': 13, 'level': 13, 'popular': 13, 're': 13, 'received': 13, 'hard': 13, 'program': 13, 'quest': 13, 'raised': 13, 'foot': 13, 'anticipated': 13, 'growing': 13, 'dallas': 13, 'association': 13, 'nfl': 13, 'businesses': 13, 'media': 13, 'sharp': 13, 'markets': 13, 'twins': 13, 'withdrew': 13, 'losing': 13, 'jump': 13, 'defensive': 13, 'gave': 13, 'david': 13, 'having': 13, 'equity': 13, 'consecutive': 13, 'yankees': 13, 'montreal': 13, 'jumped': 13, 'exports': 13, 'equipment': 13, 'registration': 13, 'networks': 13, 'wholesale': 13, 'historic': 12, 'strike': 12, 'care': 12, 'african': 12, 'among': 12, 'show': 12, 'interim': 12, 'setting': 12, 'mass': 12, 'pope': 12, 'kabul': 12, 'huge': 12, 'cause': 12, 'insurers': 12, 'teenager': 12, 'evening': 12, 'town': 12, 'delegates': 12, 'wis': 12, 'grand': 12, 'best': 12, 'facing': 12, 'support': 12, 'pull': 12, 'qaeda': 12, 'iverson': 12, 'arrived': 12, 'chinese': 12, 'factories': 12, 'building': 12, 'exporters': 12, 'hearing': 12, 'negotiations': 12, 'teams': 12, 'call': 12, 'great': 12, 'wanted': 12, 'health': 12, 'wal': 12, 'stores': 12, 'bought': 12, 'mart': 12, 'dow': 12, 'complete': 12, 'products': 12, 'anti': 12, 'supplies': 12, 'see': 12, 'series': 12, 'religious': 12, 'income': 12, 'claim': 12, 'island': 12, 'jewish': 12, 'statement': 12, 'football': 12, 'september': 12, 'applied': 12, 'profits': 12, 'medical': 12, 'done': 12, 'advanced': 12, 'athletes': 12, 'bhp': 12, 'billiton': 12, 'closely': 12, 'watched': 12, 'computer': 12, 'futures': 12, 'shooting': 12, 'cuts': 12, 'come': 12, 'roddick': 12, 'started': 12, 'palestinians': 12, 'mike': 12, 'pro': 12, 'grew': 12, 'relief': 12, 'homered': 12, 'judo': 12, 'defending': 12, 'compete': 12, 'looking': 12, 'sprinter': 12, 'torri': 12, 'auction': 12, 'technology': 12, 'lawsuit': 12, 'improvement': 12, 'ltd': 12, 'accounting': 12, 'effective': 12, 'inflationary': 12, 'area': 11, 'force': 11, 'dead': 11, 'pick': 11, 'fired': 11, 'asian': 11, 'clashes': 11, 'iran': 11, 'secretary': 11, 'crisis': 11, 'hoping': 11, 'den': 11, 'associated': 11, 'outside': 11, 'guard': 11, 'cents': 11, 'reduced': 11, 'jobs': 11, 'others': 11, 'spitz': 11, 'hotel': 11, 'lines': 11, 'jersey': 11, 'closer': 11, 'convention': 11, 'afghan': 11, 'supply': 11, 'nikkei': 11, 'knocked': 11, 'holiest': 11, 'enough': 11, 'uk': 11, 'chance': 11, 'annual': 11, 'information': 11, 'died': 11, 'average': 11, 'firms': 11, 'march': 11, 'northern': 11, 'find': 11, 'sept': 11, 'voted': 11, 'cincinnati': 11, 'events': 11, 'increased': 11, 'line': 11, 'arrested': 11, 'refused': 11, 'rebound': 11, 'terrorist': 11, 'settlements': 11, 'moved': 11, 'picked': 11, 'knew': 11, 'explosion': 11, 'chip': 11, 'track': 11, 'process': 11, 'corporate': 11, 'singapore': 11, 'decline': 11, 'mobile': 11, 'conspiracy': 11, 'allowed': 11, 'sun': 11, 'rebel': 11, 'kong': 11, 'wife': 11, 'louis': 11, 'changed': 11, 'draw': 11, 'tie': 11, 'houston': 11, 'ninth': 11, 'hitter': 11, 'bay': 11, 'figures': 11, 'offer': 11, 'jose': 11, 'panel': 11, 'crash': 11, 'hold': 11, 'vault': 11, 'minnesota': 11, 'suspension': 11, 'surgery': 11, 'orioles': 11, 'rangers': 11, 'swimmer': 11, 'larry': 11, 'scored': 11, 'largely': 11, 'familiar': 11, 'retail': 11, 'employees': 11, 'software': 11, 'hd': 11, 'payment': 11, 'goog': 11, 'costco': 11, 'venezuelans': 10, 'prisoners': 10, 'overnight': 10, 'dozens': 10, 'school': 10, 'rare': 10, 'changes': 10, 'george': 10, 'testing': 10, 'met': 10, 'ii': 10, 'remain': 10, 'faced': 10, 'determine': 10, 'himself': 10, 'analysts': 10, 'few': 10, 'hoogenband': 10, 'rivals': 10, 'pieter': 10, 'suspected': 10, 'fight': 10, 'hope': 10, 'house': 10, 'seeking': 10, 'played': 10, 'rest': 10, 'though': 10, 'tutsi': 10, 'massacre': 10, 'along': 10, 'motor': 10, 'losses': 10, 'already': 10, 'assault': 10, 'continues': 10, 'ministry': 10, 'attacks': 10, 'gone': 10, 'step': 10, 'ohio': 10, 'industrial': 10, 'upbeat': 10, 'rules': 10, 'finance': 10, 'butterfly': 10, 'tour': 10, 'cold': 10, 'aimed': 10, 'withdraw': 10, 'local': 10, 'german': 10, 'question': 10, 'hopes': 10, 'heart': 10, 'offered': 10, 'human': 10, 'stand': 10, 'banks': 10, 'launched': 10, 'light': 10, 'surging': 10, 'join': 10, 'post': 10, 'soldier': 10, 'sex': 10, 'hong': 10, 'thumb': 10, 'almost': 10, 'ban': 10, 'sport': 10, 'hamas': 10, 'attempt': 10, 'point': 10, 'too': 10, 'finished': 10, 'venus': 10, 'landing': 10, 'single': 10, 'goal': 10, 'starts': 10, 'joined': 10, 'bell': 10, 'metres': 10, 'grant': 10, 'system': 10, 'customers': 10, 'rate': 10, 'stake': 10, 'cos': 10, 'nasdaq': 10, 'airlines': 10, 'large': 9, 'korean': 9, 'wounding': 9, 'troubled': 9, 'arab': 9, 'armed': 9, 'rally': 9, 'assembly': 9, 'paid': 9, 'judge': 9, 'village': 9, 'talk': 9, 'hurt': 9, 'kept': 9, 'activists': 9, 'andrew': 9, 'woman': 9, 'struggled': 9, 'class': 9, 'middle': 9, 'raising': 9, 'working': 9, 'wounded': 9, 'use': 9, 'soon': 9, 'sale': 9, 'tiger': 9, 'governor': 9, 'operations': 9, 'breakaway': 9, 'hands': 9, 'insurer': 9, 'gain': 9, 'breaking': 9, 'bobby': 9, 'militant': 9, 'groups': 9, 'chris': 9, 'bad': 9, 'result': 9, 'mistakes': 9, 'times': 9, 'muqtada': 9, 'sending': 9, 'nepal': 9, 'management': 9, 'fuel': 9, 'response': 9, 'evidence': 9, 'immediately': 9, 'organization': 9, 'name': 9, 'driven': 9, 'settlement': 9, 'citing': 9, 'proposal': 9, 'dispute': 9, 'coming': 9, 'east': 9, 'sign': 9, 'berlin': 9, 'spot': 9, 'main': 9, 'playing': 9, 'overboard': 9, 'broke': 9, 'howard': 9, 'airline': 9, 'ireland': 9, 'positions': 9, 'minute': 9, 'lay': 9, 'bit': 9, 'holiday': 9, 'taken': 9, 'output': 9, 'beaten': 9, 'commit': 9, 'signed': 9, 'disarm': 9, 'villa': 9, 'sentiment': 9, 'suspended': 9, 'site': 9, 'ukraine': 9, 'criminal': 9, 'park': 9, 'table': 9, 'survey': 9, 'dream': 9, 'above': 9, 'hits': 9, 'oakland': 9, 'rookie': 9, 'yard': 9, 'pitcher': 9, 'amateur': 9, 'debut': 9, 'sweep': 9, 'johnson': 9, 'classic': 9, 'hal': 9, 'training': 9, 'breaststroke': 9, 'quarterback': 9, 'bargaining': 9, 'ratings': 9, 'your': 9, 'arbitration': 9, 'slow': 9, 'isn': 9, 'expectations': 9, 'hottest': 9, 'managers': 9, 'legal': 9, 'spending': 9, 'billing': 9, 'withhold': 9, 'mandate': 8, 'hunger': 8, 'georgian': 8, 'sudanese': 8, 'cease': 8, 'observers': 8, 'northeast': 8, 'dozen': 8, 'cities': 8, 'girl': 8, 'leftist': 8, 'damage': 8, 'preliminaries': 8, 'believe': 8, 'several': 8, 'muslim': 8, 'ottawa': 8, 'fellow': 8, 'within': 8, 'mining': 8, 'estimated': 8, 'calls': 8, 'linked': 8, 'session': 8, 'stunning': 8, 'named': 8, 'protests': 8, 'violent': 8, 'effort': 8, 'child': 8, 'every': 8, 'toyota': 8, 'embassy': 8, 'staff': 8, 'farm': 8, 'massive': 8, 'declared': 8, 'idea': 8, 'de': 8, 'rescue': 8, 'ease': 8, 'saw': 8, 'massachusetts': 8, 'struck': 8, 'returning': 8, 'fear': 8, 'justin': 8, 'extra': 8, 'playoff': 8, 'communist': 8, 'ordered': 8, 'helping': 8, 'waiting': 8, 'areas': 8, 'islamabad': 8, 'ryder': 8, 'climbed': 8, 'luxury': 8, 'expressed': 8, 'policy': 8, 'attempts': 8, 'lose': 8, 'carter': 8, 'cars': 8, 'flash': 8, 'prepared': 8, 'further': 8, 'defeat': 8, 'jean': 8, 'philippines': 8, 'blow': 8, 'believes': 8, 'advertising': 8, 'loyal': 8, 'threat': 8, 'focus': 8, 'netherlands': 8, 'always': 8, 'build': 8, 'weapons': 8, 'lows': 8, 'falling': 8, 'chancellor': 8, 'reached': 8, 'tomas': 8, 'berdych': 8, 'claiming': 8, 'sardinia': 8, 'strength': 8, 'needed': 8, 'online': 8, 'radioactive': 8, 'total': 8, 'stay': 8, 'scott': 8, 'paperwork': 8, 'approve': 8, 'defused': 8, 'arafat': 8, 'yasser': 8, 'matter': 8, 'offers': 8, 'injuries': 8, 'auto': 8, 'confidence': 8, 'switzerland': 8, 'struggling': 8, 'size': 8, 'clear': 8, 'woods': 8, 'seattle': 8, 'eighth': 8, 'stayed': 8, 'kansas': 8, 'streak': 8, 'homer': 8, 'knee': 8, 'gary': 8, 'comes': 8, 'jr': 8, 'rather': 8, 'instead': 8, 'teammates': 8, 'costas': 8, 'highest': 8, 'brown': 8, 'eagerly': 8, 'current': 8, 'expos': 8, 'proved': 8, 'source': 8, 'product': 8, 'declare': 8, 'airport': 8, 'opec': 8, 'sec': 8, 'civil': 8, 'kmart': 8, 'unit': 8, 'kuwait': 8, 'treasury': 8, 'revenue': 8, 'weaker': 8, 'reversed': 8, 'commerce': 8, 'cent': 8, 'numbers': 7, 'controversial': 7, 'conditions': 7, 'rwandan': 7, 'remote': 7, 'separatist': 7, 'recovery': 7, 'candidate': 7, 'prescription': 7, 'vice': 7, 'although': 7, 'heat': 7, 'rolled': 7, 'law': 7, 'drew': 7, 'defence': 7, 'afternoon': 7, 'reportedly': 7, 'elections': 7, 'known': 7, 'witnesses': 7, 'decade': 7, 'oust': 7, 'disaster': 7, 'added': 7, 'klete': 7, 'front': 7, 'sick': 7, 'october': 7, 'monthly': 7, 'bills': 7, 'captured': 7, 'together': 7, 'special': 7, 'protest': 7, 'aides': 7, 'whistling': 7, 'straits': 7, 'revolution': 7, 'eliminated': 7, 'increasing': 7, 'benefits': 7, 'sweeping': 7, 'easy': 7, 'upset': 7, 'promised': 7, 'battled': 7, 'movement': 7, 'italy': 7, 'threatened': 7, 'administration': 7, 'tim': 7, 'bringing': 7, 'borders': 7, 'zealand': 7, 'operation': 7, 'newspaper': 7, 'midday': 7, 'spokesman': 7, 'suspicious': 7, 'worst': 7, 'renewed': 7, 'affair': 7, 'published': 7, 'officer': 7, 'adam': 7, 'senate': 7, 'november': 7, 'created': 7, 'once': 7, 'wild': 7, 'serious': 7, 'khartoum': 7, 'tied': 7, 'boys': 7, 'opponents': 7, 'challenge': 7, 'shrugged': 7, 'walk': 7, 'expects': 7, 'issues': 7, 'premier': 7, 'summit': 7, 'prison': 7, 'fighters': 7, 'jay': 7, 'story': 7, 'indian': 7, 'showdown': 7, 'euros': 7, 'block': 7, 'available': 7, 'buried': 7, 'winner': 7, 'apparently': 7, 'leaving': 7, 'bring': 7, 'motors': 7, 'robert': 7, 'accept': 7, 'sea': 7, 'criticism': 7, 'serving': 7, 'tom': 7, 'sites': 7, 'countries': 7, 'abroad': 7, 'royal': 7, 'highly': 7, 'positive': 7, 'index': 7, 'qaida': 7, 'cross': 7, 'congo': 7, 'side': 7, 'madrid': 7, 'production': 7, 'consider': 7, 'thanks': 7, 'father': 7, 'diving': 7, 'tore': 7, 'operative': 7, 'gerhard': 7, 'adopted': 7, 'issued': 7, 'opener': 7, 'schroeder': 7, 'ask': 7, 'banned': 7, 'alexander': 7, 'string': 7, 'forward': 7, 'scandal': 7, 'caught': 7, 'targets': 7, 'quit': 7, 'view': 7, 'successful': 7, 'greeks': 7, 'resolve': 7, 'daily': 7, 'hockey': 7, 'let': 7, 'base': 7, 'kevin': 7, 'warnings': 7, 'needs': 7, 'violations': 7, 'paris': 7, 'moscow': 7, 'regional': 7, 'disciplinary': 7, 'separate': 7, 'launch': 7, 'friendly': 7, 'agent': 7, 'drive': 7, 'draft': 7, 'hand': 7, 'judges': 7, 'safety': 7, 'reds': 7, 'crowd': 7, 'wide': 7, 'dodgers': 7, 'shoulder': 7, 'champions': 7, 'hungarian': 7, 'ranked': 7, 'tech': 7, 'bryant': 7, 'kobe': 7, 'hamilton': 7, 'assistant': 7, 'arizona': 7, 'mason': 7, 'legg': 7, 'each': 7, 'kosuke': 7, 'kitajima': 7, 'raise': 7, 'ten': 7, 'recently': 7, 'collective': 7, 'brian': 7, 'usa': 7, 'things': 7, 'wave': 7, 'college': 7, 'detroit': 7, 'slam': 7, 'topping': 7, 'clemens': 7, 'declined': 7, 'ibm': 7, 'executives': 7, 'board': 7, 'makers': 7, 'airways': 7, 'pile': 7, 'kmrt': 7, 'gained': 7, 'satellite': 7, 'lowest': 7, 'merger': 7, 'inventory': 7, 'capacity': 7, 'bankruptcy': 7, 'nyse': 7, 'doubled': 7, 'janus': 7, 'delayed': 7, 'hare': 7, 'fleet': 7, 'devices': 7, 'nestle': 7, 'auditors': 7, 'parmalat': 7, 'either': 6, 'protesters': 6, 'violation': 6, 'independence': 6, 'edge': 6, 'congress': 6, 'required': 6, 'roman': 6, 'ailing': 6, 'lourdes': 6, 'finals': 6, 'inquiry': 6, 'small': 6, 'port': 6, 'body': 6, 'denied': 6, 'wake': 6, 'previous': 6, 'dollars': 6, 'probably': 6, 'keller': 6, 'semifinals': 6, 'millions': 6, 'catch': 6, 'coast': 6, 'terrorists': 6, 'image': 6, 'band': 6, 'powerful': 6, 'desperate': 6, 'battles': 6, 'finish': 6, 'inevitable': 6, 'dug': 6, 'winds': 6, 'hilton': 6, 'chairman': 6, 'style': 6, 'james': 6, 'period': 6, 'alert': 6, 'families': 6, 'crowds': 6, 'emergency': 6, 'control': 6, 'boy': 6, 'trees': 6, 'eastern': 6, 'cbs': 6, 'custody': 6, 'gas': 6, 'surge': 6, 'whose': 6, 'living': 6, 'self': 6, 'delivered': 6, 'malaysia': 6, 'begins': 6, 'jail': 6, 'claimed': 6, 'prince': 6, 'plc': 6, 'tell': 6, 'member': 6, 'themselves': 6, 'authority': 6, 'tight': 6, 'something': 6, 'torn': 6, 'rejected': 6, 'leonard': 6, 'bargain': 6, 'corruption': 6, 'invoices': 6, 'perhaps': 6, 'beginning': 6, 'twice': 6, 'prove': 6, 'haas': 6, 'stewart': 6, 'newly': 6, 'kathmandu': 6, 'appealed': 6, 'stop': 6, 'portugal': 6, 'throw': 6, 'carrier': 6, 'division': 6, 'pentagon': 6, 'divisions': 6, 'gap': 6, 'signing': 6, 'fought': 6, 'challenges': 6, 'jury': 6, 'rain': 6, 'reforms': 6, 'meetings': 6, 'supporters': 6, 'calm': 6, 'tenders': 6, 'fast': 6, 'thought': 6, 'maoist': 6, 'bids': 6, 'crashed': 6, 'helicopter': 6, 'amat': 6, 'expansion': 6, 'hitting': 6, 'arms': 6, 'cautious': 6, 'seen': 6, 'arena': 6, 'jason': 6, 'liverpool': 6, 'tearing': 6, 'collect': 6, 'verge': 6, 'planned': 6, 'aside': 6, 'medallist': 6, 'trap': 6, 'settler': 6, 'young': 6, 'ending': 6, 'version': 6, 'kingdom': 6, 'replaced': 6, 'change': 6, 'slower': 6, 'tomorrow': 6, 'plotting': 6, 'refugee': 6, 'thing': 6, 'conspiring': 6, 'involving': 6, 'want': 6, 'delay': 6, 'doubts': 6, 'facilities': 6, 'mariel': 6, 'fencing': 6, 'zagunis': 6, 'pinch': 6, 'jays': 6, 'pitch': 6, 'personal': 6, 'earn': 6, 'problems': 6, 'ex': 6, 'lack': 6, 'easing': 6, 'disruptions': 6, 'deficit': 6, 'branch': 6, 'stopped': 6, 'closed': 6, 'row': 6, 'sense': 6, 'hot': 6, 'microsoft': 6, 'suffered': 6, 'dismissed': 6, 'stage': 6, 'offset': 6, 'prosecutors': 6, 'advance': 6, 'earned': 6, 'preseason': 6, 'opponent': 6, 'par': 6, 'wisconsin': 6, 'mariners': 6, 'elbow': 6, 'exhibition': 6, 'looks': 6, 'baseman': 6, 'scheduled': 6, 'singles': 6, 'winged': 6, 'brazil': 6, 'volleyball': 6, 'minutes': 6, 'phillies': 6, 'steve': 6, 'patriots': 6, 'weekly': 6, 'regular': 6, 'knows': 6, 'putter': 6, 'hall': 6, 'becoming': 6, 'leg': 6, 'arlington': 6, 'professional': 6, 'brendan': 6, 'seed': 6, 'pittsburgh': 6, 'picture': 6, 'outfielder': 6, 'bowl': 6, 'twenty': 6, 'athletics': 6, 'job': 6, 'ground': 6, 'really': 6, 'slip': 6, 'ticket': 6, 'plus': 6, 'colorado': 6, 'fractured': 6, 'championships': 6, 'ny': 6, 'why': 6, 'owners': 6, 'teixeira': 6, 'upi': 6, 'nelson': 6, 'strained': 6, 'sprint': 6, 'cycling': 6, 'akron': 6, 'improved': 6, 'bonds': 6, 'mat': 6, 'tumbled': 6, 'mcdonald': 6, 'slump': 6, 'improvements': 6, 'manage': 6, 'slumping': 6, 'restaurants': 6, 'hospitals': 6, 'payments': 6, 'multi': 6, 'brokerage': 6, 'activity': 6, 'practices': 6, 'promotion': 6, 'ing': 6, 'applications': 6, 'touted': 6, 'barclays': 6, 'oq': 6, 'freddie': 6, 'laws': 6, 'mac': 6, 'flew': 5, 'black': 5, 'protect': 5, 'parade': 5, 'creating': 5, 'programme': 5, 'warning': 5, 'concerned': 5, 'programs': 5, 'appears': 5, 'must': 5, 'wants': 5, 'maria': 5, 'electricity': 5, 'pass': 5, 'grim': 5, 'path': 5, 'shia': 5, 'ravaged': 5, 'jackson': 5, 'negotiate': 5, 'details': 5, 'truce': 5, 'convicted': 5, 'important': 5, 'calling': 5, 'watson': 5, 'landmark': 5, 'quickly': 5, 'walked': 5, 'tanks': 5, 'voters': 5, 'dawn': 5, 'flood': 5, 'votes': 5, 'wreckage': 5, 'billions': 5, 'faster': 5, 'teammate': 5, 'friends': 5, 'crowded': 5, 'mary': 5, 'counting': 5, 'telephone': 5, 'pontiff': 5, 'miracle': 5, 'fueled': 5, 'wind': 5, 'closing': 5, 'pre': 5, 'ramallah': 5, 'order': 5, 'delivery': 5, 'county': 5, 'fed': 5, 'unrest': 5, 'gov': 5, 'presidency': 5, 'enter': 5, 'electoral': 5, 'el': 5, 'ethnic': 5, 'miles': 5, 'lives': 5, 'device': 5, 'improve': 5, 'globe': 5, 'snow': 5, 'warned': 5, 'threatening': 5, 'registered': 5, 'extending': 5, 'selection': 5, 'kuala': 5, 'lumpur': 5, 'sri': 5, 'powder': 5, 'tools': 5, 'typhoon': 5, 'torture': 5, 'psychological': 5, 'border': 5, 'tens': 5, 'rushed': 5, 'preliminary': 5, 'son': 5, 'urban': 5, 'disappeared': 5, 'falconio': 5, 'backpacker': 5, 'peter': 5, 'chess': 5, 'fischer': 5, 'sanctions': 5, 'situation': 5, 'raids': 5, 'heard': 5, 'prominent': 5, 'faces': 5, 'lawyers': 5, 'victims': 5, 'rallied': 5, 'development': 5, 'magazine': 5, 'aware': 5, 'joining': 5, 'oldest': 5, 'prompted': 5, 'outlooks': 5, 'speed': 5, 'toll': 5, 'revealed': 5, 'congestion': 5, 'ways': 5, 'prosecutor': 5, 'moves': 5, 'kid': 5, 'served': 5, 'saudi': 5, 'ditch': 5, 'tickets': 5, 'bases': 5, 'announcement': 5, 'boxing': 5, 'super': 5, 'antonio': 5, 'recovering': 5, 'southwest': 5, 'floods': 5, 'tourist': 5, 'swept': 5, 'happen': 5, 'gymnasts': 5, 'presence': 5, 'brought': 5, 'envoy': 5, 'cowboys': 5, 'quincy': 5, 'romania': 5, 'regulatory': 5, 'arch': 5, 'tested': 5, 'signs': 5, 'lift': 5, 'buyers': 5, 'reduction': 5, 'english': 5, 'strip': 5, 'radio': 5, 'crown': 5, 'blockade': 5, 'pm': 5, 'university': 5, 'motorola': 5, 'bujumbura': 5, 'diplomatic': 5, 'moderate': 5, 'occupied': 5, 'embarrassing': 5, 'hamid': 5, 'karzai': 5, 'heading': 5, 'headed': 5, 'our': 5, 'ambassador': 5, 'reform': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab, df, ngram_counts = get_vocab(x_raw,(1,1),r'\\b[A-Za-z][A-Za-z]+\\b', 2, 2000, stop_words)\n",
    "print(\"Vocab: \\n\", list(vocab)[:30])\n",
    "print(\"\\n Document Frequency \\n\", df)\n",
    "len(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create vocabulary id -> word and word -> vocabulary id dictionaries for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:44.069661Z",
     "start_time": "2020-04-02T14:26:44.065058Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' Created two dictionaries id_to_word and word_to_id\n",
    "    id_to_word dictionary stores id as key and words as value\n",
    "    word_to_id dictionary stores words as key and id as value'''\n",
    "def create_Dictionary(vocab):\n",
    "    id_to_word={}\n",
    "    word_to_id={}\n",
    "    for i,v in enumerate(vocab):\n",
    "        id_to_word[i]=v\n",
    "        word_to_id[v]=i\n",
    "        \n",
    "    return id_to_word, word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word,word_to_id=create_Dictionary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the list of unigrams  into a list of vocabulary indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing actual one-hot vectors into memory for all words in the entire data set is prohibitive. Instead, we will store word indices in the vocabulary and look-up the weight matrix. This is equivalent of doing a dot product between an one-hot vector and the weight matrix. \n",
    "\n",
    "First, represent documents in train, dev and test sets as lists of words in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:45.047887Z",
     "start_time": "2020-04-02T14:26:44.920631Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting News_Articles column from the dataframe for train, test, dev datasets and converting them to a list\n",
    "train_df_list = train_df['News_Articles'].tolist()\n",
    "dev_df_list = dev_df['News_Articles'].tolist()\n",
    "test_df_list = test_df['News_Articles'].tolist()\n",
    "vocab_train, df_train, ngram_counts_train = get_vocab(train_df_list,(1,1),r'\\b[A-Za-z][A-Za-z]+\\b', 1, 2000, stop_words)\n",
    "len(vocab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df_list=['My city is reuters','My name is not president']\n",
    "#Initialized 3 empty lists to store resulting ngrams for each document\n",
    "train_docs_ngrams=[]\n",
    "dev_docs_ngrams=[]\n",
    "test_docs_ngrams=[]\n",
    "''' Extract all unigrams within the document, removing the given stop words and storing them in the lists\n",
    "    This is done for train, development and test datasets.'''\n",
    "for train_doc in train_df_list:\n",
    "    train_doc_ngrams=extract_ngrams(train_doc,ngram_range=(1,1),stop_words=stop_words)\n",
    "    train_docs_ngrams.append(train_doc_ngrams)\n",
    "for dev_doc in dev_df_list:\n",
    "    dev_doc_ngrams=extract_ngrams(dev_doc,ngram_range=(1,1),stop_words=stop_words)\n",
    "    dev_docs_ngrams.append(dev_doc_ngrams)\n",
    "for test_doc in test_df_list:\n",
    "    test_doc_ngrams=extract_ngrams(test_doc,ngram_range=(1,1),stop_words=stop_words)\n",
    "    test_docs_ngrams.append(test_doc_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then convert them into lists of indices in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Converts the extracted n-grams for each document in the train, dev, \n",
    "    and test sets to their corresponding IDs. The resulting IDs for each \n",
    "    document are then stored as numpy arrays in three separate lists:\n",
    "    train_articles_ids, dev_articles_ids, and test_articles_ids'''\n",
    "train_articles_ids=[]\n",
    "dev_articles_ids=[]\n",
    "test_articles_ids=[]\n",
    "for article_ngrams_tr in train_docs_ngrams:\n",
    "    article_ids=[]\n",
    "    for ngram in article_ngrams_tr:\n",
    "        if ngram in word_to_id:\n",
    "            article_ids.append(word_to_id[ngram])\n",
    "    train_articles_ids.append(np.array(article_ids,dtype=np.int32))\n",
    "\n",
    "\n",
    "for article_ngrams_dv in dev_docs_ngrams:\n",
    "    article_ids=[]\n",
    "    for ngram in article_ngrams_dv:\n",
    "        if ngram in word_to_id:\n",
    "            article_ids.append(word_to_id[ngram])\n",
    "    dev_articles_ids.append(np.array(article_ids,dtype=np.int32))\n",
    "\n",
    "for article_ngrams_tst in test_docs_ngrams:\n",
    "    article_ids=[]\n",
    "    for ngram in article_ngrams_tst:\n",
    "        if ngram in word_to_id:\n",
    "            article_ids.append(word_to_id[ngram])\n",
    "    test_articles_ids.append(np.array(article_ids,dtype=np.int32))\n",
    "\n",
    "#print(train_articles_ids)---- output-[[81, 7200, 3771], [81, 923, 4406]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 616,  800,  933,  255,  231, 1290,  569,  834, 1781, 1030, 1976,\n",
       "        1278,  413,  621, 1136,  622,  402, 1718, 1033, 1886, 1412,  281,\n",
       "         812, 1630,  342], dtype=int32),\n",
       " array([ 616,  246,  709,  312, 1943,  867,  354,  834,  580, 1831,  553,\n",
       "        1928,   96, 1578,  936, 1774, 1301], dtype=int32),\n",
       " array([ 616,  268, 1455,  187,  731,  394, 1666,  153, 1304,  749,  834,\n",
       "         137,  445,  330, 1614, 1017, 1264, 1840], dtype=int32),\n",
       " array([1173, 1049, 1189,  529,  203,  332,  246,  328, 1711, 1705,   60,\n",
       "         203], dtype=int32),\n",
       " array([1197, 1232, 1851, 1679,   76,  669,  961, 1259, 1198,  332,  834,\n",
       "          28, 1467, 1794,  163,  158,   14,  383, 1699,  394,  788, 1503,\n",
       "        1810,  286,  268], dtype=int32),\n",
       " array([ 616, 1851, 1774,  834,  961, 1198,   28, 1467,  163,  150,  559,\n",
       "         952,  139, 1010, 1442,  553,  940, 1259, 1131,  332], dtype=int32),\n",
       " array([1197,  798, 1003, 1323, 1555,  685,   36, 1434, 1324,  834,  286,\n",
       "         580, 1200,  972, 1219, 1028, 1003,  960,  565,  332,   88, 1112,\n",
       "        1614], dtype=int32),\n",
       " array([1173,  806, 1467,  330,  957, 1839, 1817,  692,  877,  677,  787,\n",
       "         778, 1180, 1527, 1878, 1126, 1860,  692,  709,  860, 1965,  141],\n",
       "       dtype=int32),\n",
       " array([1197, 1672,  336,  929,  436,  891,  226, 1817,  983, 1062,  520,\n",
       "         899,  947,  213, 1931,  699, 1614,  470], dtype=int32),\n",
       " array([1197, 1672,  126,  336,  929,  436,  392,  115,  904, 1993, 1815,\n",
       "         469,   40,  436,  891, 1982,  553, 1288,  213, 1070, 1126,  470],\n",
       "       dtype=int32),\n",
       " array([1173, 1513, 1267,  240,  336, 1508,   50,  689,  436,  891, 1444,\n",
       "        1009, 1701,  420,   29,  527,  886, 1058], dtype=int32),\n",
       " array([1197,  780,  949, 1685,  150, 1881,  368, 1489,  491, 1319, 1609,\n",
       "         172, 1058,  107,  548,  296, 1614,  470], dtype=int32),\n",
       " array([1197,  624,  757, 1842, 1564,  275, 1350, 1383, 1841,  665,  208,\n",
       "        1591, 1972], dtype=int32),\n",
       " array([1848,  616, 1635,  297, 1119, 1592, 1669, 1490,  154, 1526,  315,\n",
       "        1359,  834,  286,  580, 1630, 1200,  645,  898, 1361, 1502,  979],\n",
       "       dtype=int32),\n",
       " array([1495,  616,  362,   38, 1738, 1928, 1457, 1867,   50,  834, 1456,\n",
       "         828, 1867,  503, 1298, 1745,  989], dtype=int32),\n",
       " array([1025, 1010,  240, 1774,  447, 1699, 1602,  240,   44, 1552],\n",
       "       dtype=int32),\n",
       " array([ 834,  800, 1819,  375,  693, 1136,  308,  316, 1237], dtype=int32),\n",
       " array([1487, 1112,  677,  244, 1874], dtype=int32),\n",
       " array([ 787, 1918, 1201, 1592, 1359,  257,  162, 1223, 1798, 1848,  406,\n",
       "          44], dtype=int32),\n",
       " array([1976, 1676, 1595, 1577, 1136,  622,  402,  915,   57, 1718],\n",
       "       dtype=int32),\n",
       " array([1499, 1947,  436,   79,  503,  136, 1892,  750, 1268, 1457,  341],\n",
       "       dtype=int32),\n",
       " array([1851, 1774,  961,  723,  150, 1867, 1713, 1198,  332], dtype=int32),\n",
       " array([1750, 1080, 1342, 1079,  566, 1941,  669,  612], dtype=int32),\n",
       " array([ 482, 1294, 1487, 1701], dtype=int32),\n",
       " array([ 428,  142,  122, 1461,  669, 1726, 1957,  312], dtype=int32),\n",
       " array([1301,  889, 1701, 1701,  240,  906,  542,  297, 1185], dtype=int32),\n",
       " array([1045,  246,  328, 1814, 1937, 1331,  162, 1322, 1821, 1028],\n",
       "       dtype=int32),\n",
       " array([ 761, 1301,  136, 1447], dtype=int32),\n",
       " array([744, 467, 286,  36,  28, 974, 342], dtype=int32),\n",
       " array([1376,  681, 1427,  956,   28,  223, 1090,  342], dtype=int32),\n",
       " array([1500, 1582, 1935,  759,  586,   81, 1577, 1280,  336, 1356, 1976,\n",
       "        1532, 1504,  407,  979, 1126,  828,  451,   82, 1582,  186, 1007],\n",
       "       dtype=int32),\n",
       " array([ 686,  168, 1205,  420, 1652, 1238,  619,  669,  414,  665,  214,\n",
       "        1125,  686, 1816,  973], dtype=int32),\n",
       " array([1884, 1671,  249, 1131, 1671,  733,  195,  286, 1200, 1201,  801,\n",
       "         337,  445,  898,  505,  359], dtype=int32),\n",
       " array([1848, 1527,  936,  922,  677, 1546, 1010, 1700,  240, 1592, 1535,\n",
       "          44, 1848,  447, 1816, 1806,  295,  613,  581,  662,  602, 1526,\n",
       "         232,  330, 1392, 1856,  464,  409,  447, 1616, 1528, 1633,  413,\n",
       "         613,  922, 1963], dtype=int32),\n",
       " array([1456, 1205,  713, 1816,  548, 1330,  892, 1661,  669,  960, 1181,\n",
       "        1360, 1616, 1768,  642, 1951, 1614, 1155], dtype=int32),\n",
       " array([ 123, 1535,   28, 1734,  878,  577, 1196, 1945,  842, 1530],\n",
       "       dtype=int32),\n",
       " array([ 342, 1106, 1617, 1966, 1342,  955,  709], dtype=int32),\n",
       " array([1500, 1577,   84, 1128,  213,  776, 1802, 1136,  622,  402,  252,\n",
       "        1734,  214, 1356, 1976,  834, 1757, 1717, 1260,   94,  527,  278,\n",
       "        1751, 1886,   54,  862, 1582, 1021,  884,  225, 1440, 1083,  839,\n",
       "          68, 1260, 1281,  231,  299], dtype=int32),\n",
       " array([ 118, 1141,  455, 1527,  326,   28, 1224,  486,  413, 1350, 1062,\n",
       "         520,  983,   34, 1447,  381, 1395, 1095, 1115, 1701, 1964],\n",
       "       dtype=int32),\n",
       " array([1062,  520,  306, 1845, 1042, 1102, 1855,   17,  915,  968,  255,\n",
       "         913,  536,  127,  646, 1841, 1402, 1112, 1614,  470,  602, 1143,\n",
       "          75,  899, 1855,  759,  398], dtype=int32),\n",
       " array([ 118, 1141,  455, 1527,  326,   28, 1224,  486,  413, 1350, 1062,\n",
       "         520,  983,   34, 1447,  381, 1395, 1095, 1115, 1701, 1964],\n",
       "       dtype=int32),\n",
       " array([ 888,  207, 1726,  659, 1049, 1264,  592, 1075,  504, 1079, 1735,\n",
       "         834,   85,  854,  255, 1616, 1462, 1921,  931,  292, 1079, 1890,\n",
       "        1572,  659, 1316, 1750, 1080,  140,  890, 1114, 1704,  475,   39,\n",
       "        1647,  521,  922, 1382,   62,  953], dtype=int32),\n",
       " array([1500, 1577,   84, 1128,  213,  776, 1802, 1136,  622,  402,  252,\n",
       "        1734,  214, 1356, 1976,  834, 1757, 1717, 1260,   94,  527,  278,\n",
       "        1751, 1886,   54, 1112, 1479,  828,  972, 1947,  436,   79,  503],\n",
       "       dtype=int32),\n",
       " array([1412,  798, 1003, 1323, 1555,  685,   36, 1434, 1324,  834,  286,\n",
       "         580, 1200, 1112, 1614, 1150, 1949, 1527,  232,  330, 1606, 1394,\n",
       "         905, 1969, 1832, 1036,  456, 1350, 1028, 1841,  666,  408, 1716,\n",
       "         158, 1614,  624,  312], dtype=int32),\n",
       " array([1430,  385,  470, 1288,  430,  822, 1451, 1219,  337, 1200, 1288,\n",
       "         972, 1582,  907, 1648, 1099], dtype=int32),\n",
       " array([ 888,  207, 1726,  659, 1049, 1264,  592, 1075,  504, 1079, 1735,\n",
       "         834,   85,  854,  255, 1616, 1462, 1921,  931,  292, 1079, 1890,\n",
       "        1572,  659, 1316, 1750, 1080,  140,  890, 1114, 1704,  475,   39,\n",
       "        1647,  521,  922, 1382,   62,  953], dtype=int32),\n",
       " array([ 750, 1140, 1947,  436,   79,  503,  688,  665, 1892,  834,  606,\n",
       "          90, 1975, 1251, 1656, 1268, 1457, 1785,  272, 1377, 1892, 1947,\n",
       "        1630, 1555, 1817,  750,  951, 1119,  341,  136,  707], dtype=int32),\n",
       " array([1301, 1578, 1774,   44, 1414,  259, 1325,  506, 1527, 1241,  787,\n",
       "        1010,  553,  464,  447], dtype=int32),\n",
       " array([1546,  557, 1816, 1301, 1412,  553], dtype=int32),\n",
       " array([ 616,  268, 1774, 1671, 1742,  355,   70, 1655,  898, 1030,   81,\n",
       "         432], dtype=int32),\n",
       " array([ 616, 1635,  297, 1119, 1592, 1669, 1490,  154, 1526,  315, 1359,\n",
       "         834,  286,  580, 1630, 1200,  645,  898, 1361, 1502,  979],\n",
       "       dtype=int32),\n",
       " array([1197,  162,  544, 1798,  898,   90,  762,  588,  834, 1301,  315,\n",
       "        1660,  437,  255,  449,   28,  979,  860], dtype=int32),\n",
       " array([1197, 1870,  335,  394, 1381,  583,  569, 1345, 1536, 1827],\n",
       "       dtype=int32),\n",
       " array([ 963, 1172,  864, 1654, 1804,  312,  337, 1174, 1710, 1868,  412,\n",
       "        1863,   20,  669, 1176,  613], dtype=int32),\n",
       " array([1173, 1941,  803,   72,  821,  794, 1871, 1412,  699,  240],\n",
       "       dtype=int32),\n",
       " array([1500, 1577,  616,  800, 1975,  834, 1781, 1819, 1356,  413,  621,\n",
       "        1136,  622,  402, 1874,  281,  828,  451,  812, 1630,  342],\n",
       "       dtype=int32),\n",
       " array([ 616, 1077, 1301, 1886,  851, 1083,  482,  142,  187, 1522, 1867,\n",
       "        1301, 1841,  694, 1614,  834], dtype=int32),\n",
       " array([1884,  616,  268, 1774, 1671, 1742,  355,   70, 1655,  898, 1030,\n",
       "          81,  432], dtype=int32),\n",
       " array([ 616, 1200, 1412,  834, 1412, 1204, 1636, 1244], dtype=int32),\n",
       " array([1630, 1163, 1183, 1140, 1656], dtype=int32),\n",
       " array([1142,  683,  107, 1024,  667,  240,  268, 1427], dtype=int32),\n",
       " array([1025,  312,  268, 1995,  788, 1398, 1578, 1774,  723,  240,  600,\n",
       "        1535, 1301], dtype=int32),\n",
       " array([1062,  520,  408,  759, 1184, 1881, 1280, 1169,  580, 1763,  486,\n",
       "         983], dtype=int32),\n",
       " array([ 888,  207, 1726,  659, 1049, 1264,  592, 1075,  504, 1079, 1735,\n",
       "         834,   85,  854,  255, 1616, 1462, 1921,  931,  292, 1079, 1890,\n",
       "        1572,  659, 1316, 1750, 1080,  140,  890, 1114, 1704,  475,   39,\n",
       "        1647,  521,  922, 1382,   62,  953], dtype=int32),\n",
       " array([ 750, 1140, 1377,  744,  527, 1377, 1947,  436,   79,  503, 1222,\n",
       "         381,  834, 1892, 1268, 1457,  601, 1200, 1824, 1178,  548, 1330,\n",
       "        1664, 1617, 1956,  723, 1003, 1949, 1288,  689], dtype=int32),\n",
       " array([1636,  702, 1425,  416,  580, 1149], dtype=int32),\n",
       " array([1500, 1577, 1440,  933,  255,   94,  569,  834, 1819, 1751, 1136,\n",
       "         622,  402,   54,  776, 1886, 1055,  602, 1822,  410, 1793, 1010,\n",
       "         278,  862, 1582, 1021,  884,   94, 1619,  225, 1440, 1083,  839,\n",
       "          68], dtype=int32),\n",
       " array([1103,  698,  569,  315,  335,  917, 1774, 1867, 1301, 1671, 1412,\n",
       "         400,  457,  969,  872,  920,   65], dtype=int32),\n",
       " array([ 422, 1429, 1793,  381, 1330,  673,  951, 1656,  834,  596,  878,\n",
       "          34, 1112, 1614], dtype=int32),\n",
       " array([ 118, 1141,  455,  983,   34,  394, 1755, 1201, 1395,  413, 1350,\n",
       "        1062,  520, 1112,  834,  927, 1125, 1763,  596], dtype=int32),\n",
       " array([ 963, 1172, 1307,  196, 1654,  232,  330,   79, 1490, 1995,  834,\n",
       "        1725,  828,  780], dtype=int32),\n",
       " array([ 791, 1273,   47,  616,  268, 1455,  187,  731,  394, 1666,  153,\n",
       "        1304,  749,  834,  137,  445,  330, 1614, 1017, 1264, 1840],\n",
       "       dtype=int32),\n",
       " array([ 750, 1140,  616, 1947,  436,   79, 1377,  744,  527, 1377, 1201,\n",
       "        1817, 1178, 1457,  834, 1222, 1416, 1470,  855, 1479, 1886],\n",
       "       dtype=int32),\n",
       " array([ 888,  207,  907,  138, 1148,  258, 1255, 1726,  659, 1007,   60,\n",
       "         740, 1961, 1294], dtype=int32),\n",
       " array([ 888,  207,  390, 1030,  453,  483, 1368,  931,  568,  680,  691,\n",
       "        1342,   74, 1294,  834, 1600, 1812], dtype=int32),\n",
       " array([1500, 1577,  616,  800, 1975,  834, 1781, 1819, 1356,  413,  621,\n",
       "        1136,  622,  402, 1874,  281,  828,  451,  812, 1630,  342],\n",
       "       dtype=int32),\n",
       " array([1197, 1377,  744,  527, 1377, 1947,  436,   79,  503, 1222,  381,\n",
       "         834, 1892, 1268, 1457,  601, 1200, 1824, 1178,  548, 1330, 1664,\n",
       "        1617, 1956,  723, 1003, 1949, 1288,  689], dtype=int32),\n",
       " array([ 118, 1141,  455,   34,  413, 1062,  520, 1534, 1905,  381, 1713,\n",
       "         596,  834, 1755, 1201, 1136,   29,  805, 1703,  398], dtype=int32),\n",
       " array([ 897, 1265, 1412, 1748, 1244, 1553,  839,   68,  834,  895,  900,\n",
       "         272, 1172, 1265, 1855,  895,  182], dtype=int32),\n",
       " array([1103, 1807,  788,  757,  917, 1585, 1374,  640,   81,  548, 1061,\n",
       "        1315, 1339,   52, 1991, 1909, 1303, 1339, 1524,  803, 1786, 1005,\n",
       "         337, 1841,  617], dtype=int32),\n",
       " array([  44, 1301, 1945, 1774, 1319, 1874,  669,  973,   44,  563,  259,\n",
       "        1270,  834, 1798,  557, 1816, 1150,  762, 1848,  514, 1660,  505,\n",
       "         898, 1059, 1502,  979], dtype=int32),\n",
       " array([1500, 1577,  901, 1655,  800,  933,  255,  569,  834, 1781, 1819,\n",
       "         163, 1757, 1136,  622,  402, 1718,  322, 1119, 1125, 1757, 1717,\n",
       "        1398, 1709, 1119,  847, 1055,  278], dtype=int32),\n",
       " array([1696,  731,  744,  973, 1412,  745,  365, 1454], dtype=int32),\n",
       " array([1197, 1577,  451, 1402,  201, 1412, 1783, 1203, 1527,  834, 1976,\n",
       "         622,  402, 1596, 1136, 1841,  998,  451, 1207, 1614,  834],\n",
       "       dtype=int32),\n",
       " array([1173,  356, 1125, 1539,  681, 1954,  409, 1124,  195, 1015, 1028,\n",
       "         255,  232,  330,  351,   93,   16, 1527, 1419, 1774,  255, 1301],\n",
       "       dtype=int32),\n",
       " array([1197, 1555, 1459,   60, 1246,  299, 1672,  126,  336,  436,  392,\n",
       "        1862,   29, 1665,  834,  456, 1522, 1815, 1042, 1464,  650],\n",
       "       dtype=int32),\n",
       " array([1546,   44, 1015, 1660,  315, 1359], dtype=int32),\n",
       " array([1781,   94, 1133, 1201,  122,  913, 1875, 1716], dtype=int32),\n",
       " array([ 883,  116,  258, 1974], dtype=int32),\n",
       " array([ 888,  207,  390, 1030,  453,  658, 1518,  152, 1148, 1368,  931,\n",
       "         568, 1600, 1812,  834,  680,  691, 1075, 1014,   74,  771,   28,\n",
       "          14], dtype=int32),\n",
       " array([1969, 1037,  616,  445, 1490,  216, 1349,  194,  834,  654,   22,\n",
       "         936,  922,  676,  489, 1168,  628, 1131,  194], dtype=int32),\n",
       " array([ 616, 1851, 1774, 1291, 1198,  834,   28, 1467,  163,  150,  559,\n",
       "         952,  139, 1010, 1442,  553,  940, 1434, 1131,  332], dtype=int32),\n",
       " array([ 857,  961,  616, 1851, 1774, 1291, 1198,  834,   28, 1467,  163,\n",
       "         150,  559,  952,  139, 1010, 1442,  553,  940, 1434, 1131,  332],\n",
       "       dtype=int32),\n",
       " array([  44, 1301, 1945, 1774, 1319, 1874,  669,  973,   44,  563,  259,\n",
       "        1270,  834, 1798,  557, 1816, 1150,  762, 1848,  514, 1660,  505,\n",
       "         898, 1059, 1502,  979], dtype=int32),\n",
       " array([1197, 1412, 1049,  763,  834,  678,  448,  245,  322,   14, 1237,\n",
       "        1867], dtype=int32),\n",
       " array([1500, 1577,  901,   94,  800,  933,  255,  569,  834, 1781, 1819,\n",
       "         163, 1757, 1136,  622,  402, 1718, 1095,  322,  133,  183, 1909,\n",
       "          81], dtype=int32),\n",
       " array([1500, 1577,  616,  800, 1975,  834, 1781, 1819, 1356,  413,  621,\n",
       "        1136,  622,  402, 1874,  281,  828,  451,  812, 1630,  342],\n",
       "       dtype=int32),\n",
       " array([ 324,  992, 1608], dtype=int32),\n",
       " array([1197, 1945, 1774, 1319, 1874,  669,  973,   44,  563,  259, 1270,\n",
       "         834, 1798,  557, 1816, 1150,  762, 1848,  514, 1660,  505,  898,\n",
       "        1059, 1502,  979], dtype=int32),\n",
       " array([1103,  501,  542, 1185, 1701,  526, 1398, 1535, 1479, 1849, 1095,\n",
       "        1214, 1235, 1521, 1029], dtype=int32),\n",
       " array([1002, 1723,  977,  822], dtype=int32),\n",
       " array([1184,  798, 1715,  580, 1200,   90,  695,  337,  855, 1323, 1555,\n",
       "         685,  834, 1841,  822, 1614], dtype=int32),\n",
       " array([ 963, 1172,  409,  961, 1197, 1232, 1851, 1679, 1291,  961, 1259,\n",
       "        1198,  332,  834,   28, 1467, 1794,  163,  158,   14,  383, 1699,\n",
       "         394,  788, 1503, 1810,  286,  268], dtype=int32),\n",
       " array([1197,  996, 1412, 1611,  715,  993, 1816,  826, 1725,  111,  446],\n",
       "       dtype=int32),\n",
       " array([1750, 1080, 1726,  659,  740,  483, 1079, 1555,  362, 1294],\n",
       "       dtype=int32),\n",
       " array([ 888,  207,  390, 1030,  453,  658, 1518,  152, 1148, 1368,  931,\n",
       "         568, 1600, 1812,  834,  680,  691, 1075, 1014,   74,  771,   28,\n",
       "          14], dtype=int32),\n",
       " array([1660,  669,  762, 1406,  788,  600, 1982, 1283,   44], dtype=int32),\n",
       " array([ 837, 1046, 1406, 1339,  315, 1385], dtype=int32),\n",
       " array([ 707,  420,  279,  910, 1046, 1534,  371,  884,  834,  425, 1219,\n",
       "         885, 1200], dtype=int32),\n",
       " array([ 883,  116, 1841,  624, 1718, 1726, 1957, 1049, 1886,  669, 1085,\n",
       "         107,  548, 1177, 1243, 1089, 1261,  809,  834,  362], dtype=int32),\n",
       " array([1125, 1570, 1412,  745, 1614,  834, 1907,  552,  527,  583,  365],\n",
       "       dtype=int32),\n",
       " array([ 118, 1141,  455,   34,  413, 1062,  520, 1534, 1905,  381, 1713,\n",
       "         596,  834, 1755, 1201, 1136,   29,  805, 1703,  398], dtype=int32),\n",
       " array([ 616,  692,  877, 1438,  494,  905,  788,  246,   72, 1842, 1614,\n",
       "        1839,  223,  826, 1675,  246,  709,  924, 1117,  335], dtype=int32),\n",
       " array([1671,  200,  716, 1102, 1686, 1821, 1720, 1781, 1418, 1196, 1645,\n",
       "        1125,  336,   81,   28, 1747, 1280,  268, 1200,  195], dtype=int32),\n",
       " array([ 422,  560,   82,  657,  878, 1174, 1760,  362, 1418,  380,  934,\n",
       "         466, 1653,  451,   82, 1845,  640,  145, 1964,  349, 1402, 1614,\n",
       "         834], dtype=int32),\n",
       " array([ 616, 1969, 1058,  149, 1090,  501,  812,  548, 1703,  623,  848,\n",
       "        1188, 1614,  826], dtype=int32),\n",
       " array([ 616, 1495, 1048,  397,  908,  638,  826,  935,  374,  669,  691,\n",
       "        1555, 1338,  636,  451,   82, 1149,  337, 1094, 1537, 1053,   99,\n",
       "         214,  721, 1980,  339,  181,  256], dtype=int32),\n",
       " array([ 616, 1969, 1058,  149, 1090,  501,  812,  548, 1703,  623,  848,\n",
       "        1188, 1614,  826], dtype=int32),\n",
       " array([ 837, 1046, 1406, 1339, 1385], dtype=int32),\n",
       " array([ 616,  666, 1016, 1839,  332,  813,  925,  507, 1527, 1338, 1002],\n",
       "       dtype=int32),\n",
       " array([ 173,  260,  738, 1887, 1742, 1209, 1571,  669,   13,  463, 1081],\n",
       "       dtype=int32),\n",
       " array([ 833,  240,   64, 1463, 1958, 1588,  445], dtype=int32),\n",
       " array([1349,  316,  826], dtype=int32),\n",
       " array([1173, 1680, 1581,  940, 1614, 1528, 1412,  553, 1241,  787,  956,\n",
       "         364, 1542, 1021,  202,   58], dtype=int32),\n",
       " array([1958, 1588,  616, 1011, 1524,  696,  854, 1291,   64,  670,  362,\n",
       "         300,  583,   64, 1354,  312, 1614,  826,  661, 1616, 1011,  696,\n",
       "        1614,   64, 1992, 1958, 1588,  696,  456,  833, 1383], dtype=int32),\n",
       " array([ 963, 1172,  924, 1197,  425,  678, 1905, 1597,  783,  826, 1770,\n",
       "        1200, 1939, 1565,  357,   10, 1603, 1980,  279, 1965,  924,   60,\n",
       "         342], dtype=int32),\n",
       " array([ 666, 1571, 1137,  362,  209, 1862,  966,   40,  531,  835, 1884],\n",
       "       dtype=int32),\n",
       " array([ 616,  137,  177, 1576, 1867, 1666, 1455,  187,  826,  275,  860,\n",
       "         544,  202, 1766], dtype=int32),\n",
       " array([1173, 1630,  977,  214, 1183, 1799,  565, 1798,  179,  936,  922,\n",
       "        1608,   14, 1249,  548, 1112, 1614], dtype=int32),\n",
       " array([1696,  616,  137,  177, 1576, 1867, 1666, 1455,  187,  826,  275,\n",
       "         860,  544,  202, 1766], dtype=int32),\n",
       " array([ 936, 1442, 1168, 1609,  651,  669,  654, 1322,  301, 1798, 1198,\n",
       "         332], dtype=int32),\n",
       " array([1412, 1184,  798, 1715,  580, 1200,   90,  695,  337,  855, 1323,\n",
       "        1555,  685,  733, 1841,  822, 1614], dtype=int32),\n",
       " array([1696,  731,  744,  973, 1412,  745, 1468, 1454, 1701,  540, 1614,\n",
       "        1644,  733,   49, 1616, 1864,   28], dtype=int32),\n",
       " array([1165, 1140,  733, 1679,  972, 1211,  268, 1435, 1268,  342, 1196,\n",
       "        1766,  408,  580, 1982,  549,  828, 1867,  503], dtype=int32),\n",
       " array([ 750, 1140, 1377,  744,  527, 1377, 1947,  436,   79,  503, 1222,\n",
       "         733,  381,  834, 1892, 1268, 1457,  601, 1200, 1824, 1178,  548,\n",
       "        1330, 1664, 1617, 1956,  723, 1003, 1949, 1288,  689], dtype=int32),\n",
       " array([1500, 1577,  616,  362, 1802,  553, 1614,  826, 1136,  622,  402,\n",
       "        1616,  870, 1976, 1819, 1356, 1886,  910,   84, 1616, 1891],\n",
       "       dtype=int32),\n",
       " array([ 616, 1802, 1136,  622,  402,  870, 1976, 1356, 1886,  895,  122,\n",
       "        1936, 1246,  822,  826,  908, 1781], dtype=int32),\n",
       " array([1173,  216,  480, 1362,  240,   64, 1958, 1588, 1527,  696,  312,\n",
       "        1614,  854], dtype=int32),\n",
       " array([1500, 1577,  616, 1802, 1136,  622,  402,  870, 1976, 1356, 1886,\n",
       "         895,  310,  122, 1936,  898, 1941, 1246, 1208,  826], dtype=int32),\n",
       " array([ 910, 1546, 1045, 1970,  246,  328,  332, 1630, 1555], dtype=int32),\n",
       " array([ 825,  298, 1861, 1701,   54,  781, 1881,  944], dtype=int32),\n",
       " array([1500, 1577, 1136,  622,  402,  363,  870, 1645, 1976,  776, 1886,\n",
       "         895,  231,  122,  826, 1219, 1577,   84,  179,  401, 1757, 1136,\n",
       "        1021,  884,  394, 1463,  196,   68, 1571, 1261,  343, 1136,  315,\n",
       "         359], dtype=int32),\n",
       " array([ 118, 1141,  455, 1622, 1308, 1411, 1042,  315,  335, 1774, 1609,\n",
       "         983,  723,   34, 1070, 1062,  520, 1603,  279, 1965, 1841,  295,\n",
       "         342], dtype=int32),\n",
       " array([1197, 1268, 1921, 1101,  448, 1592, 1183, 1592, 1816, 1468, 1614,\n",
       "         826], dtype=int32),\n",
       " array([1173,  600,  761, 1744, 1902,  657,  785,  111,  246,  709, 1841,\n",
       "        1207], dtype=int32),\n",
       " array([1197, 1136,   29,  226,  555, 1211,  268, 1774,  457,  119, 1117,\n",
       "         635, 1886,   81,  548, 1982,  650, 1891, 1593, 1679, 1227, 1046,\n",
       "        1301, 1671], dtype=int32),\n",
       " array([1630, 1951,  170,  519,  900,  285,  479, 1107, 1579,  795,  936,\n",
       "         922,  677, 1527, 1101], dtype=int32),\n",
       " array([1197,  926, 1219, 1136,   29,  495,   81, 1139,  915,  436,  891,\n",
       "         271, 1886,  217,   81, 1841,   28,  334, 1630, 1556, 1766, 1672,\n",
       "        1297, 1724,  780], dtype=int32),\n",
       " array([1197, 1002,  977,  553, 1232,  123,  759, 1841,  694,  809,  956,\n",
       "        1839,  408], dtype=int32),\n",
       " array([1696,  936,  922, 1455,  187,  472, 1666,  153, 1172, 1304,  749,\n",
       "         137,  330, 1614, 1264, 1840,  337,  187,  190,  834], dtype=int32),\n",
       " array([1495,  616,  739,  765, 1393, 1449,  587, 1398, 1090, 1342, 1456,\n",
       "         288,  449,  457, 1969, 1058, 1119,  639,  307, 1797], dtype=int32),\n",
       " array([1173,  710,  902,  847, 1850,  494, 1162, 1554, 1241,  787,  249,\n",
       "          36,   36,  457, 1913, 1614, 1989, 1872], dtype=int32),\n",
       " array([1173, 1706,  669,  899,  553, 1398, 1881,  304,  275], dtype=int32),\n",
       " array([ 616,  822, 1324,   36,  794,  826,  337, 1289, 1609,  651, 1276,\n",
       "         286, 1200, 1232,  936], dtype=int32),\n",
       " array([1696,  616,  137,  177, 1576, 1867, 1666, 1455,  187,  826,  275,\n",
       "         860,  544,  202, 1766], dtype=int32),\n",
       " array([1802, 1136,  622,  402, 1891,  908, 1440,  908, 1246, 1976, 1819,\n",
       "        1356, 1886], dtype=int32),\n",
       " array([1075, 1648, 1210, 1527,  456, 1965,  377,  888], dtype=int32),\n",
       " array([ 616,    4,  596,  961, 1198,  332,  813, 1154,  383, 1699, 1274,\n",
       "         868, 1969, 1037, 1614, 1872,  981,  616,  826], dtype=int32),\n",
       " array([ 616, 1729, 1442, 1535,  383, 1699,  601, 1775, 1198,  752, 1313,\n",
       "         602, 1969, 1037, 1614,  576,  856, 1158,  699, 1751, 1798],\n",
       "       dtype=int32),\n",
       " array([ 576,  961,  616,    4,  596,  961, 1198,  332,  813, 1154,  383,\n",
       "        1699, 1274,  868, 1969, 1037, 1614, 1872,  981,  616,  826],\n",
       "       dtype=int32),\n",
       " array([1495,  616,  739,  765, 1393, 1449,  587, 1398, 1090, 1342,   38,\n",
       "         288,  449,  457, 1969, 1058, 1119,  639,  307, 1797], dtype=int32),\n",
       " array([1265,  897,  590, 1933, 1767,  895,  900,  272, 1172, 1265, 1412,\n",
       "        1748, 1244, 1553, 1855,  231,  834,  895,  182], dtype=int32),\n",
       " array([ 888, 1118,  681, 1426,  770,  664,  799, 1023, 1807, 1220,  553,\n",
       "         704], dtype=int32),\n",
       " array([1197,  822, 1383, 1011,  696,   64,  670, 1504, 1819, 1112, 1614,\n",
       "         826, 1527,  666, 1980, 1117,  851,  677], dtype=int32),\n",
       " array([1958, 1588,  670,  616, 1011, 1524,  696,  788,  209,  661,   64,\n",
       "         670,  284,  362,  300,   64,  312, 1614,  826], dtype=int32),\n",
       " array([ 616, 1136, 1351, 1829,   81, 1414, 1614,  826,  553, 1616,  617,\n",
       "         214,  620, 1270], dtype=int32),\n",
       " array([ 616,  900, 1693,  142, 1730,  752, 1544, 1137, 1243, 1204,  383,\n",
       "        1841,  304,  408,  198, 1614,  826], dtype=int32),\n",
       " array([1173,  337, 1799, 1595, 1215,  408, 1858, 1127,  337, 1115, 1088],\n",
       "       dtype=int32),\n",
       " array([ 137, 1576, 1867,  788,  885, 1455, 1666,  153, 1304,  749],\n",
       "       dtype=int32),\n",
       " array([ 616,  826,   78,  419, 1282,  565,   36, 1219,  885, 1200, 1065,\n",
       "         788,  556], dtype=int32),\n",
       " array([ 822, 1867, 1722,  697, 1012, 1717], dtype=int32),\n",
       " array([1136, 1891,  908, 1440, 1112, 1614,   84, 1614,  553, 1616],\n",
       "       dtype=int32),\n",
       " array([ 963, 1172, 1958, 1588,  670, 1197,  822, 1383, 1011,  696,   64,\n",
       "         670, 1504, 1819, 1112, 1614,  826,  666, 1980, 1117,  851,  677],\n",
       "       dtype=int32),\n",
       " array([1500, 1577,  616, 1577,  413,  621, 1136,  622,  402,  826,  177,\n",
       "         701, 1030, 1356, 1976, 1542,  434,  751, 1781,  122,  101,  401],\n",
       "       dtype=int32),\n",
       " array([1636,  702, 1598, 1606, 1540, 1701,  669,  362, 1595, 1762,  834,\n",
       "          85,   75,  680, 1555,   28,  362,  498,  271, 1230,  191,   53,\n",
       "        1590,  370, 1425,  416], dtype=int32),\n",
       " array([ 118, 1141,  455, 1622, 1308, 1411, 1042,  315,  335, 1774, 1609,\n",
       "         983,  826,  723,   34, 1070, 1062,  520, 1603,  279, 1965, 1841,\n",
       "         295,  342], dtype=int32),\n",
       " array([1412, 1748, 1453, 1240,  193,  826,  418, 1662,  884,  451,   82,\n",
       "         128,  545,  654, 1813, 1911, 1872,  458, 1825, 1994, 1348,  134,\n",
       "        1401, 1151,  660,  643, 1201,  908, 1527,  908,  193,  107,  677],\n",
       "       dtype=int32),\n",
       " array([1488,  944, 1139,  315,  924, 1354, 1485,  307,  944,  562, 1728,\n",
       "        1587,  364,  822, 1626], dtype=int32),\n",
       " array([1242, 1284,  897, 1980, 1701, 1716, 1201, 1955,  444,  486, 1521,\n",
       "        1119,  223, 1477, 1776], dtype=int32),\n",
       " array([ 963, 1172,  864, 1654, 1804, 1929, 1810, 1179, 1663, 1039,  301,\n",
       "         837, 1835, 1259,  255,  136,  499,  330], dtype=int32),\n",
       " array([ 616,  553,  913, 1658,  785,  763,  948, 1371,  451, 1830,  451,\n",
       "        1402, 1354, 1614,  826], dtype=int32),\n",
       " array([ 963, 1172,  796, 1654,  315,  533,  757,  553,  860, 1623, 1562,\n",
       "         212, 1614,  826], dtype=int32),\n",
       " array([ 883,  116,  732,  568,  231,   14,   85,   75, 1066,  213,  103,\n",
       "        1740], dtype=int32),\n",
       " array([1538,  616,  710, 1982,  826, 1619,  214, 1872,  426, 1342, 1166,\n",
       "         115, 1498, 1176,  437,   23, 1906, 1671], dtype=int32),\n",
       " array([1921, 1862,   40,   62, 1258,  730, 1283, 1927, 1549, 1671,  900,\n",
       "         826,   55, 1747,  822, 1040], dtype=int32),\n",
       " array([1103, 1136,   29,  226,  555, 1211,  268, 1774,  457,  119, 1117,\n",
       "         635, 1886,   81,  548, 1982,  650, 1891, 1593, 1679, 1227, 1046,\n",
       "        1301, 1671], dtype=int32),\n",
       " array([1848, 1301,  762, 1301,  315, 1660,  115,  826, 1392,  259, 1856,\n",
       "        1997,  409,  447,  764,  788, 1592, 1774, 1419, 1038,  255,  557,\n",
       "        1457,   44], dtype=int32),\n",
       " array([ 888,  207, 1726,  659, 1227, 1527,  771,  390, 1014,  246, 1882,\n",
       "        1079, 1890, 1865,  826, 1075,  504,  593,  659,  754, 1766,  907,\n",
       "         138,  875,   60,  740, 1961, 1792,  209,  320, 1885, 1890,  834,\n",
       "        1767], dtype=int32),\n",
       " array([ 971, 1788,  476,  666,  121, 1347, 1446,  845,  931,  618,   91,\n",
       "        1630, 1409,  826,  931,  202,  845,  119,  812, 1950], dtype=int32),\n",
       " array([ 118, 1141,  455, 1622, 1308, 1411, 1042,  315,  335, 1774, 1609,\n",
       "         983,  826,  255,   78, 1062,  520,  867,  613, 1561,  268, 1200,\n",
       "         413], dtype=int32),\n",
       " array([1412, 1748,  679,  451,   82, 1813,   33,  458, 1825,  134, 1478,\n",
       "        1412,  418, 1453, 1240,  826, 1930,   71,  197,  193, 1401, 1300,\n",
       "         936,  922, 1733, 1254, 1650], dtype=int32),\n",
       " array([ 616, 1526,  232,  330,  794, 1592,  900,  826,  109, 1201,  788,\n",
       "        1717,  276, 1956], dtype=int32),\n",
       " array([1197,  924, 1113, 1354, 1840, 1031,  357,  580,  826, 1597,  783,\n",
       "        1905,  381, 1565, 1200, 1227, 1939], dtype=int32),\n",
       " array([1848,  616, 1526,  232,  330,  794, 1592,  900,  826,  109, 1201,\n",
       "         788, 1717,  276, 1956], dtype=int32),\n",
       " array([1268, 1341, 1929,  457, 1587,  531,  308,  286], dtype=int32),\n",
       " array([1146,   63, 1498, 1176, 1854, 1223,  226, 1028,  886, 1347],\n",
       "       dtype=int32),\n",
       " array([ 616,  744,  580, 1090,  669, 1687,  897, 1978, 1463,  826, 1169,\n",
       "         822, 1614], dtype=int32),\n",
       " array([1197, 1937, 1412,  694, 1136,   29,  848,  826], dtype=int32),\n",
       " array([1496, 1978,  616,  744,  580, 1090,  669, 1687,  897, 1978, 1463,\n",
       "         826, 1169,  822, 1614], dtype=int32),\n",
       " array([1173,  271, 1508,  437, 1777, 1816, 1701,  526, 1773, 1255,  377,\n",
       "        1484], dtype=int32),\n",
       " array([1173, 1442, 1700, 1533,   84, 1810, 1022, 1146, 1291, 1131,  332,\n",
       "        1198,  669,  842,  163], dtype=int32),\n",
       " array([1197, 1969, 1037,  915, 1175, 1073,  799, 1323,  568, 1527,  818,\n",
       "        1554, 1304,  828, 1763, 1941, 1354, 1614,  826], dtype=int32),\n",
       " array([ 296,  422, 1232, 1726, 1957, 1427,  788, 1651,  544,  900,  826,\n",
       "         255, 1083,  839, 1341, 1078,  884,  788, 1708, 1886,  342, 1177,\n",
       "        1243], dtype=int32),\n",
       " array([ 137, 1614,  826,  854, 1119, 1451,  436,   90,   94,   52, 1865,\n",
       "         867, 1003,  497,  272, 1172, 1453, 1614,  170,  396, 1412,  758,\n",
       "        1451, 1943,  858], dtype=int32),\n",
       " array([  44, 1301, 1945, 1319,  669, 1330, 1816,   44, 1502,  557,  259,\n",
       "        1457, 1119, 1270,  826,  315, 1660, 1558, 1578, 1724, 1126,  202,\n",
       "        1814, 1237, 1546], dtype=int32),\n",
       " array([1103, 1136,   29,  826, 1626, 1398, 1774,  965,  119, 1117,  408,\n",
       "          69,   14, 1237, 1803, 1867], dtype=int32),\n",
       " array([1420, 1608, 1479, 1609,  651], dtype=int32),\n",
       " array([ 312, 1099,  664], dtype=int32),\n",
       " array([1136,   29, 1532, 1626, 1398, 1774,  965,  119, 1117], dtype=int32),\n",
       " array([1756, 1136,   29,  826, 1626, 1398,   90, 1774,  965, 1131,  119,\n",
       "        1117,  408,   69,   14, 1237, 1803, 1867], dtype=int32),\n",
       " array([ 296,  422, 1427, 1726, 1957,  826, 1341, 1974,  497,  472,  669,\n",
       "         900, 1694, 1708, 1886,  342, 1177, 1243], dtype=int32),\n",
       " array([1197, 1794, 1561, 1535, 1301, 1049, 1630, 1207, 1614,  826],\n",
       "       dtype=int32),\n",
       " array([1197,  401, 1577,  413, 1136,  622,  402, 1718, 1572,  559, 1872,\n",
       "        1841,  909, 1614,  826], dtype=int32),\n",
       " array([1197, 1720, 1558, 1136,   29,  146, 1301,  901,  436,  891,  131,\n",
       "        1002,  555], dtype=int32),\n",
       " array([  17, 1381,   20, 1062,  520,  983,  231,  486,  192, 1125,  532],\n",
       "       dtype=int32),\n",
       " array([  48,  739,  765, 1449,  587, 1626, 1398,  826,   52,   38,  739,\n",
       "        1354,  206,  542, 1841,  723, 1886,  426, 1109, 1545, 1253,  237,\n",
       "        1886, 1969, 1058], dtype=int32),\n",
       " array([1173, 1969, 1058, 1340, 1201, 1774,  119, 1117, 1701,  812,  577,\n",
       "        1136,  420,   29, 1614, 1298, 1086,  698, 1394, 1867,  730, 1335,\n",
       "         613, 1412], dtype=int32),\n",
       " array([ 888,  207,  823,   70, 1891,  907,  138, 1278, 1726,  659,  754,\n",
       "          60,  740, 1961,  261, 1527, 1150,  362, 1790, 1338,  320,  826,\n",
       "        1767,  759,  690, 1508, 1075, 1865, 1239, 1239, 1694,  806, 1750,\n",
       "        1080,  504, 1079], dtype=int32),\n",
       " array([ 137, 1614,  826,  854, 1119, 1451,  436,   90,   94,   52, 1865,\n",
       "         867, 1003,  497,  272, 1172, 1453, 1614,  170,  396, 1412,  758,\n",
       "        1451, 1943,  858], dtype=int32),\n",
       " array([1197,  936,  922,  865, 1062, 1713,  983,  268,  936,   61,  757,\n",
       "        1791, 1927,  757,  216,  772, 1842, 1614,  826], dtype=int32),\n",
       " array([1197,  482, 1206,  115,  826, 1969, 1037, 1301,  648, 1546, 1010,\n",
       "        1535, 1270,  557, 1816,   44], dtype=int32),\n",
       " array([ 194,  616, 1544, 1253, 1840,  826, 1876,  936,  922,  489,  676,\n",
       "        1168, 1540, 1028,  628, 1131,  194], dtype=int32),\n",
       " array([1173,  694,  469, 1676,  301,  139,  896, 1527,  852, 1881,   28,\n",
       "        1645, 1732, 1109, 1015,  264, 1238, 1701,  526], dtype=int32),\n",
       " array([1197, 1794, 1561, 1535, 1301, 1049, 1630, 1207, 1614,  826],\n",
       "       dtype=int32),\n",
       " array([1848,  616, 1774, 1034,  393, 1699,  563,  557, 1592, 1816,   44,\n",
       "         826, 1150, 1083, 1527,  799,  786, 1669, 1848, 1693,  664,  107,\n",
       "        1407,  493, 1241], dtype=int32),\n",
       " array([1173, 1946, 1296, 1614,  860,  675,  824,  168,  535,  145, 1373,\n",
       "         451], dtype=int32),\n",
       " array([1197, 1301, 1526, 1136,  805,  826,  940, 1777, 1301,  669, 1475,\n",
       "        1260,  491, 1982,  535], dtype=int32),\n",
       " array([1197, 1343,  778,   54, 1119, 1128, 1715, 1090,  234,  107,  677,\n",
       "        1614,  826, 1090,  613,  186,  472,  342], dtype=int32),\n",
       " array([1103, 1630, 1700, 1999, 1929, 1969, 1058, 1491, 1799, 1094, 1982,\n",
       "        1136,   29,  136, 1620, 1201, 1921, 1774,  457,  812,  577, 1378,\n",
       "        1112, 1614,  826,  403,  403,  306, 1891, 1249,  373, 1990, 1491,\n",
       "        1875], dtype=int32),\n",
       " array([1197,  362,   48,  331,  826, 1878, 1412,  315,   72,  860, 1371,\n",
       "         956,  704, 1701, 1881], dtype=int32),\n",
       " array([1197, 1946, 1112,  801,  826,  898,  759,  290, 1136,   29, 1236,\n",
       "        1211,  268, 1774, 1929, 1969, 1058, 1701,  812,  577], dtype=int32),\n",
       " array([1197,  209,  865, 1982,  758, 1339, 1385,  936,  922,  107, 1950,\n",
       "        1672, 1385,  895], dtype=int32),\n",
       " array([ 194,  616, 1544, 1253, 1840,  826, 1876,  936,  922,  489,  676,\n",
       "        1168, 1540, 1028,  628, 1131,  194], dtype=int32),\n",
       " array([1884, 1671,  616,  362,  209, 1137, 1671,  826, 1536,  677,  125,\n",
       "         602,  822, 1616, 1354], dtype=int32),\n",
       " array([1197,  624,  429, 1614,  826, 1249,  856,  924,  548,  618, 1508,\n",
       "        1467, 1687,  377,  440,  898, 1146], dtype=int32),\n",
       " array([  48, 1589, 1006,  765,  630, 1840,  289,  982, 1566], dtype=int32),\n",
       " array([1412, 1748,  679,  451,   82, 1813,   33,  458, 1825,  134, 1478,\n",
       "        1095,  418, 1453, 1240, 1930,   71,  197,  193,  826, 1933, 1908,\n",
       "        1650, 1401, 1151,  537,  936,  922, 1733,  193, 1933, 1650],\n",
       "       dtype=int32),\n",
       " array([1103, 1701, 1630, 1601,  155, 1305, 1010,  209, 1088,  596,   71,\n",
       "         382, 1732, 1765, 1822,  957,  925,  846, 1811, 1292, 1909,  936,\n",
       "        1531,  617], dtype=int32),\n",
       " array([ 422, 1137, 1021,  818], dtype=int32),\n",
       " array([1500, 1577, 1136,  622,  402,  870, 1976,  776, 1886,  895,  122,\n",
       "         826,   48, 1136,  424,  613,   81,  559,  794,   84, 1759, 1781,\n",
       "          31,  401], dtype=int32),\n",
       " array([ 616, 1036, 1270, 1592,   72, 1208,  535, 1546,  557, 1816,   44,\n",
       "         409,  694,  809, 1873], dtype=int32),\n",
       " array([ 616, 1036, 1270, 1592,   72, 1208,  535, 1546,  557, 1816,   44,\n",
       "         409,  694,  809, 1873], dtype=int32),\n",
       " array([1173,  885, 1200, 1041, 1523, 1527, 1981,  954,  413, 1645, 1182,\n",
       "        1821, 1676,  867,  415, 1232,  816,  669,  879], dtype=int32),\n",
       " array([ 963, 1172,  796, 1654,   28,  223,  757,  284,  330,  553,  159,\n",
       "         826,  275,  864,  524,  936, 1244, 1562,  732,  315,  533, 1909,\n",
       "         212, 1614, 1335,  408,  936, 1342,  255, 1937,  533,  839, 1426,\n",
       "        1864,  232,  330,   79], dtype=int32),\n",
       " array([1379, 1075,  899,  714], dtype=int32),\n",
       " array([ 963, 1172, 1654, 1387,  311,  747,  408,  924,  111,  451,  880,\n",
       "        1912, 1412,  984,  246,  924,  879], dtype=int32),\n",
       " array([1500, 1577,  800, 1558, 1751, 1136,  622,  402, 1718, 1837,   84,\n",
       "        1757, 1717,  408,  993,  266, 1596,  669, 1909,  281, 1055,  278,\n",
       "          84, 1669,  512,  342,  776,  402,  179,  401, 1527,  122, 1626,\n",
       "         826,   81, 1112, 1035,  878,  908, 1440, 1616, 1614,  496, 1819,\n",
       "         915,  344, 1718, 1983,   48, 1136,  424, 1380, 1921, 1058,  122,\n",
       "         834, 1781,  285,  408,  993, 1577, 1280,  794, 1759], dtype=int32),\n",
       " array([1412, 1748,  679,  451,   82, 1813,   33,  458, 1825,  134, 1714,\n",
       "        1578,   71,  197,  193,  826, 1453, 1240, 1933, 1908, 1650, 1401,\n",
       "        1151,  537,  878, 1733], dtype=int32),\n",
       " array([1197,   45,  507,  394,  826, 1717,  467,  286,   22,  231, 1786,\n",
       "        1243, 1964, 1527,    4,  208,  788, 1136, 1847], dtype=int32),\n",
       " array([ 419, 1702,  664, 1280, 1007,  740, 1873, 1562, 1782], dtype=int32),\n",
       " array([ 875, 1577, 1136, 1356, 1781,  434, 1934,  401], dtype=int32),\n",
       " array([1412, 1872, 1313, 1791, 1996,  398, 1906], dtype=int32),\n",
       " array([ 207, 1116,  457, 1961,  893, 1075,  821,   15, 1974,  830],\n",
       "       dtype=int32),\n",
       " array([1173,  414,  481,  961, 1593, 1907,  576, 1527,  553, 1237,  889,\n",
       "        1198, 1745, 1136,  409], dtype=int32),\n",
       " array([ 963, 1172,  686, 1197,  910, 1092, 1169, 1981,  954, 1523,  826,\n",
       "        1232, 1200,  816,  371,  216, 1112, 1614], dtype=int32),\n",
       " array([1197, 1883,  244, 1272, 1935,  336, 1508, 1412, 1570, 1935,  126,\n",
       "        1136,  844, 1758,  436,  891], dtype=int32),\n",
       " array([ 686,  616,  761,  312,  949, 1991,  142, 1873,  974,  483,  215,\n",
       "        1827, 1950, 1258,  905, 1729,  409, 1124,  710,  445, 1969, 1058],\n",
       "       dtype=int32),\n",
       " array([ 936,  922,  676, 1168, 1876,  194], dtype=int32),\n",
       " array([ 118, 1141,  455,  866,  596, 1062,  520,   78,  381, 1128,  322,\n",
       "         826, 1175, 1328,  867,  362, 1163, 1527,  279,  413, 1608],\n",
       "       dtype=int32),\n",
       " array([ 632,    0, 1159, 1968,  424,  142,  294,  329, 1022, 1428, 1701,\n",
       "         142, 1649], dtype=int32),\n",
       " array([1197,  826, 1649, 1288,  583, 1210,   14, 1039, 1982,  549, 1301,\n",
       "          23,  895,  501,  909, 1210, 1623,  861, 1210], dtype=int32),\n",
       " array([1412,  854, 1434,  819, 1727], dtype=int32),\n",
       " array([1500, 1577,  616, 1577,  413,  621, 1136,  622,  402, 1891, 1976,\n",
       "        1542,  826, 1877,  688,  434, 1219,  186, 1517,  570, 1881,  799,\n",
       "          80], dtype=int32),\n",
       " array([1197, 1339,  315, 1385, 1811,  922, 1630, 1418,  868,  757, 1816,\n",
       "         312,  862,  855, 1451, 1105,  202, 1845,  856, 1200,  596,  817,\n",
       "         254,  792], dtype=int32),\n",
       " array([1197, 1136,   29,  226, 1982, 1535,  668, 1214, 1630, 1700, 1999,\n",
       "         457, 1803, 1867, 1990, 1491,  698, 1996, 1990,  513, 1102,  829,\n",
       "        1378, 1112, 1614,  826], dtype=int32),\n",
       " array([ 888,  207,  823,   70, 1891,  907,  138, 1278, 1726,  659,  754,\n",
       "          60,  740, 1961,  261, 1527, 1150,  362, 1790, 1338,  320,  826,\n",
       "        1767,  759,  690, 1508, 1075, 1865, 1239, 1239, 1694,  806, 1750,\n",
       "        1080,  504, 1079], dtype=int32),\n",
       " array([ 289,  343, 1338,  305,  311,  223, 1926, 1229,  412, 1770, 1207,\n",
       "        1614,  757,  684,  514, 1461,  669, 1655,  545,  517,  826,  305,\n",
       "        1614, 1461,  514,  551,  775, 1047], dtype=int32),\n",
       " array([1197,  800, 1558, 1751, 1136,  622,  402, 1718, 1837,   84, 1757,\n",
       "        1717,  408,  993,  266, 1596,  669, 1909,  281, 1055,  278],\n",
       "       dtype=int32),\n",
       " array([1197, 1672,  126,  336,  436,  392,  805,  826, 1125,  109,  412,\n",
       "         936, 1244, 1200, 1249, 1064, 1906], dtype=int32),\n",
       " array([ 616, 1577,  413,  621, 1136,  622,  402, 1891, 1976, 1542,  826,\n",
       "        1877,  688,  434, 1219, 1517,  570, 1881,  799,   80], dtype=int32),\n",
       " array([ 963, 1172,  686, 1197,  436,  891,  711, 1966, 1575,  119, 1909,\n",
       "         936,  922, 1136,  420,   29, 1614,  826], dtype=int32),\n",
       " array([1197, 1802, 1136,  622,  402, 1891, 1405,  243, 1859,  826, 1976,\n",
       "         701, 1701,  434, 1351,  776, 1886], dtype=int32),\n",
       " array([  84, 1059, 1074, 1759,  122, 1585,  936,  828,  309,   69,  451],\n",
       "       dtype=int32),\n",
       " array([1126, 1456, 1439, 1463, 1909,  408, 1284, 1555,   20,  892, 1567,\n",
       "         744,  759, 1645, 1861,  951], dtype=int32),\n",
       " array([1816,  234,  867,  826,  794,  337,  703,   34, 1463, 1616,   40,\n",
       "         867,   57, 1352, 1090, 1163, 1527, 1062,  520,  381,  158],\n",
       "       dtype=int32),\n",
       " array([ 900, 1936,  104,  752, 1648,  898, 1204, 1313,  408, 1614,  826],\n",
       "       dtype=int32),\n",
       " array([1592, 1724, 1807,   44,  202, 1237, 1529,  884, 1010,  240,  600,\n",
       "        1535,  406, 1270], dtype=int32),\n",
       " array([ 379,  616,  924, 1833, 1412,  136, 1682, 1841,  810, 1614, 1873,\n",
       "        1487,  556, 1134,  249], dtype=int32),\n",
       " array([1339,  315, 1385, 1150, 1630, 1418,  868, 1560, 1537, 1050, 1155,\n",
       "        1064], dtype=int32),\n",
       " array([1197,  744,  215,  826,  831, 1827, 1950, 1028, 1325, 1112, 1614],\n",
       "       dtype=int32),\n",
       " array([1173,  548, 1330,  430, 1833,  248, 1715, 1200, 1609, 1117,  548,\n",
       "         972, 1488, 1881, 1112, 1614], dtype=int32),\n",
       " array([ 616, 1389,  789,  721, 1714, 1233, 1495, 1048,  643,  908,  638,\n",
       "        1873,  644, 1653,  451,   82, 1683,   71,  600,  431,  154, 1201,\n",
       "         545, 1965, 1458, 1228, 1063,  374], dtype=int32),\n",
       " array([ 950, 1859,  960, 1059,  972,   29, 1665], dtype=int32),\n",
       " array([1197, 1263,  495, 1717, 1162,  259, 1325, 1059,  895,  611,  826,\n",
       "         944, 1969, 1058, 1100], dtype=int32),\n",
       " array([ 910, 1546,  809, 1045, 1970,  246,  328,  332, 1527, 1630, 1679],\n",
       "       dtype=int32),\n",
       " array([ 616, 1603,  357, 1965,  924,   60,  342, 1715,  580, 1200, 1338,\n",
       "        1939, 1261, 1842, 1614, 1873], dtype=int32),\n",
       " array([1173,  806,   84, 1874,   81,  863,  208,  283,  498, 1993, 1630,\n",
       "        1412, 1035], dtype=int32),\n",
       " array([ 616, 1592,  799,  786, 1669,  524,  968,  557, 1816,   44, 1873,\n",
       "         107, 1407,  785, 1392, 1034,  393, 1856, 1237, 1715,  885,  451,\n",
       "        1390], dtype=int32),\n",
       " array([ 616,  807,  467, 1270, 1954,  981, 1630, 1950,  753,  531, 1136,\n",
       "        1549, 1614, 1873], dtype=int32),\n",
       " array([1848,  616, 1592,  799,  786, 1669,  524,  968,  557, 1816,   44,\n",
       "        1873,  107, 1407,  785, 1392, 1034,  393, 1856, 1237, 1715,  885,\n",
       "         451, 1390], dtype=int32),\n",
       " array([1173, 1184, 1315,  240,  334,   72,  821,  805,  677, 1412,   72,\n",
       "        1321,  623, 1021,  780,  407, 1524,  803,   72], dtype=int32),\n",
       " array([ 173,  260,  738, 1887,  178,  894], dtype=int32),\n",
       " array([1173, 1215, 1353, 1553,  930, 1237,  965, 1676,  139,  123, 1105,\n",
       "        1078], dtype=int32),\n",
       " array([1173,  227, 1022, 1037, 1680, 1581,  940,  286, 1316, 1177, 1679,\n",
       "         819,  600, 1241,  785], dtype=int32),\n",
       " array([ 616,  731, 1679, 1540, 1715, 1630, 1455, 1270, 1656,  831,  346,\n",
       "        1195,  746, 1873, 1455,  731,  389, 1614], dtype=int32),\n",
       " array([1696,  616,  731,  232,  330,  461,  303,  164,  552,  274,  596,\n",
       "         831, 1358, 1273,   47, 1827,  491, 1969, 1058,  799,  389, 1614,\n",
       "        1873], dtype=int32),\n",
       " array([1884, 1671, 1921, 1862,   40, 1283, 1927, 1549, 1671,  900,  733,\n",
       "          55,  240, 1747,  822, 1040, 1862,  125,  860,  847], dtype=int32),\n",
       " array([ 379, 1488,  944, 1862, 1354, 1485,  307,  944,  562, 1728,  508,\n",
       "        1587,  364,  508,  822, 1626], dtype=int32),\n",
       " array([1197, 1758,  337, 1546,  905, 1206, 1547, 1130, 1627,  213,  581,\n",
       "         485,  818, 1047, 1872, 1486, 1394,  788,  409,  367], dtype=int32),\n",
       " array([1197, 1430,  727,  548, 1330,  781, 1616, 1102,   90,  613, 1824,\n",
       "        1482, 1969, 1058, 1351,  573,  411], dtype=int32),\n",
       " array([ 616, 1456,  204, 1112,  602, 1169, 1575, 1215, 1742,    5, 1494,\n",
       "         989, 1881,  900, 1354, 1614, 1873], dtype=int32),\n",
       " array([1496,  616, 1978, 1979,  445, 1535, 1539, 1873, 1071,  502, 1879,\n",
       "        1463, 1741,  255, 1687,  897, 1527,  798,  940], dtype=int32),\n",
       " array([1197, 1546, 1010,  249, 1131, 1671, 1715,  362, 1699,   88, 1338,\n",
       "          60, 1112, 1614, 1873,  162, 1996,  553, 1774], dtype=int32),\n",
       " array([1197,  731,  553, 1873, 1194,  769, 1412,  596,  831, 1358, 1273,\n",
       "          47, 1354, 1614, 1711, 1171, 1241,  226, 1935,  346], dtype=int32),\n",
       " array([1322,  602,  731, 1846,  461,  303, 1536,  743, 1778, 1412,  596,\n",
       "        1273,   47, 1358], dtype=int32),\n",
       " array([ 240,  694,  820, 1754,   45, 1099, 1137,  457, 1816, 1242],\n",
       "       dtype=int32),\n",
       " array([ 616,  581, 1533, 1982, 1873,   38,  312, 1395,  442, 1111,  175,\n",
       "         107,  677], dtype=int32),\n",
       " array([1197,  572,  441,  818,  681,  137, 1614,  583, 1554,  514, 1873,\n",
       "        1862, 1835], dtype=int32),\n",
       " array([  36,    0,  354,   28, 1342, 1941], dtype=int32),\n",
       " array([1848, 1301, 1635, 1240,  354, 1848, 1873,  286, 1476, 1200, 1705,\n",
       "        1913,  594, 1112, 1614, 1592, 1112, 1616, 1827,  809, 1309,  377,\n",
       "         798, 1716], dtype=int32),\n",
       " array([ 616, 1700, 1614, 1873, 1105,  573,  669, 1917,  940,  506,  489,\n",
       "         676, 1168,  628, 1131,  194], dtype=int32),\n",
       " array([ 616, 1453, 1240, 1021, 1249, 1873, 1653,  451,   82,  811,  122,\n",
       "        1857,  681, 1051,  439, 1269,  948,  388, 1796, 1219,  545, 1640,\n",
       "        1742,  145], dtype=int32),\n",
       " array([ 797,  616,  194, 1700, 1614, 1873, 1105,  573,  669, 1917,  940,\n",
       "         506,  489,  676, 1168,  628, 1131,  194], dtype=int32),\n",
       " array([1197, 1921, 1456,  806, 1412,  226, 1535,  668, 1219, 1521, 1614,\n",
       "         947, 1399, 1491,  457, 1679, 1614, 1528,   48, 1803, 1867, 1742,\n",
       "        1768,  617], dtype=int32),\n",
       " array([ 616, 1136, 1873,  249,  924,  573,  785,  799,  869,  379,  623,\n",
       "        1886], dtype=int32),\n",
       " array([1848,  616, 1715,   60, 1200, 1848, 1240, 1873, 1535,  512,  180,\n",
       "         400, 1034,  393,  506,  557, 1816,   44], dtype=int32),\n",
       " array([1831,  515, 1412, 1748, 1200,  588,  596,   62], dtype=int32),\n",
       " array([ 379,  616,  692,  877,  736, 1799, 1464, 1570,  787,  778,  889,\n",
       "        1478, 1987, 1873,  924,  820,  787,  570,  699, 1751, 1903, 1543],\n",
       "       dtype=int32),\n",
       " array([1197,  731,  232,  330,  461,  303,  164,  769, 1194,  596,  831,\n",
       "        1273,   47, 1358, 1527, 1827, 1950, 1487, 1292, 1969, 1058, 1701,\n",
       "         985,  346, 1221], dtype=int32),\n",
       " array([1173,  321,  451,   82, 1625,  924,  617,  548,  556, 1789,  948,\n",
       "         193, 1256, 1515, 1580, 1823,  662, 1841, 1172, 1614], dtype=int32),\n",
       " array([1197,  514,  139, 1859,   48,  232,  330,  944, 1639,  886,   81,\n",
       "         286,  362, 1200,  312, 1614, 1873], dtype=int32),\n",
       " array([1696,  616,  232,  330,  461,  303,  164,  552,  274,  936,  731,\n",
       "         346,  596, 1692, 1273,   47, 1827,  491, 1969, 1058,  799,  389,\n",
       "        1614, 1873], dtype=int32),\n",
       " array([1197, 1185,  900, 1873,  101,  493,  748,  763,  342, 1549,  752,\n",
       "         115,   19], dtype=int32),\n",
       " array([1819,  711,  594, 1245, 1150,  337, 1767, 1888,  973], dtype=int32),\n",
       " array([1412, 1748,   71,   11,  605,  688, 1873,  916, 1201,  826,  451,\n",
       "        1070], dtype=int32),\n",
       " array([1414, 1545,  799, 1160,  379, 1201], dtype=int32),\n",
       " array([1928,   88,  933,  379,  594,  669, 1888], dtype=int32),\n",
       " array([  90,   78,  983, 1603, 1062,  342, 1227,  368, 1868,  480],\n",
       "       dtype=int32),\n",
       " array([1173, 1268, 1821,  364, 1463, 1619,  820, 1405,  268, 1516],\n",
       "       dtype=int32),\n",
       " array([ 616, 1037, 1547, 1989, 1987, 1498, 1873, 1969, 1058,  613, 1184,\n",
       "         513,  730, 1822,  372,   80], dtype=int32),\n",
       " array([1173, 1184, 1777,  624, 1874, 1432, 1698,  336, 1781, 1614,  613,\n",
       "        1982,  860, 1919, 1886], dtype=int32),\n",
       " array([1848,  616, 1724, 1592,  799,  786, 1669,   76,  255, 1848, 1982,\n",
       "        1873, 1329,   44,  785, 1237, 1034,  393,  764,  762, 1614],\n",
       "       dtype=int32),\n",
       " array([1197, 1965,  620, 1062,  520, 1841,  909, 1614, 1564], dtype=int32),\n",
       " array([1624,  616, 1946,  553, 1614, 1873, 1398, 1419,  255, 1774,   66,\n",
       "         119, 1999,  456, 1965,   96, 1675, 1629, 1537,  398], dtype=int32),\n",
       " array([ 888,  207, 1941, 1921,  653,  610, 1750,   13,  375, 1075,  504,\n",
       "        1079,  953, 1873,  183,  950,  898, 1616,  405, 1161,  758, 1889,\n",
       "         140,  890, 1114, 1704, 1752,  519, 1750, 1080,  269,  292, 1555,\n",
       "        1527, 1382, 1726,  659, 1079, 1891, 1080], dtype=int32),\n",
       " array([ 118, 1141,  455,  337, 1200,   57, 1371,   54,  983, 1062,  520,\n",
       "        1112,  736, 1418,  911,  368], dtype=int32),\n",
       " array([1219,  828,  410, 1369,  613, 1072,  834,  472,  740, 1126, 1088,\n",
       "        1595], dtype=int32),\n",
       " array([1884,  616, 1969, 1058,  952,  139, 1010, 1747,  506, 1717, 1468,\n",
       "        1131,  678, 1103,  481, 1884, 1614, 1873, 1952,  984,   75, 1614,\n",
       "        1948,  952,  139,  316,  736], dtype=int32),\n",
       " array([1866, 1912,  982, 1443,  484], dtype=int32),\n",
       " array([1173, 1308,  234, 1821,  692, 1939, 1527, 1981, 1281,  833, 1453,\n",
       "         867,  221,  381, 1182,   41, 1555,  839], dtype=int32),\n",
       " array([1197, 1260, 1050, 1357,  774,  232,  330,  514, 1126, 1873, 1537,\n",
       "         280,  985, 1203], dtype=int32),\n",
       " array([1173,  519, 1959,  277, 1510,  913, 1346,  197,  193,   37,  548,\n",
       "        1615,  556,   82, 1125,  720,  985, 1764,  156,  924,  662, 1614],\n",
       "       dtype=int32),\n",
       " array([1197, 1185,  900, 1873, 1492,   42,  451, 1207,  493,  553,  699,\n",
       "           1, 1763, 1874, 1565, 1321, 1979, 1521,   69,  451,  438],\n",
       "       dtype=int32),\n",
       " array([  44, 1301,  616, 1592,  799,  786, 1669, 1351, 1237, 1392, 1034,\n",
       "         393,  764,   76,  669,   44, 1873, 1119, 1774,  506,  512,  180,\n",
       "         400, 1656,  898, 1016, 1850,  254], dtype=int32),\n",
       " array([ 616, 1774, 1715,  974, 1700,  940, 1527,  681, 1183,  961, 1266,\n",
       "         163, 1700, 1614, 1873], dtype=int32),\n",
       " array([1173,  119, 1908,  838, 1012, 1142,  415, 1941, 1609], dtype=int32),\n",
       " array([ 888,  207, 1941, 1921,  653,  610, 1750,   13,  375, 1075,  504,\n",
       "        1079,  953, 1873,  183,  950,  898, 1616,  405, 1161,  758, 1889,\n",
       "         140,  890, 1114, 1704, 1752,  519, 1750, 1080,  269,  292, 1555,\n",
       "        1527, 1382, 1726,  659, 1079, 1891, 1080], dtype=int32),\n",
       " array([1103, 1823,   82,  548,  466, 1387, 1789,  397,  908,  719, 1275,\n",
       "        1789, 1060,  993, 1795,  172,    5,  553,  809, 1873, 1894,  909,\n",
       "        1614, 1795,  671,  423, 1823, 1373, 1458,   28,   14,  908,  679,\n",
       "         107,  217, 1795,  186, 1795, 1387, 1789], dtype=int32),\n",
       " array([ 616, 1969, 1058,  952,  139, 1010, 1747,  506, 1717, 1468, 1131,\n",
       "         678, 1103,  481, 1884, 1614, 1873], dtype=int32),\n",
       " array([1471, 1237, 1438,  240,  602], dtype=int32),\n",
       " array([  75, 1760,  526, 1709,   27, 1088, 1427, 1148,  828], dtype=int32),\n",
       " array([  44, 1301,  616, 1592,  799,  786, 1669, 1351, 1237, 1392, 1034,\n",
       "         393,  764,   76,  669,   44, 1873, 1119, 1774,  506,  512,  180,\n",
       "         400, 1656,  898, 1016, 1850,  254], dtype=int32),\n",
       " array([ 616,  862,  268,   50,  858, 1624,  993, 1182, 1873,  788, 1701,\n",
       "         692,  879], dtype=int32),\n",
       " array([1197, 1382, 1635,  884, 1681, 1250, 1684, 1351,  723, 1150,  839,\n",
       "        1210,  822, 1614], dtype=int32),\n",
       " array([1173,  172,  483,  215, 1258,  730,  686,  613, 1523, 1630, 1418,\n",
       "        1196, 1648, 1044, 1099,  613, 1100], dtype=int32),\n",
       " array([1538,  616, 1684, 1413,  710,  107, 1950, 1036,  409, 1124, 1428,\n",
       "        1873, 1935,  781,  900,  822, 1886], dtype=int32),\n",
       " array([ 312,  479, 1991,  172,  483,  730,  972, 1044, 1099], dtype=int32),\n",
       " array([1294, 1470,  445, 1527,  744,  669, 1865, 1003, 1977, 1572],\n",
       "       dtype=int32),\n",
       " array([ 548, 1330,  744, 1648,  286,   48,  854], dtype=int32),\n",
       " array([1848, 1301, 1301,  315, 1660,  833, 1724, 1241, 1399,   44, 1873,\n",
       "        1260, 1237, 1392,  259, 1856, 1997,  409,  447,  505,  770, 1502,\n",
       "         979,  315, 1660, 1848, 1979, 1024,  166,  606,  133,  286,   60,\n",
       "        1200, 1705, 1705,  895,  284, 1913], dtype=int32),\n",
       " array([1696,  731,  232,  330,  461,  303,  164,  769, 1194,  596,  831,\n",
       "        1273,   47, 1358, 1527, 1827, 1950, 1487, 1292, 1969, 1058, 1701,\n",
       "         985,  346, 1221,  228, 1194, 1171,  647,  226,  137,  901, 1194],\n",
       "       dtype=int32),\n",
       " array([1412, 1748,   71, 1637,  666,   49, 1349, 1873,  679, 1823,   82,\n",
       "        1795, 1653,  451,  510, 1873,  802,  916, 1979, 1480, 1149,  337,\n",
       "         948,  580,  693,  308, 1401, 1151,  660,  643,  244, 1760,  907,\n",
       "          28, 1342,  878, 1630, 1418], dtype=int32),\n",
       " array([ 731,  232,  330,  461,  303,  164,  769, 1194,  596,  831, 1273,\n",
       "          47, 1358, 1527, 1827, 1950, 1487, 1292, 1969, 1058, 1701,  985,\n",
       "         346, 1221], dtype=int32),\n",
       " array([ 409,   28, 1075,  454,  740,  483, 1974, 1091, 1507], dtype=int32),\n",
       " array([1649,   92,  723, 1237, 1546, 1010, 1699, 1486,  259, 1325, 1856,\n",
       "         464,  409,  447, 1592,  240, 1535, 1301,  557, 1816,   44,  895,\n",
       "         611,  557, 1773,  817], dtype=int32),\n",
       " array([ 216,  480, 1395,  930, 1821, 1873, 1200, 1527, 1981,  954, 1139,\n",
       "        1453,  867, 1690,  381], dtype=int32),\n",
       " array([ 924, 1617,  692,  877, 1464, 1570,  792, 1639,  812,  166,  807,\n",
       "         944,  778,  787,  379], dtype=int32),\n",
       " array([1495, 1456,   38,  430, 1398,   48,  828,  739,  765, 1449,  587,\n",
       "         136], dtype=int32),\n",
       " array([1538,  710,  102,  409, 1124, 1093, 1522, 1706, 1950, 1616,   23,\n",
       "        1162, 1656,  710, 1747,  651, 1490,  613,  730,  986], dtype=int32),\n",
       " array([ 268, 1200,  381,  581, 1609, 1491,  301, 1406,  788,  637],\n",
       "       dtype=int32),\n",
       " array([607, 784, 456,  34, 456, 868, 680, 879], dtype=int32),\n",
       " array([  36, 1261,  716, 1412, 1649,   28, 1342,   14, 1680, 1241, 1903,\n",
       "         382, 1676, 1595, 1581, 1986, 1676, 1273, 1487, 1292, 1560, 1794,\n",
       "          80], dtype=int32),\n",
       " array([ 580, 1715, 1025, 1010,  249,  246,  425], dtype=int32),\n",
       " array([ 337,  489, 1715,  194, 1003,  526, 1876,  826, 1053,   22, 1150,\n",
       "        1039,   48,  132,  681,  965,  553, 1672,  784, 1917], dtype=int32),\n",
       " array([1696,  616,  232,  330,  461,  303,  164,  274,  941,  936,  731,\n",
       "         832,  596, 1692, 1273,   47, 1398, 1827,  491, 1103,  799,  389,\n",
       "        1614, 1873], dtype=int32),\n",
       " array([  48, 1921,  871, 1862,  730,  986, 1671,  331, 1884,  900,  885,\n",
       "         106, 1464,  240,  553], dtype=int32),\n",
       " array([  21, 1215,  583, 1900,  826,   63, 1412,  663,  199,  362,  548,\n",
       "        1330], dtype=int32),\n",
       " array([1649, 1680, 1873, 1581,  940, 1036,  546,  276, 1324,  898,  501,\n",
       "        1913, 1614], dtype=int32),\n",
       " array([ 462, 1257,  462, 1257,  553,  822,  142,  215], dtype=int32),\n",
       " array([ 907,   84, 1717,  907,  232,  330,  102,  553,  972, 1846, 1718,\n",
       "         337, 1282,  734,  540], dtype=int32),\n",
       " array([ 616, 1706, 1648,  172,  483, 1873, 1044, 1099,  613,  905, 1243,\n",
       "        1095, 1398, 1980, 1412, 1748, 1926,  775,  905, 1950], dtype=int32),\n",
       " array([1173, 1341, 1490,  681, 1799, 1010,   36,  710], dtype=int32),\n",
       " array([ 686,  616, 1706, 1648,  172,  483, 1873, 1044, 1099,  613,  905,\n",
       "        1243, 1095, 1398, 1980, 1412, 1748, 1926,  775,  905, 1950],\n",
       "       dtype=int32),\n",
       " array([1696,  616,  232,  330,  461,  303,  164,  274,  941,  936,  731,\n",
       "         832,  596, 1692, 1273,   47, 1398, 1827,  491, 1103,  799,  389,\n",
       "        1614, 1873], dtype=int32),\n",
       " array([1624,  616, 1491, 1614, 1873, 1398, 1419,  255, 1774,   66,  119,\n",
       "        1999, 1616, 1965,   96, 1675, 1629, 1537,  398], dtype=int32),\n",
       " array([  36,   28,   74, 1923, 1120, 1885, 1075, 1792], dtype=int32),\n",
       " array([1696,  731,  553, 1334,  769, 1873, 1412,  596,  831, 1273,   47,\n",
       "        1358, 1194, 1103,  243, 1909, 1303, 1171, 1241,  226], dtype=int32),\n",
       " array([ 888,  207, 1518,  152, 1548,  165,  100, 1398,  683, 1873, 1767,\n",
       "        1075, 1368,  931,  515,  884, 1245, 1014, 1600, 1812,  242],\n",
       "       dtype=int32),\n",
       " array([1848, 1301,  214, 1724, 1291,   44,  442, 1873, 1392,  259, 1856,\n",
       "        1997,  409,  447, 1241, 1399, 1086,  543, 1046,  557, 1816,  162,\n",
       "        1724, 1996, 1546,   44,  580,  408], dtype=int32),\n",
       " array([1412, 1748,   71, 1637,  666,   49, 1349, 1873,  679, 1823,   82,\n",
       "        1873,  802,  916, 1979, 1480, 1149,  337,  948,  580,  693,  308,\n",
       "        1275,   82,  970,  172, 1950,    7, 1823, 1373, 1458, 1720, 1376,\n",
       "         679,  719,  956,  466, 1387,   82], dtype=int32),\n",
       " array([1696,  616,  232,  330,  461,  303,  164,  274,  941,  936,  731,\n",
       "         832,  596, 1692, 1273,   47, 1398, 1616, 1827,  491, 1103,  799,\n",
       "         389,  602], dtype=int32),\n",
       " array([  48,  102,  906,  909,  107, 1767, 1171,   48, 1726, 1583, 1250,\n",
       "         555,  436, 1915, 1701, 1282,  734,  540], dtype=int32),\n",
       " array([1624,  616, 1946,  876,   21,  112,  518,  199,  548, 1330, 1185,\n",
       "         892, 1946,  809], dtype=int32),\n",
       " array([1173, 1896, 1614, 1616,  375,   28, 1806,  722, 1014,  337,  908,\n",
       "        1675,  675, 1789,  162,  698], dtype=int32),\n",
       " array([1201,  443,  860, 1094], dtype=int32),\n",
       " array([ 436,  891,  437, 1832, 1817, 1790, 1832, 1119, 1955, 1200, 1820,\n",
       "        1167,  886, 1709], dtype=int32),\n",
       " array([ 616,  915,  629,  895,  761, 1872], dtype=int32),\n",
       " array([ 168, 1223, 1546,  747,  809,  529, 1045,  565,  784,  246,  328,\n",
       "        1798, 1083, 1527, 1045,  246,  328, 1569,  984, 1676, 1966, 1794],\n",
       "       dtype=int32),\n",
       " array([ 616,   34,  349,  635, 1421,   20, 1993,  463, 1804, 1969, 1920,\n",
       "         937,  684,  463,  365, 1614, 1873], dtype=int32),\n",
       " array([1802, 1717,  622,  402,  434,  401, 1976, 1891], dtype=int32),\n",
       " array([  44, 1301,  616, 1592, 1241, 1724,  794, 1392, 1034,  393, 1856,\n",
       "         555,  884,  764, 1816,   44, 1119,  240, 1774,  506, 1667, 1656,\n",
       "         898, 1016, 1850,  254], dtype=int32),\n",
       " array([1467,  330,  615, 1532,  848,  787,  692,  709, 1112,  202, 1488,\n",
       "        1841,  679,  778,  546,  533], dtype=int32),\n",
       " array([ 713,  633, 1649, 1924, 1183, 1810,  513, 1873, 1304, 1314, 1769,\n",
       "         924,  758,   36, 1572, 1936, 1237, 1630, 1555, 1498], dtype=int32),\n",
       " array([ 616,   36,   28, 1885,  888, 1294, 1873,   28,   74, 1923, 1120],\n",
       "       dtype=int32),\n",
       " array([1197, 1672,  944, 1112, 1754,  781, 1873, 1684,  217,   96, 1527,\n",
       "        1626,  143, 1508, 1090, 1163, 1827], dtype=int32),\n",
       " array([1848, 1301, 1656,   44, 1546,  259, 1270, 1873], dtype=int32),\n",
       " array([ 888,  207, 1726,  659,  955,  785, 1007,   60,  740, 1961, 1754,\n",
       "        1201,  666,  701,  888, 1792,  863,  504,  593, 1873, 1767, 1075,\n",
       "         875, 1342,  659,  397, 1150,  693, 1088,  828,  875, 1077,  884,\n",
       "        1007,  725], dtype=int32),\n",
       " array([ 888,  207, 1941,  614, 1895, 1111,  255,  666,  166, 1075, 1306,\n",
       "        1873, 1767,  631, 1809,  589, 1302,  784, 1895, 1891, 1760, 1630,\n",
       "         342, 1423,  378, 1067,  278,  844, 1638], dtype=int32),\n",
       " array([1884, 1862,   40, 1927,  966, 1276, 1137, 1884,  340,  911, 1571,\n",
       "         899, 1616,  125, 1921,   64], dtype=int32),\n",
       " array([ 268, 1116, 1406,  788,  637, 1946, 1463,  733,  553,  860,  664,\n",
       "         747, 1659,  886,  936,  617], dtype=int32),\n",
       " array([1197, 1672,  944, 1112, 1754,  781, 1873, 1684,  217,   96, 1527,\n",
       "        1626,  143, 1508, 1090, 1163, 1827], dtype=int32),\n",
       " array([ 522, 1194, 1913, 1873, 1644,  274,  337, 1412,  522, 1273,   47,\n",
       "        1358], dtype=int32),\n",
       " array([ 548, 1330, 1455, 1002, 1540,   50,  731, 1679, 1003,  669, 1273,\n",
       "          47, 1816,  895, 1455, 1888,  389], dtype=int32),\n",
       " array([ 542,  501,  387,  936,  605,   72,  922,  818, 1047], dtype=int32),\n",
       " array([1848, 1301,  408,  240,  871, 1715,  606,  855,  695, 1025, 1592,\n",
       "        1034,  393, 1699, 1848,  826,  240, 1982, 1614, 1873], dtype=int32),\n",
       " array([1173,  240, 1982,  955, 1322, 1671, 1616, 1547,  665, 1546, 1273,\n",
       "         898], dtype=int32),\n",
       " array([1173, 1330,  744,  319,  865,  624,  594,  951,  354, 1412],\n",
       "       dtype=int32),\n",
       " array([1173, 1672,  336,  436,  891,   41,  246, 1810,  666,  518,  891,\n",
       "         239, 1215,  331, 1166], dtype=int32),\n",
       " array([ 888,  207,  754,  907,  138,  255, 1595, 1726,  659,  382, 1874,\n",
       "         863,  740,  548, 1330, 1448,  179,  666,  740, 1885,  888, 1792,\n",
       "         504,  593, 1873, 1767, 1303,  755, 1766, 1088,  828,  875],\n",
       "       dtype=int32),\n",
       " array([ 172,  483,  900,  962, 1862, 1497, 1100, 1706, 1969, 1058],\n",
       "       dtype=int32),\n",
       " array([1649, 1924,  553,  194, 1969, 1037, 1021, 1201,  628, 1146, 1291,\n",
       "         489, 1168,  654, 1028,   35,  628,  194,  107, 1564,  414, 1112,\n",
       "        1614, 1126], dtype=int32),\n",
       " array([1624, 1946,  876,   21,  112,  518,  199, 1354,  112,  944, 1614,\n",
       "        1873,  112,  518,  112, 1754, 1201,  548, 1330, 1185,  892, 1723,\n",
       "         606, 1418, 1196], dtype=int32),\n",
       " array([ 694, 1701,  526, 1035,  681, 1851, 1679, 1861, 1435, 1609, 1774,\n",
       "         743, 1810, 1022], dtype=int32),\n",
       " array([ 408, 1367, 1947,  436,   79,  503, 1003,  526, 1817, 1268, 1457,\n",
       "        1261,  810,  826,  275, 1499, 1664, 1742,  856], dtype=int32),\n",
       " array([1197, 1239,  356,  212,  351,   93,   27,  820, 1706, 1199, 1749,\n",
       "           3,  358,  874,  761, 1301, 1867], dtype=int32),\n",
       " array([1649, 1924, 1969, 1037,  104,  752,  247, 1533,  801, 1701,   22,\n",
       "          35,  628,  194,  107, 1564,  414, 1354, 1614, 1126, 1873],\n",
       "       dtype=int32),\n",
       " array([ 232,  330,  436, 1915, 1614,  860,  936,  223,  515,  705, 1282,\n",
       "         734, 1408], dtype=int32),\n",
       " array([1624, 1491, 1197, 1491,  501,  330, 1533, 1873,  337,  240, 1398,\n",
       "        1535,  668,  904,  913, 1965, 1491, 1630,  910, 1999, 1456,  806,\n",
       "         904], dtype=int32),\n",
       " array([1197,  974,    5, 1301,   14,  490,  910, 1031, 1921, 1136,   29,\n",
       "          96,  968, 1867,  107,  123,  124,  223,  847,   29,  965,  223,\n",
       "        1819, 1867,  165, 1925, 1819], dtype=int32),\n",
       " array([ 240,  906,  542,  240,   72,  280,  275, 1412], dtype=int32),\n",
       " array([1239,  356,  212,  351,   93,   27,  820, 1706, 1199, 1749,    3,\n",
       "         358,  874,  761], dtype=int32),\n",
       " array([1173,   36,  924, 1471,  828,  759,  898, 1292,  908,  812,  342,\n",
       "         536,  936,  922, 1763, 1200,  240,  741, 1614], dtype=int32),\n",
       " array([  48, 1409, 1476,  548,  435, 1244, 1345], dtype=int32),\n",
       " array([1103, 1823,   82,  397,  908,  719, 1275,   82,  657, 1219, 1277,\n",
       "         446,  522, 1194, 1060, 1229,  449,  617, 1908, 1894,  909, 1614,\n",
       "        1873, 1795,  671,  423, 1823, 1373, 1458,   28,   14,  908,  679,\n",
       "         107,  217, 1616, 1201,  908, 1615, 1909,  908, 1742, 1732, 1387,\n",
       "        1789], dtype=int32),\n",
       " array([1412, 1748, 1338, 1310,  451,   82,  196, 1070, 1453, 1240, 1873,\n",
       "        1303,   71, 1346, 1389,    9,  426, 1823, 1373, 1872, 1979,   90,\n",
       "         916,  948, 1854, 1148, 1656,  308,   71, 1784, 1349, 1764, 1527,\n",
       "         553, 1823, 1373, 1458, 1720, 1376,  679,  719,  956,  466, 1387,\n",
       "          82], dtype=int32),\n",
       " array([ 686, 1649, 1924,  761,  312, 1873, 1648,  172,  905,  986, 1044,\n",
       "        1099], dtype=int32),\n",
       " array([1173, 1921, 1810,  915,  240,  757, 1630, 1841,  240,  334,  177],\n",
       "       dtype=int32),\n",
       " array([ 686,  616, 1706, 1648,  172,  905,  986, 1873, 1614,  408, 1616,\n",
       "        1398, 1943,  730, 1554, 1422,  995, 1412, 1748, 1412,  745, 1103],\n",
       "       dtype=int32),\n",
       " array([ 806, 1467,  330,  692,  877, 1841,  778,  546], dtype=int32),\n",
       " array([1197, 1969, 1058, 1873, 1819,  137, 1398,  941, 1412,  596,  831,\n",
       "        1358, 1273,   47,  600, 1822,  372, 1241,  226], dtype=int32),\n",
       " array([1696,  616,  232,  330,  461,  303, 1786,  944, 1701,  226, 1431,\n",
       "        1195,  746,  164,  936,  731,  832,  596, 1273,   47, 1298, 1745,\n",
       "         605,  535, 1873, 1103], dtype=int32),\n",
       " array([  44, 1301,  616, 1301, 1392, 1856,  464,  409,  447, 1074, 1873,\n",
       "        1335, 1724, 1592,  799,  786, 1669, 1824, 1237,  557, 1816,   44,\n",
       "         613,  898], dtype=int32),\n",
       " array([1696,  616,  232,  330,  461,  303, 1786,  944, 1701,  226, 1431,\n",
       "        1195,  746,  164,  936,  731,  832,  596, 1273,   47, 1298, 1745,\n",
       "         605,  535, 1873], dtype=int32),\n",
       " array([ 963, 1172,  796, 1654, 1251, 1227,  456, 1804, 1465,  162, 1827,\n",
       "         548, 1383,  860, 1614, 1873], dtype=int32),\n",
       " array([1696,  616,  232,  330,  461,  303, 1786,  944, 1701,  226, 1431,\n",
       "        1195,  746,  164,  936,  731,  832,  596, 1273,   47, 1298, 1745,\n",
       "         605,  535, 1873, 1103], dtype=int32),\n",
       " array([ 207,  419,  726,  931, 1678,  680,  804, 1873, 1966,  382, 1366,\n",
       "         788,  931, 1616,  405,  608], dtype=int32),\n",
       " array([1242,  757,  435, 1993, 1815, 1468, 1626, 1873, 1841,  860, 1234,\n",
       "          40,  851, 1950,  723,   34,  463, 1804,  937, 1969, 1920, 1841,\n",
       "        1891,  463, 1398,  338,  963, 1207, 1841,   34, 1467,  164, 1841,\n",
       "         284], dtype=int32),\n",
       " array([ 686,  761,  312, 1648,  172, 1100,  986, 1873, 1604,  603, 1099,\n",
       "         494,  591,  439, 1845, 1154,  714,   19,  184,  323, 1941,  409,\n",
       "         367, 1093,  973,  730, 1539, 1950], dtype=int32),\n",
       " array([  44, 1301,  616, 1301, 1392, 1856,  464,  409,  447, 1074, 1873,\n",
       "        1335, 1724, 1592,  799,  786, 1669,  147, 1886,  555,  884,  764,\n",
       "         557, 1816,   44,  613, 1906], dtype=int32),\n",
       " array([1183, 1810, 1669, 1693, 1021,  562, 1086,  359, 1672, 1542,  332],\n",
       "       dtype=int32),\n",
       " array([1103,  409,  408,  730,  986, 1648, 1497, 1028,  240, 1422,   36,\n",
       "         761,  312, 1873, 1648, 1398], dtype=int32),\n",
       " array([ 686,  761,  312, 1614, 1532,  408, 1616, 1210, 1981,  190, 1041,\n",
       "        1523, 1308,  549], dtype=int32),\n",
       " array([ 408,   44, 1430, 1541, 1546], dtype=int32),\n",
       " array([ 553, 1240, 1475, 1880], dtype=int32),\n",
       " array([1792, 1050, 1386, 1505], dtype=int32),\n",
       " array([1206,  670, 1201,  908], dtype=int32),\n",
       " array([1799, 1799, 1799, 1412], dtype=int32),\n",
       " array([1535, 1539, 1639, 1021,  940, 1015, 1879, 1463], dtype=int32),\n",
       " array([1197, 1309,  803, 1195, 1816,  231,  282, 1715,  580, 1630, 1200,\n",
       "        1455, 1951, 1614, 1983, 1360, 1169], dtype=int32),\n",
       " array([1195,  616, 1309, 1690,  381,  457,  102, 1646, 1162, 1195,  282,\n",
       "         286,  580, 1090, 1200, 1705, 1886,  337,  295,  613,  921, 1614],\n",
       "       dtype=int32),\n",
       " array([ 616,  961, 1398, 1974,  830,  312, 1198,  898, 1467,  330, 1614,\n",
       "        1873, 1078, 1223, 1322,  383, 1699, 1227, 1168], dtype=int32),\n",
       " array([1103, 1456, 1491,   90,  613, 1290,  513, 1078, 1128,  308,  936,\n",
       "         663, 1282,  656, 1177,  602, 1219,  828,  913,  380,  443,  908,\n",
       "        1456,  290,  908, 1701,  812, 1806,  895,  388, 1936, 1873, 1927],\n",
       "       dtype=int32),\n",
       " array([ 888,  207,  965, 1865, 1726,  659,   52,  884, 1873, 1767, 1969,\n",
       "        1058,  249,  214, 1676,  759, 1508,  888, 1792], dtype=int32),\n",
       " array([  48,  757, 1354, 1171,  241,  396,  331,  436, 1915,  362, 1163,\n",
       "         839,   81, 1282,  734,  396], dtype=int32),\n",
       " array([1173, 1706, 1075, 1260,  740, 1885, 1716,  677], dtype=int32),\n",
       " array([  44, 1301,  616, 1392, 1856,  464,  409,  447, 1873, 1074, 1335,\n",
       "        1592, 1241, 1724,  899,  508, 1921,  508,  240, 1774,  506, 1667,\n",
       "          44, 1656,  898, 1016, 1850,  254], dtype=int32),\n",
       " array([1696, 1197,  731,  553, 1873, 1334,  769, 1412,  596,  831, 1273,\n",
       "          47, 1358, 1711, 1171, 1241,  226, 1194,  243,  408,  240],\n",
       "       dtype=int32),\n",
       " array([1195,  616, 1309, 1690,  381,  457,  102, 1646, 1162, 1195,  282,\n",
       "         286,  580, 1090, 1200, 1705, 1886,  337,  295,  613,  921, 1614],\n",
       "       dtype=int32),\n",
       " array([1197, 1802,   81, 1112, 1693, 1873,  122, 1356, 1976, 1891, 1136,\n",
       "         622,  402, 1487,   84, 1243,   81,  401, 1112,  331,  272, 1172],\n",
       "       dtype=int32),\n",
       " array([1195,  616, 1309, 1690,  381,  457,  102, 1646, 1717, 1195,  282,\n",
       "         286,  580, 1090, 1200, 1705, 1886,  337,  295,  613,  921, 1614],\n",
       "       dtype=int32),\n",
       " array([1404,   60, 1566, 1282, 1116, 1874, 1969, 1058, 1177,  480,  981,\n",
       "         216, 1085, 1282,  826, 1282,  432, 1716,  472, 1482, 1301, 1927],\n",
       "       dtype=int32),\n",
       " array([ 888,  207,   75,  886,  604,  683,  207, 1886,  326,  253,  375,\n",
       "        1969, 1058,  208, 1630, 1733,  209, 1891, 1873, 1767,  666,  166,\n",
       "        1075,  483, 1368, 1306,  395, 1235, 1388,  922,  913,  209, 1874,\n",
       "        1245, 1014, 1600, 1812,  242,  860, 1014, 1697, 1143,  342],\n",
       "       dtype=int32),\n",
       " array([ 888,  207,  965, 1865, 1726,  659,   52,  884, 1873, 1767, 1969,\n",
       "        1058, 1941,  249,  759, 1508,  888, 1792], dtype=int32),\n",
       " array([ 379,  924, 1614,  733,  276,  692,  877,  255,  807,  944,  787,\n",
       "         162, 1103,  146,   75,   58], dtype=int32),\n",
       " array([1412, 1748,  414,  542,  624, 1873, 1113,  737,  337, 1333,  647,\n",
       "         979,  794, 1881, 1982, 1669, 1983,  142, 1129,  979, 1717,  289],\n",
       "       dtype=int32),\n",
       " array([1197, 1860, 1668,  482, 1873, 1614,  860,  137,  778,  831, 1841,\n",
       "        1028,  482,  778,   18], dtype=int32),\n",
       " array([1173,  268, 1200, 1941,  645, 1288,  437, 1095,  489,   35,  628,\n",
       "         194], dtype=int32),\n",
       " array([1884, 1747, 1136, 1432, 1698,  623, 1817,  710,  168,  826,  389,\n",
       "        1614, 1873], dtype=int32),\n",
       " array([1257, 1649,   10,   81,  213, 1126, 1049, 1532, 1236, 1672,  944,\n",
       "         408, 1616,  215,  924, 1243], dtype=int32),\n",
       " array([1195,  616,  102, 1646, 1717,  870,  731,  253,  282,  580, 1476,\n",
       "         613,  921, 1715, 1767, 1342, 1309, 1690,  381, 1195,  457],\n",
       "       dtype=int32),\n",
       " array([ 203, 1136,  206,  828, 1669, 1873, 1405, 1660,   80, 1970,  246,\n",
       "         328, 1119,  923, 1867], dtype=int32),\n",
       " array([1649,   92, 1717,  806, 1266,   84, 1894,  944,  907,  895,  611,\n",
       "        1936,  944], dtype=int32),\n",
       " array([1523, 1308,  234, 1821,  692, 1873, 1939, 1527, 1981, 1281,  833,\n",
       "        1453,  867,  221,  381, 1182,   41, 1555,  839], dtype=int32),\n",
       " array([1197, 1136,   29, 1614, 1873,  553,  257, 1036, 1815], dtype=int32),\n",
       " array([1197, 1267,  824, 1385, 1831], dtype=int32),\n",
       " array([1197, 1873, 1649, 1288,  583, 1210,   14, 1039, 1982,  549, 1301,\n",
       "          23,  895,  501,  909, 1210, 1623,  861, 1210], dtype=int32),\n",
       " array([ 616,   48, 1853,  710,   36, 1201,   28,  888, 1294, 1873, 1133,\n",
       "          40, 1792], dtype=int32),\n",
       " array([ 576, 1649, 1422,  732,  830, 1982,  559, 1198,  332,  961, 1819,\n",
       "        1442,  553, 1969, 1037,  102,  414], dtype=int32),\n",
       " array([1491,  876,   21,  518,  199,  362,  548, 1330, 1185,  892,  813,\n",
       "        1701, 1491, 1892, 1188,   63,  733], dtype=int32),\n",
       " array([ 232,  330,  436, 1915, 1923,  759,  572,  232,  330,  436, 1614],\n",
       "       dtype=int32),\n",
       " array([1197, 1630, 1163, 1527,  413, 1095, 1947,  436,   79,  503, 1227,\n",
       "        1873, 1626, 1937, 1499, 1664,  812], dtype=int32),\n",
       " array([1103, 1649, 1924,  240,  542, 1841,  115, 1185, 1467,  330, 1701,\n",
       "          80, 1045, 1970,  332,  246,  328, 1841,  909, 1992,  298, 1614,\n",
       "        1873], dtype=int32),\n",
       " array([ 576, 1649, 1924, 1442,  553, 1614, 1873, 1338, 1198, 1775,  158,\n",
       "        1676,  984, 1969, 1037], dtype=int32),\n",
       " array([ 686,  761,  312, 1648,  172, 1100,  986, 1873, 1604,  603, 1099,\n",
       "         494,  591,  439, 1845, 1154,  714,   19,  184,  323, 1941,  409,\n",
       "         367, 1093,  973,  730, 1539, 1950], dtype=int32),\n",
       " array([1649,   92,  731, 1679, 1350, 1309,  803, 1943, 1646, 1195, 1816,\n",
       "        1715, 1476,  921,  731,  923, 1188, 1614,  485,  137,  501, 1535,\n",
       "        1354], dtype=int32),\n",
       " array([1197, 1495,   71,  231,  282,  550,  337, 1125,  451,   82, 1478,\n",
       "        1453, 1240,    9, 1384, 1650, 1421,  788,   38], dtype=int32),\n",
       " array([ 172, 1297,  931, 1592,  799,  786, 1669,   76,   44, 1963,  447,\n",
       "        1873, 1173], dtype=int32),\n",
       " array([ 686, 1706, 1648,  172,  905,  986, 1614,  408, 1616, 1398, 1943,\n",
       "         730, 1554,  240, 1422,  995, 1412, 1748, 1412,  745, 1103],\n",
       "       dtype=int32),\n",
       " array([ 379, 1649,  616,  924, 1614,  282, 1966,  807, 1426, 1799,  787,\n",
       "         692,  877,  778,  889,  860, 1751, 1352,   57,  257,   58,  162,\n",
       "         453], dtype=int32),\n",
       " array([ 761,  871, 1715, 1338,   88,  107, 1767, 1527, 1025,  406, 1038,\n",
       "        1183, 1592, 1816], dtype=int32),\n",
       " array([1412, 1616,  733, 1146, 1136,  511, 1778], dtype=int32),\n",
       " array([ 379, 1649,  616,  924, 1614,  282, 1966,  807, 1426, 1799,  787,\n",
       "         692,  877,  778,  889, 1751, 1352,   57,  257,   58,  162,  453],\n",
       "       dtype=int32),\n",
       " array([ 334, 1461, 1678,  669,   31,  232,  330,  519,  223, 1701, 1282,\n",
       "         734,  540, 1894,  334, 1717,  436, 1614], dtype=int32),\n",
       " array([  14, 1136,   29,  433, 1177,  413, 1350, 1972,  904,  223,  186,\n",
       "        1447], dtype=int32),\n",
       " array([1724,  409,  447, 1594,  506,  344, 1457, 1919,  799, 1903],\n",
       "       dtype=int32),\n",
       " array([ 609,  218,  924, 1572,  419, 1873,   28, 1921,  740,  976, 1889],\n",
       "       dtype=int32),\n",
       " array([ 686, 1649,  616, 1706, 1648,  172,  483, 1532, 1044, 1099,  613,\n",
       "         905, 1243, 1095, 1398,  240, 1943, 1554], dtype=int32),\n",
       " array([1309, 1690,  381,  457,  102, 1646, 1717, 1195,  733,  286,  580,\n",
       "        1476, 1200, 1705, 1886,  337,  295,  613,  921, 1614], dtype=int32),\n",
       " array([1103, 1649, 1173, 1969, 1058, 1010, 1045,  246,  328, 1219, 1260,\n",
       "         240, 1045, 1412, 1136, 1794,   80], dtype=int32),\n",
       " array([1913, 1112, 1616, 1827, 1614, 1873, 1169,  377,  798,  580, 1476,\n",
       "         816], dtype=int32),\n",
       " array([ 616,  828,  830,  408,  614, 1895, 1941,   16,  548, 1075,  988,\n",
       "         740, 1885,  490,  868, 1886, 1555, 1873], dtype=int32),\n",
       " array([1848, 1301, 1592,  762,  661, 1241, 1399, 1956, 1997,  409,  447,\n",
       "          44, 1873, 1162, 1856, 1074, 1335,  998,  557, 1816], dtype=int32),\n",
       " array([1122, 1965,  998, 1974,  884,  743,  808,  579, 1230,  666, 1779,\n",
       "        1388,  956, 1122, 1574,  473, 1007, 1701,  796,  711, 1761, 1873,\n",
       "        1767, 1231,  229, 1745, 1776,  808,  255,  666,  263,  884,  302],\n",
       "       dtype=int32),\n",
       " array([ 686, 1308,  234,  930, 1821,  692,  733, 1200, 1939, 1527, 1981,\n",
       "        1281, 1169, 1092, 1965, 1706, 1870], dtype=int32),\n",
       " array([1538,  710, 1649, 1136,  624, 1873,  794, 1218, 1389, 1482, 1200,\n",
       "        1397, 1189,  898], dtype=int32),\n",
       " array([1197, 1948, 1714,  952,  139, 1873,  426, 1671, 1419,  868, 1463],\n",
       "       dtype=int32),\n",
       " array([1197,  884, 1090, 1339, 1873,   81, 1540, 1181,  308,  644,  624,\n",
       "          81], dtype=int32),\n",
       " array([1301, 1649, 1150, 1476, 1163, 1527, 1291, 1126,  736, 1701, 1700,\n",
       "        1616,   44,   14, 1827, 1412, 1858, 1486,  132,  259], dtype=int32),\n",
       " array([ 761,  871, 1715,  580,  408,  613,   88, 1003,  506, 1038, 1183,\n",
       "        1592, 1816,  733], dtype=int32),\n",
       " array([1538,  710, 1649, 1948,  409, 1738,  115, 1136,  624, 1126, 1873],\n",
       "       dtype=int32),\n",
       " array([1136,  622,  402, 1125, 1527,  701, 1356, 1781,  677,  662,  602,\n",
       "        1742, 1053, 1537,  799,  238, 1438,  111,  451,  898], dtype=int32),\n",
       " array([ 379,  616,  924,  563,  282, 1751,  807, 1595,  787,  692,  877,\n",
       "         778,  469,  275, 1966,  860,  700, 1352,   58,  162,  453,  238],\n",
       "       dtype=int32),\n",
       " array([ 232,  330,  436, 1915, 1614,   48,  223, 1616, 1171,   31,   48,\n",
       "         241,  337, 1282,  734,  540], dtype=int32),\n",
       " array([ 963, 1172,  924, 1197,  924, 1728, 1250, 1900, 1799, 1755, 1261,\n",
       "        1842,  809,  282], dtype=int32),\n",
       " array([1197,  553, 1873, 1662,  884, 1336,  940, 1565, 1608, 1232,  972,\n",
       "         362,  209], dtype=int32),\n",
       " array([1197,  924,  206,  692,  877, 1873, 1919, 1464,  792,  839, 1412,\n",
       "         166,  807,  944,  787,  778,  533,  162,  692,  285,  840],\n",
       "       dtype=int32),\n",
       " array([ 900, 1771, 1717, 1099, 1137,  104,  752,  862], dtype=int32),\n",
       " array([1101, 1301, 1630, 1418, 1196, 1085, 1592,  312, 1467, 1913, 1992,\n",
       "         409,  331,  733], dtype=int32),\n",
       " array([ 616,  379, 1969, 1058, 1528,  554, 1325, 1452, 1273,  332,  915,\n",
       "        1701,  379,  833,  691,  898], dtype=int32),\n",
       " array([ 379,  616,  379, 1969, 1058, 1528,  554, 1325, 1452, 1273,  332,\n",
       "         915, 1701,  379,  833,  691,  898], dtype=int32),\n",
       " array([  44, 1649, 1592, 1241, 1724,  794,  406, 1717,  464,  447, 1873,\n",
       "         555,  884,  764,   44,  240, 1774,  506, 1667, 1656,  898, 1016,\n",
       "         254], dtype=int32),\n",
       " array([ 379, 1197,  924,  206,  692,  877, 1873, 1919, 1464,  792,  839,\n",
       "        1412,  166,  807,  944,  787,  778,  533,  162,  692,  285,  840],\n",
       "       dtype=int32),\n",
       " array([1496,  616,  502,  940, 1546, 1978,  375,  884, 1966, 1920, 1463,\n",
       "         282, 1328, 1561, 1816,  703, 1200], dtype=int32),\n",
       " array([1726, 1957,  198,  253,  125,  255], dtype=int32),\n",
       " array([  28, 1150, 1338,  508, 1270, 1482,  508, 1614], dtype=int32),\n",
       " array([ 761,  312, 1648,  172, 1100,  986, 1604,  603, 1099,  494,  591,\n",
       "         439, 1845,  508, 1154,  714,  508,   19,  184,  323, 1941,  409,\n",
       "        1124, 1093,  240], dtype=int32),\n",
       " array([ 137,  665,  163, 1547, 1028, 1195, 1816,  552,  231,  282,  286,\n",
       "         580, 1476, 1200,  895, 1951, 1112, 1582,   60, 1200,  186,  695,\n",
       "        1090], dtype=int32),\n",
       " array([ 761,  871, 1715, 1338,   88, 1527, 1025,  506, 1535, 1183, 1592,\n",
       "        1816, 1614,  761, 1700], dtype=int32),\n",
       " array([1969, 1037, 1649, 1969, 1037,  331, 1982, 1903,  979,  315,  350,\n",
       "         508,  508,   84,  799], dtype=int32),\n",
       " array([ 172,  483, 1648, 1044, 1099, 1497,  494,  439,  316,  900],\n",
       "       dtype=int32),\n",
       " array([1848, 1660, 1301,  500,  669,  715, 1555, 1241,  785,   44],\n",
       "       dtype=int32),\n",
       " array([ 315, 1660, 1301,  500, 1807,  669,  715, 1555,  762,  255,   28,\n",
       "        1346, 1867], dtype=int32),\n",
       " array([  84, 1717,  907,  747,    8,  141,  707,  594, 1527,  456],\n",
       "       dtype=int32),\n",
       " array([1173, 1196, 1414,  819,  413,  457, 1465,  371, 1330,  965, 1438],\n",
       "       dtype=int32),\n",
       " array([ 414,  542,  624,  136, 1671,  314,  723,  445,  336,  359, 1880],\n",
       "       dtype=int32),\n",
       " array([1103, 1969, 1058,  523, 1301,  501,  542, 1626,  240, 1535,   44,\n",
       "         788,  259, 1270, 1856,  464,  447,  508,  508,  279,  951,  557,\n",
       "        1026,  506], dtype=int32),\n",
       " array([ 616,  580, 1630,  203, 1679, 1715, 1476,  695,  139, 1970,  332,\n",
       "         246,  328,  203, 1112, 1614,  282], dtype=int32),\n",
       " array([ 436, 1728, 1529, 1137, 1701,  323, 1099,  761, 1107, 1579,  795,\n",
       "        1434, 1777,  362,  342, 1196], dtype=int32),\n",
       " array([1197, 1103, 1398, 1201, 1982,  263, 1799,  668, 1201, 1010, 1771,\n",
       "         784,  553, 1392, 1850, 1313,  662,   84,  602], dtype=int32),\n",
       " array([ 616, 1774, 1301,  456,  731, 1700, 1622, 1982,  263,  137, 1791,\n",
       "        1188, 1614,  282], dtype=int32),\n",
       " array([ 616, 1747,  506, 1992, 1468, 1131,  678, 1614,  282, 1535,  952,\n",
       "         139, 1555, 1827,  481, 1671], dtype=int32),\n",
       " array([1696,  616, 1774, 1301,  456,  731, 1700, 1622, 1982,  263,  137,\n",
       "        1791, 1188, 1614,  282], dtype=int32),\n",
       " array([ 686,  616,  172,  761,  730,  986, 1648, 1044,  603, 1099, 1954,\n",
       "         445, 1422,  995, 1412, 1748, 1412,  745, 1103,  664,   28,  900,\n",
       "         282], dtype=int32),\n",
       " array([1173,  362,  203, 1679, 1715, 1476,  695, 1025, 1045,  565,  332,\n",
       "         246,  328,  694,  809, 1354,  203, 1913], dtype=int32),\n",
       " array([1173,  451,   82, 1421,  989, 1650,  117,  550, 1701,  934,  460,\n",
       "        1227, 1656,  875,  477, 1614], dtype=int32),\n",
       " array([  44, 1301,  616, 1854, 1546,   44,   87,  282, 1527, 1592, 1241,\n",
       "        1724,  355, 1010, 1774, 1699, 1201, 1457, 1816], dtype=int32),\n",
       " array([ 625,  246, 1882], dtype=int32),\n",
       " array([ 686,  761,  312,  733, 1648,  172,  483, 1044, 1099, 1642,  905,\n",
       "        1527, 1630,  304, 1422, 1103, 1412, 1748], dtype=int32),\n",
       " array([ 519,  900, 1728,  436, 1529, 1137, 1701, 1099,  761, 1107, 1579,\n",
       "         795, 1939,  362,  342], dtype=int32),\n",
       " array([1173,  924,  206, 1969, 1058, 1578, 1527, 1103, 1614,  456,  437,\n",
       "         240, 1982,  263,  554,  860,  924], dtype=int32),\n",
       " array([ 793,  616, 1947,  436,   79, 1817,  750,  107,  526,  413,  341,\n",
       "        1457,  703, 1437,  703, 1938,  899,  911, 1633, 1892, 1362, 1760],\n",
       "       dtype=int32),\n",
       " array([ 713,  825,   45,  733, 1717,  681, 1862,  286,  268, 1527, 1949,\n",
       "        1099, 1137,  104,  752, 1313, 1758, 1412,  240, 1171,  553],\n",
       "       dtype=int32),\n",
       " array([ 456, 1243,  818, 1799,  208,  572,  681, 1798,  733], dtype=int32),\n",
       " array([1500, 1223,  701,  281, 1802, 1136,  622,  402,  528,   84, 1011,\n",
       "         240, 1665, 1742, 1893], dtype=int32),\n",
       " array([1020,  268, 1185, 1282,  199, 1686,  548, 1155,  968, 1980, 1125,\n",
       "        1736,  596, 1717, 1491], dtype=int32),\n",
       " array([ 797,  194,  194, 1015,  733, 1578, 1679,  669, 1917,  214,  936,\n",
       "         922,  489,  676, 1168,  414,  628,  194, 1344,   80, 1799, 1882],\n",
       "       dtype=int32),\n",
       " array([ 616,  710, 1644,  807,  759,  639, 1176,  282, 1877,  304,   52,\n",
       "        1630,  409, 1124, 1954, 1270,  639, 1701,  237, 1136], dtype=int32),\n",
       " array([ 683,  686, 1226,  859], dtype=int32),\n",
       " array([  44, 1301, 1392, 1034,  393, 1325, 1856,  464,  409,  447, 1074,\n",
       "         733, 1335, 1724, 1592,  799, 1311, 1616, 1362, 1126, 1848,  107,\n",
       "        1407, 1108, 1241], dtype=int32),\n",
       " array([1696,  137, 1021,  884, 1309, 1656,  803,  102, 1646,  282,  286,\n",
       "        1476,  921, 1705,   60,  153, 1734, 1150,  839,  232,  330,  461,\n",
       "         303, 1447,  886,  944, 1081, 1954], dtype=int32),\n",
       " array([ 616, 1747,  506, 1992, 1468, 1131,  678, 1614,  282, 1535,  952,\n",
       "         139, 1555, 1827,  240,  481, 1671], dtype=int32),\n",
       " array([1195,  102, 1646, 1717,  870,  731,  253,  231, 1532,  580, 1476,\n",
       "         613,  921, 1715, 1309, 1690,  381, 1195,  457], dtype=int32),\n",
       " array([1136,   29,  226,  848,  759,  240, 1774,  255,  119,  878,  342,\n",
       "        1527,  644, 1624, 1453,   90], dtype=int32),\n",
       " array([1649,   92,  647, 1978, 1463, 1496, 1000, 1488,  940,  394, 1879,\n",
       "        1816,  156,  142,  616,  809], dtype=int32),\n",
       " array([ 761,  871, 1715, 1338,   88, 1025, 1592,  132,  733], dtype=int32),\n",
       " array([ 379,  462, 1257, 1129,  979,  929,  924, 1243,  884,  810,   48,\n",
       "         761,  733, 1793,  379, 1351], dtype=int32),\n",
       " array([1728, 1532, 1529, 1137,  323, 1099,  761, 1107, 1579,  795,  519,\n",
       "         362,  342, 1196], dtype=int32),\n",
       " array([ 379,  616,  924,  172,  341, 1777,  678, 1486, 1947,  441,  786,\n",
       "         165,  681, 1614,  282], dtype=int32),\n",
       " array([ 289,   51,  422,  305,  311, 1398, 1298, 1639, 1898,  223, 1926,\n",
       "        1229,  669,  123,  551,  775, 1047, 1017, 1707,  412, 1770, 1465,\n",
       "         517,  547], dtype=int32),\n",
       " array([ 686, 1630,  986, 1616, 1398,  552, 1649], dtype=int32),\n",
       " array([1467,  330,  615, 1877, 1201,  271, 1237,  787,  337,  692,  709,\n",
       "         778,  889], dtype=int32),\n",
       " array([ 519,   84, 1717,  907,  594, 1718, 1614], dtype=int32),\n",
       " array([ 356,  312,  798, 1656,  351,   93,  358, 1753, 1083, 1527, 1817,\n",
       "         479, 1199, 1749], dtype=int32),\n",
       " array([1862, 1604,  603, 1099,  323,  409, 1093, 1648, 1398,  240, 1422],\n",
       "       dtype=int32),\n",
       " array([1696,  731,  232,  330,  461,  303,  944,  299,  303, 1620,   84,\n",
       "         944,  669,  553, 1555, 1527, 1584,  743, 1778, 1194, 1412,  596,\n",
       "         831, 1358, 1273], dtype=int32),\n",
       " array([ 312,  251,  798, 1656,  358,  457,  351,   93,  874,  232,  330,\n",
       "        1150, 1083, 1527, 1817, 1199, 1749], dtype=int32),\n",
       " array([1430, 1532, 1236, 1529, 1137,  806, 1099,  761, 1107, 1579,  795],\n",
       "       dtype=int32),\n",
       " array([1197, 1839,    2,  914,  282, 1455,   75,  103,  805], dtype=int32),\n",
       " array([ 616,  936,   38,  778,  487, 1481,   54,  880, 1614,  282, 1527,\n",
       "        1322, 1728,  553,  654, 1715, 1090,  234,  107,  677], dtype=int32),\n",
       " array([ 445, 1201,  651,  951, 1527,  194, 1614, 1578, 1774,  669, 1672,\n",
       "         784, 1917], dtype=int32),\n",
       " array([ 616, 1630,  362, 1687,  385,  607,  879, 1832, 1922, 1294],\n",
       "       dtype=int32),\n",
       " array([1884,  616, 1671, 1136, 1432, 1698,  805,  445,  123,  968, 1781,\n",
       "         898,   28,   74, 1672,   81, 1003, 1323, 1555,  282], dtype=int32),\n",
       " array([ 686,  616, 1630,  362, 1687,  385,  607,  879, 1832, 1922, 1294],\n",
       "       dtype=int32),\n",
       " array([ 118, 1141,  455, 1321,  870, 1062,  520,  279, 1227, 1768, 1886,\n",
       "        1163, 1527,  375,  381], dtype=int32),\n",
       " array([  44, 1301,  616,  910, 1546,  255, 1010,  240, 1774, 1034,  393,\n",
       "        1699, 1592, 1816,   44, 1527, 1241, 1724,  355, 1331, 1237,  878,\n",
       "        1630, 1418, 1025], dtype=int32),\n",
       " array([1197, 1290, 1309, 1965,  354,  282,  276, 1385,  973, 1119,  886,\n",
       "         315,  799,  786, 1669, 1572, 1555], dtype=int32),\n",
       " array([1978,  616,  502,  940, 1546, 1978,  375,  884, 1966, 1920, 1463,\n",
       "         282, 1328, 1561, 1816,  703, 1200], dtype=int32),\n",
       " array([ 616, 1455, 1136,    2,  914, 1676, 1907, 1258, 1485, 1517, 1614,\n",
       "         282, 1095, 1112, 1616,  794,  699, 1966,  103], dtype=int32),\n",
       " array([1173,  312, 1630, 1656, 1753,  874,  232,  330,  351,   93, 1150,\n",
       "        1083, 1527, 1490,  761, 1199, 1749,  819], dtype=int32),\n",
       " array([1173, 1094,  673, 1465,  277, 1510, 1060,  993,  722, 1252,  519,\n",
       "        1515, 1280,  466,   82, 1764,  156,  924,  244,  722, 1252, 1201,\n",
       "         908, 1763,  240,  646], dtype=int32),\n",
       " array([ 791, 1273,   47,  616, 1455, 1136,    2,  914, 1676, 1907, 1258,\n",
       "        1485, 1517, 1614,  282, 1095, 1112, 1616,  794,  699, 1966,  103],\n",
       "       dtype=int32),\n",
       " array([ 379,  616,  924,  172,  341, 1486, 1947,  215,  813,  495,  588,\n",
       "        1313, 1614,  282], dtype=int32),\n",
       " array([  44, 1301,  616,  910, 1546, 1548,  255, 1010, 1774, 1034,  393,\n",
       "        1699, 1592, 1816,   44, 1119, 1301, 1526,  501,  330, 1614,  913,\n",
       "        1414,  736, 1189,  282], dtype=int32),\n",
       " array([1894,  136, 1444, 1907,  232,  330, 1718,  223,  334, 1461,  669,\n",
       "        1156], dtype=int32),\n",
       " array([1495, 1649, 1924, 1090, 1200, 1715, 1630,  855, 1939,  357, 1965,\n",
       "        1273, 1456,  282, 1456, 1842, 1614], dtype=int32),\n",
       " array([ 874,  312, 1532, 1614, 1616,  251,  798, 1656,  874,  232,  330,\n",
       "         351,   93,  358, 1753,  680, 1083, 1527, 1817,  232,  330, 1199,\n",
       "        1749], dtype=int32),\n",
       " array([ 616,  194, 1344, 1669, 1498,  282, 1797,  788,  132, 1535,  315,\n",
       "         936,  922,  489,  676, 1168], dtype=int32),\n",
       " array([ 806, 1467,  330,  615, 1201,  787,  692,  877, 1529,  884, 1701,\n",
       "         898,  699,  778,  546], dtype=int32),\n",
       " array([ 141,  806,   28,  467, 1100, 1604,  409, 1124,  950, 1201,  731,\n",
       "          64, 1489, 1026,  775,  788,  730,  986, 1467], dtype=int32),\n",
       " array([  44, 1301,  616,  910, 1546, 1548,  255, 1010,  240, 1774, 1034,\n",
       "         393, 1699, 1592, 1816,   44, 1119, 1301, 1526,  501,  330, 1614,\n",
       "         913,  508, 1414,  508,  736, 1189,  282], dtype=int32),\n",
       " array([1173, 1136, 1432, 1698,  115,  513, 1270,  669, 1671,  381, 1029,\n",
       "        1394,  788,  730], dtype=int32),\n",
       " array([1197, 1672,  336,  436,  891,  208, 1701, 1136,   29, 1841,  847,\n",
       "        1982,  650,  895, 1936,  282], dtype=int32),\n",
       " array([1546, 1592, 1816,   44, 1010,  240, 1774,  406, 1527, 1241,  785],\n",
       "       dtype=int32),\n",
       " array([ 904,  692,  877,  617,  600, 1328,   82,   90, 1200,  414,  136],\n",
       "       dtype=int32),\n",
       " array([1025, 1970,  332,  246,  328, 1630,  203, 1679], dtype=int32),\n",
       " array([1848, 1301, 1301,  315, 1660, 1074,  282, 1578,  666, 1724,  557,\n",
       "        1816,   44, 1814, 1237, 1546, 1010, 1774, 1392, 1856, 1997,  409,\n",
       "         447, 1555, 1527,  156,  297], dtype=int32),\n",
       " array([1566,  822,   31,  240,  413,   60,  199, 1282,  472, 1482, 1301],\n",
       "       dtype=int32),\n",
       " array([ 710,  687,  936,  922,  807, 1941,  409, 1124,  986], dtype=int32),\n",
       " array([1908,  272, 1351,  697], dtype=int32),\n",
       " array([ 312,  251,  798, 1656,  212,  351,   93, 1753,  819,    3,  231,\n",
       "        1532,  654,  884, 1392, 1757,  681, 1527, 1817,  761,  232,  330,\n",
       "        1199, 1749], dtype=int32),\n",
       " array([1020, 1649, 1924,  362,  203, 1715, 1219, 1476,  855,  695,  529,\n",
       "        1546, 1045, 1970,  678,  246,  328, 1185, 1261,  809,  485,  203,\n",
       "        1112], dtype=int32),\n",
       " array([1197, 1354, 1393, 1991, 1840,  924,  993,   19,   14, 1249, 1488,\n",
       "        1542], dtype=int32),\n",
       " array([1301,  315, 1660, 1074, 1532, 1578,  666, 1724, 1814, 1237, 1546,\n",
       "        1010, 1774,  259, 1856,  409,  447], dtype=int32),\n",
       " array([ 506, 1038, 1486, 1392, 1856,  464,  409,  447, 1532,  985, 1414,\n",
       "         240, 1679,   44, 1083, 1527, 1724, 1301,  315, 1660, 1616,  413,\n",
       "        1816,  123, 1331, 1108,  933,  214], dtype=int32),\n",
       " array([ 312,  251,  798, 1656,  874,  212,  351,   93, 1753,    3,  231,\n",
       "        1532, 1527, 1199, 1749,  261, 1817,  874, 1717], dtype=int32),\n",
       " array([1430,  206, 1921,  494, 1272,   81], dtype=int32),\n",
       " array([1173, 1946,  876,   21,  112, 1614,  451,   82,  993,  617,  671,\n",
       "          20, 1653], dtype=int32),\n",
       " array([1197,  669,  530, 1777, 1592, 1816,  282,  286,  580, 1476, 1982,\n",
       "        1614], dtype=int32),\n",
       " array([1173,  172,  483,  215, 1258, 1100, 1729, 1630, 1418, 1196, 1021,\n",
       "        1125,  445,  900,  686, 1243, 1044, 1099, 1497,  591, 1554],\n",
       "       dtype=int32),\n",
       " array([1455, 1717,    2,  914, 1839,  103,   75, 1676, 1886], dtype=int32),\n",
       " array([1412, 1748,   71,   11,  884,  688,  282, 1373,  451,  902, 1299,\n",
       "         782], dtype=int32),\n",
       " array([ 791, 1273,   47,    2,  914,  282, 1455,  704, 1616,   75,  103,\n",
       "        1839,  363, 1086,  936, 1758,  337,  553,  922,  856, 1443,  904,\n",
       "         577, 1455,  704,  914,  237,  911, 1886, 1394, 1354, 1485,  280,\n",
       "         445,  480, 1095, 1656], dtype=int32),\n",
       " array([ 616,  798,  251,  529, 1656,  358, 1753,  232,  330,  351,   93,\n",
       "        1083, 1527, 1817, 1199, 1749,  874,  312,  602], dtype=int32),\n",
       " array([ 907, 1467,  330,  615, 1490,  692,  877, 1467,  330, 1136, 1359,\n",
       "         408, 1155, 1131, 1037, 1987, 1841], dtype=int32),\n",
       " array([1978, 1463, 1496,  502,  940, 1879, 1816], dtype=int32),\n",
       " array([ 710, 1626, 1644,  807,  639, 1100,  986,  972,  102,  409,  367,\n",
       "        1093, 1862, 1630,  237, 1136], dtype=int32),\n",
       " array([1197,  362,  203, 1715,  529, 1091,  246,  328, 1112, 1614,  282,\n",
       "        1219, 1521, 1045,  493, 1467, 1970,  332, 1119,  923, 1854, 1867],\n",
       "       dtype=int32),\n",
       " array([ 315,  973, 1756, 1674], dtype=int32),\n",
       " array([ 312,  937,  496, 1630,  483,  215, 1099,  247], dtype=int32),\n",
       " array([ 240,  752, 1313, 1771, 1717, 1862, 1099], dtype=int32),\n",
       " array([1261,  168,  194, 1742, 1964, 1578, 1774,  669, 1917,  252,  736,\n",
       "         861,  788, 1227, 1582, 1934, 1564,   22], dtype=int32),\n",
       " array([ 761,  871, 1715, 1527, 1025,  506, 1535, 1183, 1592, 1816, 1532,\n",
       "        1723,   79, 1998], dtype=int32),\n",
       " array([1173, 1534,  910, 1092, 1706, 1140,  677, 1021,  257, 1527,   10,\n",
       "         580, 1090, 1200], dtype=int32),\n",
       " array([1197,  774, 1614,  282, 1400, 1966, 1658,  670, 1527, 1597],\n",
       "       dtype=int32),\n",
       " array([1197, 1150, 1155,  133,  420,   29,   48, 1718, 1841, 1607,  786,\n",
       "         496,   90, 1200,  625,  860, 1781], dtype=int32),\n",
       " array([1946,  876,   21,  112,  282,  518,  199, 1177,  809,  892, 1185,\n",
       "         115,  810, 1430], dtype=int32),\n",
       " array([ 118, 1141,  455, 1875,  368, 1964,  284, 1112,  936, 1318, 1062,\n",
       "         520,  922, 1003,  279], dtype=int32),\n",
       " array([ 686,  172,  483, 1862, 1497,  603, 1099, 1845,  591,  439,  363,\n",
       "         900,  282,   19, 1954,  730, 1539, 1950], dtype=int32),\n",
       " array([ 888,  207,  893,  653,  173,  260,  738, 1887,  523,  255,  888,\n",
       "        1792,  282,  878,  677, 1527,   13, 1815, 1081, 1716,  654, 1011,\n",
       "         919, 1294,  260, 1614, 1527,  297, 1405, 1075,  821,  766, 1047],\n",
       "       dtype=int32),\n",
       " array([ 289,   51,  422,   66,  305,  311, 1898,  223, 1229, 1372, 1732,\n",
       "         913,  412, 1770, 1465,   65,  830,  545, 1186, 1474,  927, 1689,\n",
       "        1472, 1373, 1668], dtype=int32),\n",
       " array([ 616,  817,  379,   75,  305, 1770,  174, 1095,  119,  916, 1276,\n",
       "        1737, 1898,  223, 1229], dtype=int32),\n",
       " array([ 729,  856,  586,  445,   61,  708, 1532], dtype=int32),\n",
       " array([ 624,  153,  115, 1841, 1995,  788,  132, 1215], dtype=int32),\n",
       " array([ 762, 1592,  315, 1660, 1848, 1077,  715, 1555,  787], dtype=int32),\n",
       " array([1112, 1045,  602,  580, 1630, 1679, 1715, 1476,  695,  426,  400,\n",
       "         565, 1038, 1970,  332,  246,  328], dtype=int32),\n",
       " array([ 508, 1451,  807,  257,  787,  508,  924, 1467, 1913, 1614,  508],\n",
       "       dtype=int32),\n",
       " array([1496,  502,  940,  375,  884, 1978, 1463, 1496,   28, 1879, 1816,\n",
       "          14, 1547,  172,  342, 1196], dtype=int32),\n",
       " array([ 414,  542,  624,  136, 1982,  553,  226,  979, 1371,   84,  315,\n",
       "        1135,  979], dtype=int32),\n",
       " array([1727,  616, 1488,  940,  282, 1630, 1679, 1616,  437,  508,  187,\n",
       "        1867,  508,  936,  922, 1476,    5,  275,  639,  144, 1241,  787,\n",
       "         553], dtype=int32),\n",
       " array([ 616, 1624, 1944, 1154, 1742, 1886, 1840, 1112, 1614, 1873],\n",
       "       dtype=int32),\n",
       " array([1197,  104,  752,  862,  282,  462, 1257, 1129,  979, 1483, 1900,\n",
       "        1371, 1137,  807,    5,  495,  381, 1894], dtype=int32),\n",
       " array([1848, 1649, 1924, 1592,  906,  330,  409,  282,  259, 1270,  557,\n",
       "        1816,   44,  851, 1083, 1592, 1774,  860,  938, 1290, 1028],\n",
       "       dtype=int32),\n",
       " array([1810, 1344, 1669,  282, 1498,  194, 1241, 1903, 1228,   22,  489,\n",
       "         676,  414,   35,  628,  913, 1125], dtype=int32),\n",
       " array([ 565, 1270, 1090,  583, 1325, 1430, 1219,  312,   50,  102, 1162,\n",
       "        1717], dtype=int32),\n",
       " array([1439, 1327,  207, 1630,  483,  269, 1572, 1540, 1979,  282, 1260,\n",
       "         664, 1909,  936, 1280, 1439,  318, 1584, 1294,  342, 1196],\n",
       "       dtype=int32),\n",
       " array([ 289,   51,  422,   66,  305,  311, 1898,  223, 1229, 1372, 1732,\n",
       "         913,  412, 1770, 1465, 1474,  927, 1689, 1472, 1373, 1668,   65,\n",
       "         830,  545, 1186], dtype=int32),\n",
       " array([  96,   75, 1532, 1527,  132, 1856,  409,  447, 1074, 1335,   28,\n",
       "        1241,  931, 1873], dtype=int32),\n",
       " array([ 616,  436,  891,  282,  228, 1028, 1136,   29, 1399, 1340, 1921,\n",
       "        1774,  119, 1117, 1071,  315,  445, 1867,  730,  213, 1956, 1614],\n",
       "       dtype=int32),\n",
       " array([ 797, 1649,  616,  312,  194, 1463,  797, 1635,  560, 1133,  282,\n",
       "         788,   22,  580,  489, 1969, 1037,   35,  628, 1273,  898],\n",
       "       dtype=int32),\n",
       " array([1197,  312,  251,  798,  951, 1656,  232,  330,  351,   93, 1753,\n",
       "         819,    3,  282, 1527,  761,  232,  330, 1199, 1749,  946, 1817,\n",
       "         874, 1717], dtype=int32),\n",
       " array([ 785, 1751, 1831, 1339,  315, 1385,   92, 1665, 1229,  799,  862],\n",
       "       dtype=int32),\n",
       " array([ 828, 1328, 1180, 1115,  443,  936, 1328, 1555,  961, 1259, 1198,\n",
       "         332], dtype=int32),\n",
       " array([ 232,  330,  436, 1915,  733, 1614, 1218,  300, 1171, 1583, 1250,\n",
       "         555,   48,  241, 1701, 1282,  734,  540], dtype=int32),\n",
       " array([ 883,  116, 1950,  897,  793, 1265,   28,  457,  965, 1101],\n",
       "       dtype=int32),\n",
       " array([ 791, 1273,   47,    2,  914,  282, 1455,  704,   75,  103, 1839,\n",
       "         363, 1086,  936, 1758,  337,  553,  922,  856, 1443,  904,  577,\n",
       "        1455,  704,  914,  237,  911, 1886, 1394, 1354, 1485,  280,  445,\n",
       "         480, 1095, 1656], dtype=int32),\n",
       " array([1173, 1405,  326,  889,  961, 1259, 1198,  332,  240, 1948, 1969,\n",
       "        1037,  436, 1614], dtype=int32),\n",
       " array([1173, 1523,  483,  949,  898,  337, 1405,  597,  564], dtype=int32),\n",
       " array([1197, 1392,  259, 1856, 1997,  409,  447, 1241,  226, 1592,  315,\n",
       "        1660,  860,  214,  748,  506,  557, 1457, 1816,   44,  331, 1660,\n",
       "         282], dtype=int32),\n",
       " array([1197,  617,   87,  908,   28,   60,    5,  548,  699,  884,  924,\n",
       "         617, 1487, 1854], dtype=int32),\n",
       " array([ 312,  251,  798,  951, 1656,  232,  330,  351,   93, 1753,  819,\n",
       "           3,  282, 1527,  761,  232,  330, 1199, 1749,  946, 1817,  874,\n",
       "        1717], dtype=int32),\n",
       " array([1199, 1151, 1839, 1817,  692,  877,  806, 1467,  330,  615, 1614,\n",
       "        1532,  437,  787,  699, 1237, 1881,  778,  533], dtype=int32),\n",
       " array([ 104,  752,  862,  282,  462, 1257, 1129,  979, 1483, 1900, 1371,\n",
       "        1137,  807,    5,  508,  495,  381, 1894,  508], dtype=int32),\n",
       " array([ 482,  153,  731,  137, 1554, 1850,  784,  778,   18, 1614],\n",
       "       dtype=int32),\n",
       " array([1173,  761,  232, 1199, 1749, 1239, 1701,   54, 1777,  937],\n",
       "       dtype=int32),\n",
       " array([1848,  616, 1592,  762, 1660,  315, 1359, 1614,  282, 1392, 1034,\n",
       "         393, 1856,  464,  409,  447, 1616, 1693, 1237,  764,  557, 1816,\n",
       "          44], dtype=int32),\n",
       " array([1669,  794, 1532,  257,  435,  748, 1911,  924, 1995,  788,  966,\n",
       "         104,  752,  555, 1734], dtype=int32),\n",
       " array([1783, 1352, 1195,  746,  878, 1949, 1128, 1414, 1010, 1455, 1270,\n",
       "         731, 1774,  413,  580,  408,  731,  871,  408, 1162,   50],\n",
       "       dtype=int32),\n",
       " array([ 542,  624, 1969, 1037, 1867,   90, 1620,  104,  752, 1241,  828],\n",
       "       dtype=int32),\n",
       " array([1646, 1270, 1548,  362, 1418, 1352, 1195, 1873, 1918,  884,  798,\n",
       "        1614, 1974,  250], dtype=int32),\n",
       " array([ 168,  301, 1546, 1592, 1816,  409,   44, 1010,  240, 1592, 1774,\n",
       "        1034, 1699, 1486, 1392, 1856, 1997,  409,  447], dtype=int32),\n",
       " array([1412,  552, 1273,   47, 1954,  799,  461,  303, 1298,  124, 1582,\n",
       "        1405,   29, 1665,  553,  137,  338], dtype=int32),\n",
       " array([1020, 1649, 1173, 1946,  876,   21,  112, 1268, 1136, 1335, 1185,\n",
       "        1136, 1503,  879, 1237, 1950, 1941, 1992,  275], dtype=int32),\n",
       " array([ 576, 1649,  616, 1969, 1037, 1614,  282,  947,  961,  350, 1214,\n",
       "         445, 1198, 1119,  936,  922,  703, 1200,  596, 1154,  506],\n",
       "       dtype=int32),\n",
       " array([ 710,  282, 1877,   94,  304,   52,  807,  759,  639, 1176, 1954,\n",
       "         409, 1124,  972, 1297,  629, 1862, 1630, 1554], dtype=int32),\n",
       " array([1848,  616, 1592,  762, 1660,  315, 1359, 1614,  282, 1392, 1034,\n",
       "         393, 1856,  464,  409,  447, 1616, 1693,  553, 1237,  764,  557,\n",
       "        1816,   44], dtype=int32),\n",
       " array([ 232,  330,  436, 1915,  733, 1223, 1461,  669, 1282,  734,  860,\n",
       "        1175, 1616], dtype=int32),\n",
       " array([ 580,  807, 1200, 1210, 1527,  910,  357, 1131, 1456,  819, 1873,\n",
       "         282, 1112, 1614], dtype=int32),\n",
       " array([ 907,  213, 1471,  232,  330, 1916, 1527, 1894, 1717, 1362,  594,\n",
       "        1886, 1966,  223, 1875,  812], dtype=int32),\n",
       " array([1848, 1301, 1197, 1301,  315, 1660, 1848,  136, 1162,  259, 1856,\n",
       "        1693, 1594, 1419, 1535, 1457,   44], dtype=int32),\n",
       " array([1455,  704,  330, 1841, 1614, 1873, 1717, 1742,  226,  998, 1596],\n",
       "       dtype=int32),\n",
       " array([1630,   29, 1665,  247, 1867,  730, 1877,  448, 1873, 1030, 1555,\n",
       "         408, 1342,  409,  367, 1636], dtype=int32),\n",
       " array([1848,  259,  506, 1717,  464,  447, 1693, 1594, 1431,  557, 1457,\n",
       "          44, 1301,  886,  315, 1660,  408,  297, 1614], dtype=int32),\n",
       " array([1649,   92,   19,  788,  172,  483, 1648, 1044,  603, 1099,  255,\n",
       "        1100, 1554, 1330,  686,  354,  945,  900], dtype=int32),\n",
       " array([  44, 1301,  616, 1717, 1034,  393,  764, 1301, 1693,  282,  344,\n",
       "         557, 1457,  240, 1083, 1527, 1526,  553, 1015,  279, 1519,  255,\n",
       "        1038], dtype=int32),\n",
       " array([ 312,  194, 1463,  797, 1635,  560, 1133, 1532,  788,   22,  580,\n",
       "         489,  414,   35,  628, 1273,  898], dtype=int32),\n",
       " array([1185, 1717,  737, 1701, 1398, 1157,  359, 1301,  812, 1950],\n",
       "       dtype=int32),\n",
       " array([ 616,  921, 1693, 1237,   80, 1354, 1261, 1842, 1614, 1873],\n",
       "       dtype=int32),\n",
       " array([1173,   34, 1754, 1595,  381,  344,  279, 1713, 1906, 1727,  553,\n",
       "        1614,  998,  255, 1244,  957,  756,  480], dtype=int32),\n",
       " array([ 330, 1586,  553,  506,  213,  788, 1131,  332], dtype=int32),\n",
       " array([1185, 1136,  826, 1841, 1817, 1280, 1010, 1185, 1037], dtype=int32),\n",
       " array([ 162, 1982,  822,  315, 1135,  979,  583,  985,  213, 1305, 1935,\n",
       "         142, 1966,  799,  187,  972, 1669], dtype=int32),\n",
       " array([1969, 1037,  828, 1328, 1180, 1201,  759,  887,   24, 1283, 1328,\n",
       "         885,  268, 1200,   80, 1198,  332, 1131,  961, 1410], dtype=int32),\n",
       " array([ 616, 1314, 1112, 1614, 1873,   48,  227, 1022, 1314,  860, 1764,\n",
       "         929,  208,  828, 1314, 1380, 1000,  693, 1886], dtype=int32),\n",
       " array([ 828, 1669, 1100, 1028, 1964, 1532,  798, 1656, 1199, 1749,  358,\n",
       "        1753,    3], dtype=int32),\n",
       " array([ 379,  692,  877,  617, 1169, 1328,   82, 1139, 1412, 1313, 1200,\n",
       "         846,  487,  828, 1328,  533, 1614,  282], dtype=int32),\n",
       " array([1624, 1946, 1299,  144, 1060, 1310,  756, 1701,  970,  451,   82,\n",
       "        1714, 1097,  550,  337, 1053,  321, 1437, 1203,  119,  993,  617],\n",
       "       dtype=int32),\n",
       " array([ 227, 1022, 1742,  749,  787,  535, 1140, 1450], dtype=int32),\n",
       " array([1848,  616, 1258,  240, 1856,  464,  409,  447,  344, 1457,   44,\n",
       "        1921,  600, 1535, 1419, 1874,  557, 1034,  393,  318,  447,  136],\n",
       "       dtype=int32),\n",
       " array([ 895,  761, 1684,  409,  409, 1124,  282,   36,  409,  448,   60,\n",
       "         483,  215, 1258,  730, 1729, 1648, 1497,  591], dtype=int32),\n",
       " array([ 548, 1330,  430, 1840,  733,  680, 1224, 1028, 1784,  457],\n",
       "       dtype=int32),\n",
       " array([ 686, 1197,  172,  986,  972,  744, 1969, 1058,  102,  409,  367,\n",
       "         363,  900, 1676,  910,  335,  282,  312,  323,  603, 1099, 1845,\n",
       "         591], dtype=int32),\n",
       " array([1649,  616, 1969, 1037,  104,  752,  282,  115,  194, 1672,  784,\n",
       "        1917, 1157, 1350,   22,  580,  489,  676, 1168], dtype=int32),\n",
       " array([1476,  921, 1715,  613,   60,   88, 1532,   68,  731,  665,  255,\n",
       "         408,  731,  442,  803,   52, 1850, 1059, 1646,  409,  372, 1195],\n",
       "       dtype=int32),\n",
       " array([ 345, 1312, 1874,  703,  703,   48, 1430], dtype=int32),\n",
       " array([ 902,  408,  993,  756,  699,  961, 1198,  332,  910,  255, 1410,\n",
       "         913,  107, 1338, 1950, 1630], dtype=int32),\n",
       " array([  44, 1301,  616, 1392, 1034,  393, 1856,  464,  409,  447, 1693,\n",
       "        1594, 1592,  506,  344,  557, 1457,  680, 1527, 1331,  240],\n",
       "       dtype=int32),\n",
       " array([ 616,  921, 1693, 1237,   80, 1354, 1261, 1842, 1614, 1873],\n",
       "       dtype=int32),\n",
       " array([1197, 1841,  909,  209,  813,  736,  491, 1906,  623, 1965, 1798],\n",
       "       dtype=int32),\n",
       " array([ 963, 1172, 1443, 1727, 1197,  756, 1092,  536, 1227, 1163, 1527,\n",
       "        1874, 1874, 1727,  854,  936, 1426, 1840, 1031,  580, 1200, 1112,\n",
       "        1614, 1873], dtype=int32),\n",
       " array([1103, 1197,   29, 1665,  282, 1322, 1392,  259, 1856, 1997,  409,\n",
       "         447, 1616, 1759, 1241,  226, 1237, 1546,   44], dtype=int32),\n",
       " array([1538,  710, 1197,  710, 1644,  282, 1609,  898,  807,  730,  986,\n",
       "         972,  102,  409, 1124, 1093,  136, 1350,  237, 1881, 1136, 1877,\n",
       "        1290], dtype=int32),\n",
       " array([ 788, 1625, 1171,  137, 1103, 1217, 1614,  286,  408,  871, 1705,\n",
       "        1090], dtype=int32),\n",
       " array([1649, 1173,  334, 1614,  282,  860, 1820,  906,  821, 1492, 1136,\n",
       "        1095,  961, 1867, 1722, 1198,  332], dtype=int32),\n",
       " array([1173,  888, 1294,  165, 1207,  419, 1822,  337, 1459,  878, 1686,\n",
       "        1386], dtype=int32),\n",
       " array([1173,   42, 1024,  697, 1765, 1321,   67,  884,  408,  560,  880,\n",
       "        1078, 1299, 1521,  451, 1465, 1841], dtype=int32),\n",
       " array([1197,  731,  442, 1635, 1630, 1195, 1816, 1527,  282,  408,  552,\n",
       "        1951, 1614, 1918,  139], dtype=int32),\n",
       " array([1197, 1969, 1037, 1328,  756, 1842, 1201,  887, 1168, 1131,  961,\n",
       "         936, 1200,  245, 1798, 1410, 1112, 1614,  282], dtype=int32),\n",
       " array([1197, 1258, 1867,  681, 1565, 1555,  839, 1385, 1617,  428,  282,\n",
       "        1816, 1112,  511, 1831,  354, 1104], dtype=int32),\n",
       " array([1737, 1412, 1748,  885,  268, 1831,  752, 1859, 1894,  752,  862,\n",
       "          23,  381, 1070, 1995, 1003, 1339,  315, 1385], dtype=int32),\n",
       " array([1197, 1941, 1339,  944, 1572, 1163,  803, 1567,  275,  894, 1982,\n",
       "        1435, 1301, 1775,  669], dtype=int32),\n",
       " array([1197,  272, 1172, 1090,  613,  389,  337,   48,  778,  546, 1393,\n",
       "        1036,  198,  331,  757,  428,  282], dtype=int32),\n",
       " array([  44, 1301,  616, 1392, 1592, 1856,   52, 1034,  393,  764, 1693,\n",
       "         282, 1594,  506,  344,  408,  898, 1016, 1850,  680, 1527, 1331],\n",
       "       dtype=int32),\n",
       " array([ 616,  232,  330,  461,  303,  944,  940, 1979, 1633,  137,   28,\n",
       "         282,  563,  491, 1245, 1837, 1701, 1195,  226], dtype=int32),\n",
       " array([ 797, 1969, 1037,  616,  491, 1412,  288,  332, 1882,  654,   22,\n",
       "         489, 1168, 1131,  194, 1969, 1037, 1614,  282], dtype=int32),\n",
       " array([1538,  616, 1747, 1850, 1856, 1210, 1085,  710,  282, 1616, 1560,\n",
       "         966, 1530,   72, 1354, 1614], dtype=int32),\n",
       " array([1850, 1856,  215,  710, 1036,  409, 1124, 1085, 1112,  602],\n",
       "       dtype=int32),\n",
       " array([1778, 1701, 1609, 1322], dtype=int32),\n",
       " array([ 686,  172,  986,  972,  744, 1969, 1058,  102,  409,  367,  363,\n",
       "         686,  900, 1676,  910,  335,  282,  312,  323,  603, 1099, 1845,\n",
       "         591,  439, 1708,  186, 1614,  312,  337,  268,  110,  936, 1243,\n",
       "         788,  986, 1729, 1609, 1523, 1630, 1418, 1196], dtype=int32),\n",
       " array([ 131,  702,  361,  578, 1633], dtype=int32),\n",
       " array([ 888,  207,  378, 1067, 1965,  408,  107, 1540,  669,  722, 1239,\n",
       "        1885,   15, 1701,  693, 1219, 1716,  742,  541, 1276, 1893],\n",
       "       dtype=int32),\n",
       " array([1756,  436,  891, 1871,  748, 1614,  282, 1136,   29,  226, 1340,\n",
       "        1774,  119, 1117,  860], dtype=int32),\n",
       " array([1816,  422,  428, 1226, 1099, 1137, 1586, 1555,  282,  275, 1217,\n",
       "        1179,   19,  428], dtype=int32),\n",
       " array([1412, 1748,  431, 1979,  248,  282,  884,  556, 1653,  510,  305,\n",
       "         311,  524,  440, 1926, 1898,  223, 1229, 1452, 1421, 1373,  922,\n",
       "          97], dtype=int32),\n",
       " array([1756, 1672,  336,  929,  436,  891,  282, 1136,   29, 1399, 1356,\n",
       "        1201, 1467, 1774, 1626,  226,  936,  737,  337,  922], dtype=int32),\n",
       " array([1412, 1748,  431, 1979,  248,  881, 1390,  282,  884,  556, 1653,\n",
       "         510,  305,  311,  524,  440, 1926, 1898,  223, 1229, 1452, 1421,\n",
       "        1373,  922,   97], dtype=int32),\n",
       " array([1816,  422,  428, 1226, 1974, 1099, 1137, 1586, 1555,  282,  275,\n",
       "        1217, 1179,   19,  428], dtype=int32),\n",
       " array([ 259, 1856,  464,  447,  860, 1759,  506, 1457,   44, 1617, 1342,\n",
       "         747,   58], dtype=int32),\n",
       " array([ 977,  822, 1730,  441, 1717, 1398, 1439,  977,  678,  107,  677,\n",
       "        1232, 1921,  344,  332, 1791, 1112, 1614,  282], dtype=int32),\n",
       " array([1649,  312,  251,  798, 1532,    3, 1066,  951, 1119,  232,  330,\n",
       "         351,   93, 1616,  761,  232,  330, 1199, 1749,  518], dtype=int32),\n",
       " array([ 252,   80, 1198, 1131,  961,  840, 1405,  810,  580, 1527,    5,\n",
       "        1099,  602,  245, 1115], dtype=int32),\n",
       " array([1538,  616, 1747, 1850, 1856, 1210, 1085,  710,  282, 1616, 1560,\n",
       "         966, 1530,   72, 1354, 1614], dtype=int32),\n",
       " array([ 963, 1172,  576,  961, 1197, 1969, 1037, 1328,  756, 1842, 1201,\n",
       "         887, 1168, 1131,  961,  936, 1200,  245, 1798, 1410,  414, 1112,\n",
       "        1614,  282], dtype=int32),\n",
       " array([  44, 1301,  616, 1392, 1592, 1856,   52, 1034,  393,  764, 1693,\n",
       "         282, 1594,  506,  344,  408,  898, 1016, 1850, 1527, 1336,  553,\n",
       "        1535], dtype=int32),\n",
       " array([1476, 1679,  900,  246, 1882, 1862], dtype=int32),\n",
       " array([ 731,  442, 1630,  995, 1195, 1816, 1455, 1951,  602], dtype=int32),\n",
       " array([1527,  970, 1805,  421,   79, 1610, 1979, 1630,  759, 1567, 1007,\n",
       "        1075,  740,   28,   74,  744], dtype=int32),\n",
       " array([  44, 1301, 1392,  259, 1856, 1997,  409,  447, 1241,  226,  282,\n",
       "          44,  860, 1594, 1699,  557, 1457, 1119,  255,  895, 1992,  409,\n",
       "         447,  639, 1814,  625, 1026,  860], dtype=int32),\n",
       " array([ 289,   51,  422,  551,  775, 1047, 1572,  243,  282,  547, 1591,\n",
       "         305,  311, 1898,  223, 1229, 1595,  412, 1770, 1465, 1021, 1373,\n",
       "         545,  848,   67, 1527,  606,  107,  725,  649, 1229], dtype=int32),\n",
       " array([ 888,  207, 1527,  970, 1009, 1973, 1098, 1805,  421,   79, 1610,\n",
       "        1630,  759, 1567, 1007,  483, 1966, 1479,  571,  267, 1075,   74,\n",
       "        1125, 1255, 1889,  804,  246,  877,  740, 1610], dtype=int32),\n",
       " array([  44, 1301,  616, 1301,  906, 1913, 1728, 1034,  393, 1699,   44,\n",
       "        1983,  319,  214,  546,  344,  557, 1457, 1816], dtype=int32),\n",
       " array([ 203, 1112,  809, 1090,  203, 1679, 1715,  910, 1546,  246,   80,\n",
       "        1933,  168], dtype=int32),\n",
       " array([1816, 1649, 1924,  282,  531,  289,  413, 1031,  580,   50, 1338,\n",
       "          88], dtype=int32),\n",
       " array([  44, 1301,  616, 1392, 1592, 1856,   52, 1034,  393,  764, 1693,\n",
       "         282, 1594,  506,  344,  408,  898, 1016, 1850, 1527, 1336,  553,\n",
       "        1535], dtype=int32),\n",
       " array([1528, 1672,  508,  508,  508,  508, 1455, 1136,    2,  914,  344,\n",
       "         408, 1003, 1273,   47,  791,  914], dtype=int32),\n",
       " array([ 168,   92, 1549,  354, 1921, 1881,  857, 1715,  580,  187,  695,\n",
       "        1338, 1261, 1842,  809,  485, 1424], dtype=int32),\n",
       " array([ 232,  330,  436, 1915,  519,  223, 1701, 1282,  734,  540],\n",
       "       dtype=int32),\n",
       " array([ 359,  847, 1192, 1077, 1672,  359, 1183, 1810, 1179, 1498,  677],\n",
       "       dtype=int32),\n",
       " array([ 616,  731,  232,  330,  461,  303,  165,  621,  944, 1558,  282,\n",
       "         973,  413, 1894,  262, 1195,  226,  895,  231], dtype=int32),\n",
       " array([ 616, 1969, 1037, 1614,  282,  947,  961,  350, 1214,  445, 1198,\n",
       "        1119,  936,  922,  703, 1200,  596, 1154,  506, 1028], dtype=int32),\n",
       " array([ 137,  616,  731,  232,  330,  461,  303,  165,  621,  944, 1558,\n",
       "         282,  973,  413, 1894,  262, 1195,  226,  895,  231], dtype=int32),\n",
       " array([ 616, 1535, 1715,  936,  922, 1034,  393, 1699,  282,  262,  669,\n",
       "        1848, 1392, 1856,  464,  409,  447, 1982, 1614], dtype=int32),\n",
       " array([ 408, 1016,  408], dtype=int32),\n",
       " array([ 172,  323, 1176, 1648, 1706,  677,  408,  124, 1398,  606,  240],\n",
       "       dtype=int32),\n",
       " array([ 984,  409,  447,   75, 1760,  860, 1333,  889, 1616,   90, 1301,\n",
       "        1015, 1526,  553,  232,  330], dtype=int32),\n",
       " array([1862, 1835,  818, 1799,  681,  788, 1969, 1058,  508,  899,  137,\n",
       "         508, 1238,  164, 1951,  282], dtype=int32),\n",
       " array([1113, 1873,    4, 1948,  137,  349,  895, 1354, 1261, 1842, 1298,\n",
       "         860,  842, 1987, 1527, 1090,  548, 1766], dtype=int32),\n",
       " array([ 963, 1172,  686, 1197,  936,  922, 1200,  442,  282, 1527, 1630,\n",
       "        1169,  910, 1092,  312, 1614], dtype=int32),\n",
       " array([ 616,  731,  232,  330,  461,  303,  944, 1558,  282, 1886,   84,\n",
       "        1245,  950,  226, 1340, 1692, 1195], dtype=int32),\n",
       " array([1301,  132,  259, 1856,  464,  409,  447, 1241,  226,  319,  214,\n",
       "         748, 1919, 1848, 1660, 1526,  315, 1359], dtype=int32),\n",
       " array([1884,  616, 1432, 1698, 1671,   28, 1136, 1873, 1620, 1241, 1867,\n",
       "        1722, 1881, 1237,  617], dtype=int32),\n",
       " array([1195,  616, 1090, 1455, 1270,  731,  871, 1715, 1873, 1195, 1546,\n",
       "          14,    2,  914, 1840, 1585,  301, 1798, 1781], dtype=int32),\n",
       " array([ 564,  616,  564, 1717,  955, 1873, 1616, 1693,  434, 1237,  889,\n",
       "         898,  669,  866, 1010, 1521, 1273], dtype=int32),\n",
       " array([1848, 1173,  830,  240, 1679, 1715, 1301, 1527, 1798,  232,  330,\n",
       "         524,  787, 1020, 1086, 1357,  162, 1521,   84,  240,  600, 1867],\n",
       "       dtype=int32),\n",
       " array([1197, 1046, 1406, 1339,  315, 1385, 1412, 1748, 1209,  125, 1298,\n",
       "         788, 1528,  681, 1941,  905, 1354, 1614,  282], dtype=int32),\n",
       " array([  48,  240, 1700,  630,  692,  877, 1261,  817,  254, 1090, 1196,\n",
       "        1614, 1858, 1238,  240, 1982,  531,  692,  709, 1105], dtype=int32),\n",
       " array([1106, 1189, 1941, 1509, 1095,  989,  513,  186,  527, 1255],\n",
       "       dtype=int32),\n",
       " array([  44, 1301,  616, 1392, 1592, 1856,   52, 1034,  393,  764, 1693,\n",
       "        1594,  506,  344,  408,  898, 1016, 1850, 1527, 1336,  553, 1535],\n",
       "       dtype=int32),\n",
       " array([ 898,  456, 1113,   24, 1964,  888,  408,  936,  936,  922,  232,\n",
       "         330,  436, 1915, 1346], dtype=int32),\n",
       " array([1197,  692,  744, 1700,  692,  877,  878,  342, 1196, 1133, 1873,\n",
       "         518, 1434,  819, 1777, 1456, 1119, 1614, 1398, 1163], dtype=int32),\n",
       " array([1173,  936,  922, 1200, 1271, 1527, 1169, 1046, 1092,  816,  647,\n",
       "        1150, 1163, 1527, 1981,  954, 1182, 1821, 1041, 1523], dtype=int32),\n",
       " array([1197, 1726,  659, 1049, 1264,  592, 1075,  504, 1079, 1735,  834,\n",
       "          85,  854,  255, 1616, 1462, 1921,  931,  292, 1079, 1890, 1572,\n",
       "         659, 1316, 1750, 1080,  140,  890, 1114, 1704,  475,   39, 1647,\n",
       "         521,  922, 1382,   62,  953], dtype=int32),\n",
       " array([1197, 1691, 1740, 1274, 1090, 1967, 1835, 1756,  313,  271,  289,\n",
       "         470, 1767,  289,   99,  255, 1472, 1289,  130,  208, 1014, 1242,\n",
       "         701, 1701,  883,  116, 1827, 1555], dtype=int32),\n",
       " array([1173,  315, 1368, 1159,    0, 1351, 1007,  715, 1719, 1075,  740,\n",
       "        1885, 1969, 1058,  225, 1201,  555, 1834,  931, 1163, 1161, 1909,\n",
       "        1859], dtype=int32),\n",
       " array([1197, 1688, 1326,  523,  851,  408, 1388,  409,  354,  208, 1090,\n",
       "        1967,   28,  579,  273, 1287, 1516,  470, 1767, 1350,  457, 1967],\n",
       "       dtype=int32),\n",
       " array([1197,  302,   75,   60,  883,  116, 1616, 1630, 1619,  804,  470,\n",
       "        1767], dtype=int32),\n",
       " array([1197, 1183,  848,  501,   28,  315,  267,  342,  931, 1881,  975,\n",
       "        1941,   41,  474,  272, 1172, 1936,  470,   28, 1189,  299, 1733],\n",
       "       dtype=int32),\n",
       " array([1197,  356, 1891, 1864,  688,   28,  497,  267,  273, 1302,  784,\n",
       "         470, 1527, 1268,  688,  765,  143,  839, 1735,  899,  714],\n",
       "       dtype=int32),\n",
       " array([ 888,  616, 1726,  659,   28,  740, 1885,  811,  381,  812, 1075,\n",
       "         751,  534, 1079,  834, 1639, 1027], dtype=int32),\n",
       " array([ 888,  616, 1987, 1075,  834,  123,  828, 1337,  765,  355,  664,\n",
       "         731,  790], dtype=int32),\n",
       " array([1818,  616, 1581, 1613,  355,  664,  759, 1574, 1372, 1249, 1590,\n",
       "         370,  691,  166,  470, 1483,  362, 1676, 1826], dtype=int32),\n",
       " array([ 888,  616,   96,  178, 1105, 1151,  888, 1294], dtype=int32),\n",
       " array([1412, 1748,  616,  436,   48,  931, 1630,  998, 1731, 1436,  579,\n",
       "        1519,  968, 1639, 1967,  975, 1412, 1748,  135, 1007, 1701,  820,\n",
       "        1520, 1225, 1921, 1135,  470], dtype=int32),\n",
       " array([1412, 1748,  616, 1151, 1630, 1630,  998, 1974,  579, 1274,  863,\n",
       "        1967,  718,  707,  633, 1367,  315, 1135,  470], dtype=int32),\n",
       " array([ 888,  616,  207,  634, 1923, 1369,  915,  893,  810, 1614,  834,\n",
       "        1527,  898, 1941, 1369, 1400,  931, 1676,   77], dtype=int32),\n",
       " array([ 888,  616, 1969, 1058,  271, 1804,  828, 1255, 1342, 1572,  483,\n",
       "        1075, 1508,  834, 1630,  301, 1832, 1643], dtype=int32),\n",
       " array([1933,   99,  613,  884, 1543, 1598, 1606,  437,  408,  208, 1701,\n",
       "        1230,  191, 1329,  669, 1572,  166,  370], dtype=int32),\n",
       " array([ 436, 1929, 1251, 1630,  998, 1731,  135, 1837], dtype=int32),\n",
       " array([1430, 1555, 1792,  759,    0, 1479, 1990, 1897, 1282, 1966,  607],\n",
       "       dtype=int32),\n",
       " array([ 135,   13, 1564, 1767, 1388, 1391, 1874,  931,  897,  733,  657,\n",
       "         493, 1090, 1388,  728, 1334,  719, 1122,  394,  844, 1891,  683,\n",
       "        1875, 1602, 1287], dtype=int32),\n",
       " array([ 891, 1691,  802, 1090,  224,  172, 1187,   52, 1242, 1701,  883,\n",
       "         116, 1293,  733], dtype=int32),\n",
       " array([1226,  645,  426,  404,  714, 1574,  473,  107, 1767,  363, 1298,\n",
       "        1371, 1886, 1235,  859,  473,  860,  664, 1236,  107, 1767,  714,\n",
       "         239, 1003,   28,  579, 1388], dtype=int32),\n",
       " array([1347, 1572,   47, 1908,  648,   62, 1368, 1430, 1933,  107,  677,\n",
       "        1996,  508,  508, 1643,  413,  663, 1427,  508,  508], dtype=int32),\n",
       " array([1636,  702, 1125, 1425,  416, 1565, 1290, 1370,  407,  703,  846,\n",
       "        1587, 1974,  915], dtype=int32),\n",
       " array([ 888, 1341,  653,  173,  260,  738, 1887, 1400,  733,  893, 1075,\n",
       "         931, 1939, 1815,    8,  413, 1861, 1405, 1075,  821], dtype=int32),\n",
       " array([ 632,   99,  255, 1014,  585, 1816,  107, 1616,  641, 1988,  217,\n",
       "        1657, 1836, 1007, 1701,  289,  343,  107, 1767], dtype=int32),\n",
       " array([1564, 1767, 1734,  276, 1330, 1342, 1791, 1414,  527,  495,  623,\n",
       "        1773,  196,  196, 1909,  196, 1163,  362, 1791,  508,  508,  865,\n",
       "        1201,  667, 1767,  619, 1513, 1574,  473, 1524,  473,  508,  508],\n",
       "       dtype=int32),\n",
       " array([1103,  108,  378, 1017, 1909, 1342,  911, 1524, 1128,  745, 1914,\n",
       "        1251,  107, 1767,  839,  804, 1055,  788, 1969, 1476, 1527, 1388,\n",
       "        1933, 1876, 1167, 1540,  158,  931], dtype=int32),\n",
       " array([1874,  457,  523,  868, 1007, 1104,  733, 1543,  875], dtype=int32),\n",
       " array([1636,  702,  971, 1788,  860, 1616,  526,  884,  920,  258,  815,\n",
       "         497], dtype=int32),\n",
       " array([1183, 1261,   28,  474,   14], dtype=int32),\n",
       " array([1636,  702, 1619, 1370,  733,  691,  166, 1090, 1875, 1161,   27,\n",
       "        1633, 1555, 1616,  727, 1616,   75], dtype=int32),\n",
       " array([ 309,  808,  654,  592,  962, 1153, 1307], dtype=int32),\n",
       " array([1411, 1150,  946, 1572, 1508,  592,  395, 1421,  922,  733],\n",
       "       dtype=int32),\n",
       " array([ 931,  625,  664, 1201, 1667, 1153], dtype=int32),\n",
       " array([ 931,  381, 1180, 1371,  337, 1316, 1796, 1508], dtype=int32),\n",
       " array([1845,  801, 1219,  266, 1508], dtype=int32),\n",
       " array([ 715, 1190,  208,  436,  224,  265, 1251, 1869,  470, 1033, 1007],\n",
       "       dtype=int32),\n",
       " array([1632,  315,  765,  835, 1410,  165, 1119,  413,  884,  548,  839,\n",
       "        1941, 1193,  931, 1197, 1941], dtype=int32),\n",
       " array([1155, 1003,  230,   28,  644, 1559,  470], dtype=int32),\n",
       " array([1630, 1342, 1966, 1490, 1347,  548, 1916, 1014,  548, 1330, 1103,\n",
       "          48, 1129], dtype=int32),\n",
       " array([1448, 1649, 1909,  839, 1062,  520, 1201,  372, 1870,  394,  679,\n",
       "        1508,  407, 1966, 1616,  490,  884, 1543,  271, 1201,  407],\n",
       "       dtype=int32),\n",
       " array([ 666,  352,  241, 1191,  936, 1102,  410,  864,  922,  408, 1448,\n",
       "         928, 1941], dtype=int32),\n",
       " array([ 419,  483,  931, 1582, 1021,  828, 1075, 1459,  834,  448,  519],\n",
       "       dtype=int32),\n",
       " array([1726,  659, 1723,  504, 1079, 1890,  931,  655,  834, 1572,  754,\n",
       "        1552,  172, 1075,  740, 1961], dtype=int32),\n",
       " array([1197,   26, 1551, 1435, 1137,  959,  848, 1811,  922, 1630, 1418,\n",
       "        1424,  567, 1595, 1991, 1527, 1109], dtype=int32),\n",
       " array([1197, 1396,  241, 1566,    6,  834, 1527, 1807,  362, 1056],\n",
       "       dtype=int32),\n",
       " array([1197,  514,  501,  930,  267, 1007,  834,  856,   49, 1792, 1371,\n",
       "        1014,  212, 1135], dtype=int32),\n",
       " array([1197, 1891,   28,  525,  267,  834,  273,   51, 1572, 1864,  688],\n",
       "       dtype=int32),\n",
       " array([1197,  420, 1874, 1338, 1540,  582, 1412,  408,  362,    4, 1306,\n",
       "         835,  826,  558,  858, 1370,  859,  541,  615, 1816,  424, 1587,\n",
       "         107,  548,  370, 1891,  806], dtype=int32),\n",
       " array([ 888,  616,  382, 1304,  924, 1075, 1368, 1088, 1460,  834],\n",
       "       dtype=int32),\n",
       " array([ 702,  616,  761,  688,  765, 1355,   75,   28, 1298, 1590,  370,\n",
       "        1572,  166,  834,  529, 1598, 1606,  524, 1205,  884], dtype=int32),\n",
       " array([ 888,  616,  482,  828, 1337,  765, 1074,  655,  788,  731,  834,\n",
       "        1223,  889, 1075, 1792, 1119, 1508, 1386], dtype=int32),\n",
       " array([ 888,  616, 1969, 1058, 1106,  407,  691,   49, 1075,  740, 1885,\n",
       "         123,  806,  834], dtype=int32),\n",
       " array([ 616, 1726,  875, 1007, 1410, 1153, 1307,  834, 1312,  931, 1181,\n",
       "         267], dtype=int32),\n",
       " array([1173,  555,  931], dtype=int32),\n",
       " array([1197, 1341, 1738,  936,  922,  983, 1959,   75, 1655,   67, 1972,\n",
       "        1112,  331,  188], dtype=int32),\n",
       " array([ 888,  616, 1969, 1058,  568,   28, 1368, 1345, 1294,   14,  834,\n",
       "         123, 1600, 1812, 1584], dtype=int32),\n",
       " array([ 888,  616,  246, 1882, 1726,  659, 1834,  863,  875,  172,  740,\n",
       "        1961,  888, 1294,  183,  701,  483,  534, 1079, 1572,  834],\n",
       "       dtype=int32),\n",
       " array([ 888,  616,  482, 1075,  834, 1337,  731,  888, 1792,  856,  837,\n",
       "         921,  839,  740, 1961], dtype=int32),\n",
       " array([ 888,  616, 1966, 1009,  829, 1639, 1969, 1058, 1075,  419,  571,\n",
       "         931,  592,  834], dtype=int32),\n",
       " array([ 973, 1738,  936,  922,  983, 1959,   75, 1655,   67, 1972, 1112,\n",
       "         331,  188], dtype=int32),\n",
       " array([1197,  936, 1429,  922,  790, 1632,  740,  742,  541,  588, 1345,\n",
       "         888, 1792,  988, 1306,  378, 1067,   75, 1075, 1512,  701, 1701,\n",
       "        1473, 1288], dtype=int32),\n",
       " array([ 888,  616, 1921, 1742, 1584,  931,  258,  701, 1701, 1456,  588,\n",
       "        1345, 1294,  333, 1306,  834], dtype=int32),\n",
       " array([1969, 1058,  483, 1368,  931,  568, 1600, 1812, 1014, 1979, 1921,\n",
       "         740, 1885,   15], dtype=int32),\n",
       " array([ 616, 1726,  875, 1007, 1410, 1153, 1307,  834, 1312,  931, 1181,\n",
       "         267], dtype=int32),\n",
       " array([1197, 1841, 1353,  178, 1635,  834, 1701,  401,  184,    0,  169,\n",
       "         178,  323,  401,  110,  854,  355,  169, 1660,  960, 1614],\n",
       "       dtype=int32),\n",
       " array([ 888,  616, 1301, 1075,  998,  985,  834,  123,  271,  536, 1190,\n",
       "         612,  888, 1792], dtype=int32),\n",
       " array([ 246, 1882, 1891,  740, 1885,  834,  483,  504, 1079, 1890,  828,\n",
       "         875, 1342, 1960], dtype=int32),\n",
       " array([1197, 1998,  590, 1567, 1125, 1090,  224,  972,  968, 1639, 1974,\n",
       "        1448,  172,  998, 1436,  579,  928, 1065,  701, 1701,  796,  711,\n",
       "        1761,  834], dtype=int32),\n",
       " array([1197,  802,  408,  998,  807, 1187,   28, 1007,  878, 1950, 1714,\n",
       "         289,  343, 1587,  362, 1388,   12,  188, 1721,  701,  834],\n",
       "       dtype=int32),\n",
       " array([ 888,  616,  246, 1882, 1726,  659, 1834,  863,  875,  172,  740,\n",
       "        1961, 1928, 1555,  566,  888, 1294,  834], dtype=int32),\n",
       " array([1412, 1748, 1197, 1123, 1049, 1090,  229,  669,  808,  579,  723,\n",
       "         347, 1237,  974, 1388,  631, 1054,  834,  273, 1170, 1412, 1748],\n",
       "       dtype=int32),\n",
       " array([  40, 1874, 1379,  108, 1410, 1527,  641, 1836, 1388,  788, 1412,\n",
       "        1523, 1568], dtype=int32),\n",
       " array([1818,  616, 1921, 1230,  191, 1391, 1543,  666,  111,  267, 1590,\n",
       "         370, 1572,  166,  834, 1630, 1027,  974,  683], dtype=int32),\n",
       " array([1197, 1875, 1410,  242,  818,  788,  718, 1125,  413], dtype=int32),\n",
       " array([1197,  743, 1691, 1170, 1965,  968, 1639, 1731, 1436,  579,  883,\n",
       "         116, 1293, 1065,  701, 1701, 1242,  834], dtype=int32),\n",
       " array([1197,  548, 1330,  718,  476,   28, 1503, 1007,  525,  370,  988,\n",
       "        1159, 1012], dtype=int32),\n",
       " array([1197,  590,  808,  579, 1731, 1231,  229, 1965, 1630,  998, 1974,\n",
       "         579,  600,  983, 1701,  834], dtype=int32),\n",
       " array([ 616, 1123,  190,  255, 1187,  723,  347, 1237,  974, 1388,  631,\n",
       "        1054, 1007, 1701, 1412, 1748,  834], dtype=int32),\n",
       " array([1412, 1748,  616, 1123,  190,  255, 1187,  723,  347, 1237,  974,\n",
       "        1388,  631, 1054, 1007, 1701, 1412, 1748,  834], dtype=int32),\n",
       " array([1412, 1748,  616, 1630,  998,  353, 1941,  579,  723, 1287, 1516,\n",
       "        1157,   28, 1189, 1921, 1135,  354,  647, 1007, 1701, 1688, 1326],\n",
       "       dtype=int32),\n",
       " array([1425,  416,  981, 1003,  370,  362,  744, 1762, 1010, 1230,  191,\n",
       "        1598, 1606,   53], dtype=int32),\n",
       " array([1600, 1812, 1969, 1058,  483, 1368,  953,  834,   28, 1014, 1792,\n",
       "         362, 1342, 1632,  740,   14], dtype=int32),\n",
       " array([1096,  826, 1254, 1929,  812,  677,  627], dtype=int32),\n",
       " array([1965, 1630,  998,  353,  579, 1516, 1630,  998, 1938, 1007,  834,\n",
       "         956, 1630, 1388,  208, 1701,  666, 1189, 1326], dtype=int32),\n",
       " array([1396,  210, 1227, 1342, 1448,  928, 1832, 1718, 1150,  314,  700,\n",
       "        1128, 1209], dtype=int32),\n",
       " array([ 616, 1630,  998,  353, 1941,  579,  723, 1287, 1516, 1157,   28,\n",
       "        1189, 1921, 1135,  354,  647, 1007, 1701, 1688, 1326], dtype=int32),\n",
       " array([1197,  680, 1598, 1606,   75, 1966, 1555,  680,  408, 1966,  255,\n",
       "        1590,  370, 1164,  355, 1886, 1606, 1049, 1933, 1230,  191,  911,\n",
       "         669,  362, 1595, 1762,  834, 1425,  416,   85,   75,  680, 1701,\n",
       "         362,  498, 1007, 1572,  111,  548], dtype=int32),\n",
       " array([1598, 1606, 1230,  191,   53,  362, 1595, 1762, 1007, 1590,  370,\n",
       "         834, 1425,  416, 1636, 1818], dtype=int32),\n",
       " array([1197,   53,   53, 1447, 1008, 1907,  107,  166, 1590,  370,  911,\n",
       "         699,  936, 1066, 1619, 1220,  669, 1907, 1446,  845,  834,  548,\n",
       "        1572,  111,  946,  666, 1527,  362,  744,  362, 1762,  715, 1633,\n",
       "        1209,  971, 1788, 1170,  255, 1941, 1446,  845,  840], dtype=int32),\n",
       " array([1197, 1726,  659, 1017,  418,  754, 1007,  172,  740, 1961, 1701,\n",
       "         771, 1603,  106,  504, 1079, 1890, 1441, 1677, 1018,  884,  965,\n",
       "         468, 1508, 1966,  788, 1750, 1080,  659, 1616,  123, 1858,  751,\n",
       "         907,  138], dtype=int32),\n",
       " array([1197,  917,  705,   99,  408,  725,  669, 1342,   52,  294,  701,\n",
       "        1701,   28, 1189,  834], dtype=int32),\n",
       " array([1197, 1581, 1613, 1734, 1201,  111, 1964, 1227, 1193, 1347,  828],\n",
       "       dtype=int32),\n",
       " array([1197,  230,  419, 1368,  178, 1321, 1693, 1412,  362,  548,  338,\n",
       "         362,  936,  342,  960, 1626,  834], dtype=int32),\n",
       " array([1197, 1540, 1676,  834, 1007,  497, 1373,  370,   28,  701],\n",
       "       dtype=int32),\n",
       " array([1943, 1090,  931,   43,  834, 1559], dtype=int32),\n",
       " array([1726, 1891, 1153, 1307,  834, 1918,   24, 1338,  875, 1213,   28,\n",
       "         408, 1410], dtype=int32),\n",
       " array([ 123,  468,  592, 1266, 1366, 1932,  882,  988, 1984,  630,  210,\n",
       "        1979, 1157], dtype=int32),\n",
       " array([1197,  718,  604, 1237,  165, 1008, 1559,  834,  123, 1319, 1787],\n",
       "       dtype=int32),\n",
       " array([1197, 1562, 1527,    5,  337,    9,  413, 1008, 1863,   40, 1874,\n",
       "         252, 1095, 1616,  968, 1639,  265,  998,  632,  474, 1512, 1835,\n",
       "        1863, 1007, 1701,  834], dtype=int32),\n",
       " array([ 606,    0, 1598, 1606,  834, 1886,  370], dtype=int32),\n",
       " array([1818,  616, 1598, 1606,  437, 1007, 1590,  370,  362, 1595, 1762,\n",
       "         834,  691,  111,  267, 1567], dtype=int32),\n",
       " array([1303, 1616, 1618, 1527, 1939,  375, 1590,  370, 1564,  288,  834],\n",
       "       dtype=int32),\n",
       " array([1197, 1382, 1921, 1572,  688,  834], dtype=int32),\n",
       " array([1226, 1691, 1740, 1471,   28, 1347,  536, 1410,  707,  633, 1367,\n",
       "         718,  834, 1767], dtype=int32),\n",
       " array([ 624, 1553,  436,  936,  922, 1760,  737, 1125, 1980,  751, 1410,\n",
       "        1966], dtype=int32),\n",
       " array([1197,  997, 1616, 1633, 1864,  931,  860,  736, 1446,  845, 1966,\n",
       "         239, 1572,  166, 1590,  370], dtype=int32),\n",
       " array([ 548, 1574,  473,  681, 1105,  408,  348,  508,  508,  968,  508,\n",
       "         508,  899, 1156,  625,  620,  202,  473,  911,  754, 1941,   41,\n",
       "        1289,  130, 1508], dtype=int32),\n",
       " array([  19,   75,  936, 1127,  613,  922, 1806, 1396, 1572,  255,   24,\n",
       "        1338, 1109,  408,  998,  374, 1574,  473, 1489, 1427, 1451, 1966,\n",
       "          75, 1630,  677], dtype=int32),\n",
       " array([  53,  707,  633,  523,  255,  828,  845, 1853,  733,  714, 1423,\n",
       "         963,  931,  971,  983,   96,  468, 1811,  922,  677,  839,  963,\n",
       "         931,   95,  628,  931, 1559,   28, 1342, 1564,  864], dtype=int32),\n",
       " array([ 802,  408,  998,  807, 1187,   28, 1007,  878, 1950,  289,  343,\n",
       "        1460, 1481, 1289,  130,  208,  701, 1701, 1721,  733,  188,  362,\n",
       "        1388,   12], dtype=int32),\n",
       " array([1574,  473, 1207, 1289,  130,  767, 1566, 1691, 1274,  362, 1967,\n",
       "         972,    6,  271,  554,  733,   12,  362, 1388, 1109], dtype=int32),\n",
       " array([ 472, 1201, 1049, 1251,  625, 1907], dtype=int32),\n",
       " array([1778,  568, 1908, 1623, 1155, 1017,  290,  408], dtype=int32),\n",
       " array([1055,  178, 1170, 1426,   46,  378, 1545, 1714, 1189,   28, 1630,\n",
       "         584, 1572, 1007,  294,  282,   28,  470, 1767, 1388, 1969, 1055,\n",
       "         666], dtype=int32),\n",
       " array([ 888, 1003,  733, 1785,  893, 1616, 1075, 1792, 1156,  602,  337,\n",
       "         207, 1131,  898, 1227, 1841, 1701], dtype=int32),\n",
       " array([ 884, 1540,  413, 1312, 1150,  666,  839, 1572,  381, 1033,  701,\n",
       "        1701,  733,   28, 1555, 1075,  483, 1368,  888], dtype=int32),\n",
       " array([ 619,  377,  878,  523,  884,  860,  993,  390, 1280,  666, 1527,\n",
       "        1115,  107,  733, 1405], dtype=int32),\n",
       " array([ 240,  483,  419,  523,  884,   94,  733, 1075, 1433, 1918,  828,\n",
       "         262,  834,  612], dtype=int32),\n",
       " array([ 888,  292,  394,  214,  240,  566,  931,   28,  828,  765, 1222,\n",
       "         457,  309,  504,  593,   85,  828,  875,  682,  124, 1456,  171,\n",
       "         160,  450,  252,  483, 1079, 1890,  946,  691, 1350,  246, 1882],\n",
       "       dtype=int32),\n",
       " array([1199,  107,  666, 1540, 1760, 1033, 1921,  867,  931,  701, 1306,\n",
       "         242,  733], dtype=int32),\n",
       " array([ 888, 1456,  171,  160, 1766,  562,  123,  271,  828,  875,  682,\n",
       "         666,  733, 1075,  504,  450,  240,   25,  179,  160], dtype=int32),\n",
       " array([ 888,  123, 1075, 1936,  394, 1381,  214, 1163,  508, 1388,  405,\n",
       "         508, 1614,  508, 1280,  806,  508, 1875,  733,  408], dtype=int32),\n",
       " array([ 888,  680,  408, 1006,  107, 1767, 1128,  647, 1075, 1792, 1734,\n",
       "        1237, 1006, 1677,   28, 1394,  403,   28, 1394, 1049, 1886],\n",
       "       dtype=int32),\n",
       " array([ 888,  616, 1726,  659,  408,  740, 1891,  408,  172,  267,  965,\n",
       "         568, 1750, 1080,  140,  890, 1114, 1704, 1128, 1110, 1694,  534,\n",
       "        1079, 1572,  826], dtype=int32),\n",
       " array([1197, 1287, 1516,  413, 1688,   28, 1189,  409,  354,   10,  567,\n",
       "        1639,  812, 1694, 1326], dtype=int32),\n",
       " array([ 888,  616,  893,  653,  465,  260,  738, 1887, 1891, 1630, 1555,\n",
       "         888, 1294,  123,  766, 1571,   13,  463,    8, 1875,  282],\n",
       "       dtype=int32),\n",
       " array([ 888,  616,   77,  820,  898, 1075, 1792, 1502,  826,  123, 1571,\n",
       "         184,  207, 1630, 1941,  653, 1964], dtype=int32),\n",
       " array([ 702,  616, 1598, 1606,  437, 1007, 1590,  370,  362, 1595, 1762,\n",
       "         834,  691,  111,  267, 1567], dtype=int32),\n",
       " array([ 888,  616, 1921,  161,  541, 1614,  826,  860, 1066,  931, 1151,\n",
       "        1075,  534, 1890], dtype=int32),\n",
       " array([ 888,  616,  691,  478, 1429,  790,  536,  666,  166, 1075,  483,\n",
       "         525,  701,  826], dtype=int32),\n",
       " array([1197, 1075,  483, 1368, 1306, 1119,  771,  590,  107, 1189,  806,\n",
       "        1412, 1611,  924, 1632,  828,  815], dtype=int32),\n",
       " array([1197, 1302,  784,  315, 1853,  931,  178,   48, 1469,  178, 1210,\n",
       "         826, 1527, 1158, 1318,  377,  919, 1354, 1614], dtype=int32),\n",
       " array([ 888,  616,  893,  653,  465,  260,  738, 1887, 1891, 1630, 1555,\n",
       "         888, 1294,  826,  766, 1571,  669,   13,  463,    8, 1995],\n",
       "       dtype=int32),\n",
       " array([1197, 1412, 1523, 1568,  433,   28,  166, 1450,  154, 1652,  826,\n",
       "         543, 1829, 1237], dtype=int32),\n",
       " array([ 971, 1788,  476,  666,  121, 1347, 1446,  845,  931,  618,   91,\n",
       "        1630, 1409,  826,  931,  202,  845,  119,  812, 1950], dtype=int32),\n",
       " array([1686,  430,  207,  584, 1368, 1347, 1963, 1267, 1341,  888],\n",
       "       dtype=int32),\n",
       " array([ 161,  386,  392,  363, 1571,   77,   19, 1504, 1819,  504,  828,\n",
       "         765,  888, 1792], dtype=int32),\n",
       " array([1338,   26, 1551, 1435,   19,  900,  304,  337, 1900, 1215,  669,\n",
       "         900], dtype=int32),\n",
       " array([1197,  816, 1979, 1201, 1186,  120, 1244, 1126,  123,  252, 1342,\n",
       "        1394, 1430,  385, 1886,  669, 1160,  384], dtype=int32),\n",
       " array([1197, 1435,   19,  788,  883,  116, 1341,   26, 1551], dtype=int32),\n",
       " array([1242,  619, 1192, 1242,  826, 1693,  408,  548,  338, 1689,  931,\n",
       "         146, 1422, 1026], dtype=int32),\n",
       " array([ 616, 1412, 1523, 1568,  826, 1626, 1912,   28,  166, 1450,  154,\n",
       "        1652, 1689,  931,  146, 1829, 1237, 1026, 1936], dtype=int32),\n",
       " array([ 888,  616,  519,  233, 1750, 1080,  271, 1134,  249, 1726,  659,\n",
       "         483,  504, 1079,  826, 1969, 1058,  924,  806, 1456, 1961, 1098,\n",
       "        1555,  362, 1075, 1792], dtype=int32),\n",
       " array([ 888,  616,  519,  233, 1750, 1080,  271, 1134,  249, 1726,  659,\n",
       "         483,  504, 1079,  826, 1969, 1058,  924,  806, 1456, 1961, 1098,\n",
       "        1555,  362, 1075, 1792], dtype=int32),\n",
       " array([1197, 1354,  828, 1370, 1677,  595,  107,  548,  337,    0,  438,\n",
       "          81, 1598, 1606], dtype=int32),\n",
       " array([ 289,  982, 1566,  616,   48, 1405, 1006, 1589,  765,  630, 1715,\n",
       "         231,  826,  312, 1614], dtype=int32),\n",
       " array([ 888,  616,  806, 1750, 1080, 1891,  740, 1885,  483,  534, 1079,\n",
       "        1508,  888, 1294,  826], dtype=int32),\n",
       " array([ 616, 1137,   26, 1551,   19, 1021,  968, 1639, 1716, 1950, 1527,\n",
       "        1841, 1953,  900,  788, 1424,  826, 1701,  886,  125, 1125, 1736,\n",
       "          19], dtype=int32),\n",
       " array([1197, 1242, 1524,  473, 1553, 1400,  826, 1630, 1792, 1003, 1649,\n",
       "        1388,  788, 1688], dtype=int32),\n",
       " array([ 616, 1137,   26, 1551,   19, 1021,  968, 1639, 1716, 1950, 1527,\n",
       "        1841, 1953,  900,  788, 1424,  826, 1701,  886,  125, 1125, 1736,\n",
       "          19], dtype=int32),\n",
       " array([1197, 1493, 1144, 1239,  507,  821,  604, 1518,  152, 1201, 1617,\n",
       "         472, 1555, 1527,   28, 1075, 1014,  342,  209], dtype=int32),\n",
       " array([ 888,  616, 1740, 1075,  333,  454, 1714, 1412,  931, 1414,  806,\n",
       "        1007,  788,  924,  826], dtype=int32),\n",
       " array([1197, 1966, 1341,  366, 1018, 1616, 1988,  826, 1722,  165,  913,\n",
       "        1929,   95], dtype=int32),\n",
       " array([1197,    4,  845,  765,  554,  623,  736,   41,  705, 1423, 1886,\n",
       "         548, 1196], dtype=int32),\n",
       " array([1197,  860,  264, 1410, 1150,  620, 1330, 1412,  936,  922, 1614,\n",
       "        1754,  884,  732,  107, 1410,  408, 1811,  922,  632,  325, 1669,\n",
       "         123,  255, 1685, 1102, 1562], dtype=int32),\n",
       " array([ 742,  541,  290,  607,  924,   28,  166, 1306], dtype=int32),\n",
       " array([1197,  693, 1776, 1479, 1908, 1966, 1678,  625,  395, 1353,  239,\n",
       "          14,  548,  533, 1891,  680,  408, 1388, 1637, 1506, 1528, 1732,\n",
       "        1599,  931,  252,  326,  960,  936, 1360, 1406,  922], dtype=int32),\n",
       " array([1197, 1396,  891,  493,  101,  826,  848,  844, 1476, 1388,  728,\n",
       "        1873], dtype=int32),\n",
       " array([1197,  654, 1892, 1115,  501, 1732, 1599, 1410], dtype=int32),\n",
       " array([ 888,  616,  742,  541,  785,  666, 1075,  988,  740,  261,  826,\n",
       "         123, 1529,  568,  977, 1436,  607,   28,  166], dtype=int32),\n",
       " array([ 888,  616, 1456,  571, 1760,  123,  924, 1969, 1058, 1075,  483,\n",
       "         931,  740,   28, 1342,  342,  826], dtype=int32),\n",
       " array([1197,  987,  342, 1716, 1012, 1874, 1885, 1529, 1921,  483,  571,\n",
       "         533, 1128, 1891, 1075, 1120,  826, 1090,  548,  736,  457, 1885,\n",
       "          28, 1342,   14, 1792], dtype=int32),\n",
       " array([ 888,  616, 1834,  907,  138,   60, 1075,  740, 1961, 1701, 1726,\n",
       "         659, 1921, 1074, 1527,  691,  759, 1573, 1110, 1508,  888, 1792,\n",
       "         826], dtype=int32),\n",
       " array([ 888,  616,  998, 1284,  252, 1049, 1304,  123, 1891,  740, 1885,\n",
       "         419,  534,  888, 1294,  826], dtype=int32),\n",
       " array([ 806, 1750, 1080, 1760,  140,  890, 1114, 1704, 1752, 1007,  759,\n",
       "         690, 1508, 1294, 1726,  659,  946,  691], dtype=int32),\n",
       " array([ 777, 1936,  366,  135,  826, 1912, 1135,  338, 1527, 1464,  255,\n",
       "          95], dtype=int32),\n",
       " array([1807,  997, 1723,  971, 1788,   91, 1409,  548, 1969, 1058, 1446,\n",
       "         845,  931], dtype=int32),\n",
       " array([ 888,  616,  331,  491, 1258, 1921,  782, 1369, 1742,  214, 1893,\n",
       "         395,  231, 1163,  888, 1792], dtype=int32),\n",
       " array([ 888,  616, 1294,  844,  931], dtype=int32),\n",
       " array([1197,  632,    0, 1159, 1968,  424,  142,  294,  329], dtype=int32),\n",
       " array([1197,  883,  116, 1635,  178,  826,  162,   52,  111, 1135,  726,\n",
       "        1733], dtype=int32),\n",
       " array([ 888,  616, 1750, 1080,  476,  806, 1934,  456, 1255,  233,  123,\n",
       "        1891, 1508,  888,  826], dtype=int32),\n",
       " array([1197, 1599, 1163, 1527, 1650, 1441, 1122, 1227, 1864,  974, 1342,\n",
       "        1966, 1341,   74], dtype=int32),\n",
       " array([ 616,  632,    0, 1159,  826, 1428,  294,  329, 1701, 1228,  142,\n",
       "        1904, 1968,  424,  340, 1380, 1711, 1135, 1550,  990,  984],\n",
       "       dtype=int32),\n",
       " array([1103,  619, 1192,  632,    0, 1159,  826, 1428,  294,  329, 1701,\n",
       "        1228,  142, 1904, 1968,  424,  340, 1380, 1711, 1135, 1550,  990,\n",
       "         984], dtype=int32),\n",
       " array([1197,  707,  633, 1367,  366, 1135,  826,  974, 1163, 1527,  373,\n",
       "         931, 1492, 1567], dtype=int32),\n",
       " array([1197,  632,    0, 1159, 1428,  826,  788,  294,  329, 1968,  424,\n",
       "         340, 1904, 1936,  931,  632, 1550,  990,  984], dtype=int32),\n",
       " array([1197,   48,  740,   28,  352,  382,   28, 1567, 1249,  666,  263],\n",
       "       dtype=int32),\n",
       " array([1197,  823,   70, 1891,  907,  138, 1278, 1726,  659,  754,   60,\n",
       "         740, 1961,  261, 1527, 1150,  362, 1790, 1338,  320,  826, 1767,\n",
       "         759,  690, 1508, 1075, 1865, 1239, 1239, 1694,  806, 1750, 1080,\n",
       "         504, 1079], dtype=int32),\n",
       " array([1197, 1630,  342,   14,  107,  180, 1227,  894, 1964,  162,  165,\n",
       "         641], dtype=int32),\n",
       " array([1818,  616, 1446,  845, 1409,  997,  971, 1788,   91,  812, 1950,\n",
       "        1345,  788,  119], dtype=int32),\n",
       " array([1197,  907,  180,  172, 1187,  807, 1965, 1657,  235, 1943,  362,\n",
       "         998,  309,  579,  271, 1448,  928,  826, 1767], dtype=int32),\n",
       " array([1197, 1007, 1701,  826, 1767,   28,  166, 1932,  882,  988, 1984],\n",
       "       dtype=int32),\n",
       " array([1302,  784, 1853,  740, 1885, 1294,  476,  178, 1469, 1630,  342,\n",
       "        1716], dtype=int32),\n",
       " array([1197,  715, 1638, 1904,  600, 1520, 1630, 1049,  658, 1512,  743,\n",
       "         554, 1836,  701, 1701,  826, 1767], dtype=int32),\n",
       " array([1197,  459, 1965,  353, 1181,  579, 1566,    6,  863, 1054, 1090,\n",
       "        1792,  701, 1701, 1688, 1326,  826, 1767], dtype=int32),\n",
       " array([1197, 1151, 1965, 1567,  457,  998,  826, 1767,  718, 1388,  289],\n",
       "       dtype=int32),\n",
       " array([ 290,  680,  691, 1342, 1792, 1115, 1396,  907,  172, 1764, 1187,\n",
       "         928,  826, 1767], dtype=int32),\n",
       " array([1201,   28,  842, 1559,  826, 1527, 1939,   28, 1630, 1418,   95,\n",
       "         628], dtype=int32),\n",
       " array([  53,  666,  548, 1129,  554, 1082,  723,  701, 1701, 1756,  826],\n",
       "       dtype=int32),\n",
       " array([ 715, 1638, 1904,  600, 1520, 1630,  826, 1767], dtype=int32),\n",
       " array([ 632,  331,  826,  949,  936, 1318,    0,  275, 1076,  146,  861,\n",
       "        1967, 1952], dtype=int32),\n",
       " array([1197, 1122, 1574,  473,  180,  229,  273, 1242, 1524,  473],\n",
       "       dtype=int32),\n",
       " array([1630, 1350,  680,  408, 1950,  413,  839, 1135,    0, 1159, 1929,\n",
       "         990, 1098, 1873], dtype=int32),\n",
       " array([1197, 1691,  241,  457,  968, 1639,  998, 1835,  983,  271, 1293,\n",
       "         826, 1767,   79, 1929,  883,  116], dtype=int32),\n",
       " array([1412, 1748,  616,  907,  180,  172, 1764, 1187, 1657,  235,   28,\n",
       "        1189, 1921, 1135, 1273,  647, 1007, 1701, 1448,  928,  826],\n",
       "       dtype=int32),\n",
       " array([ 616, 1691,  309, 1342,  107, 1090, 1792, 1274, 1476, 1967,  707,\n",
       "         633, 1367, 1756,  313,  315, 1135,  683, 1914,  826], dtype=int32),\n",
       " array([1412, 1748,  616, 1691,  309, 1342,  107, 1090, 1792, 1274, 1476,\n",
       "        1967,  707,  633, 1367, 1756,  313,  315, 1135,  683, 1914,  826],\n",
       "       dtype=int32),\n",
       " array([1197, 1599, 1163, 1527, 1650, 1441, 1122, 1227, 1864,  974, 1342,\n",
       "        1966, 1341,   74], dtype=int32),\n",
       " ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_articles_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the labels `Y` for train, dev and test sets into arrays: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:03:13.183996Z",
     "start_time": "2020-04-02T15:03:13.077575Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Extracts values from dataframe column 'Class' in train_df, dev_df, \n",
    "   test_df and converts them into a numpy array. The class labels are integers starting \n",
    "   from 1. The -1 operation subtracts 1 from each class label that converts\n",
    "   them to a zero-indexed array'''\n",
    "train_label = np.array(train_df['Class'])\n",
    "train_label= train_label-1\n",
    "dev_label = np.array(dev_df['Class'])\n",
    "dev_label=dev_label-1\n",
    "test_label = np.array(test_df['Class'])\n",
    "test_label=test_label-1\n",
    "train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture\n",
    "\n",
    "Your network should pass each word index into its corresponding embedding by looking-up on the embedding matrix and then compute the first hidden layer $\\mathbf{h}_1$:\n",
    "\n",
    "$$\\mathbf{h}_1 = \\frac{1}{|x|}\\sum_i W^e_i, i \\in x$$\n",
    "\n",
    "where $|x|$ is the number of words in the document and $W^e$ is an embedding matrix $|V|\\times d$, $|V|$ is the size of the vocabulary and $d$ the embedding size.\n",
    "\n",
    "Then $\\mathbf{h}_1$ should be passed through a ReLU activation function:\n",
    "\n",
    "$$\\mathbf{a}_1 = relu(\\mathbf{h}_1)$$\n",
    "\n",
    "Finally the hidden layer is passed to the output layer:\n",
    "\n",
    "\n",
    "$$\\mathbf{y} = \\text{softmax}(\\mathbf{a}_1W) $$ \n",
    "where $W$ is a matrix $d \\times |{\\cal Y}|$, $|{\\cal Y}|$ is the number of classes.\n",
    "\n",
    "During training, $\\mathbf{a}_1$ should be multiplied with a dropout mask vector (elementwise) for regularisation before it is passed to the output layer.\n",
    "\n",
    "You can extend to a deeper architecture by passing a hidden layer to another one:\n",
    "\n",
    "$$\\mathbf{h_i} = \\mathbf{a}_{i-1}W_i $$\n",
    "\n",
    "$$\\mathbf{a_i} = relu(\\mathbf{h_i}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Training\n",
    "\n",
    "First we need to define the parameters of our network by initiliasing the weight matrices. For that purpose, you should implement the `network_weights` function that takes as input:\n",
    "\n",
    "- `vocab_size`: the size of the vocabulary\n",
    "- `embedding_dim`: the size of the word embeddings\n",
    "- `hidden_dim`: a list of the sizes of any subsequent hidden layers. Empty if there are no hidden layers between the average embedding and the output layer \n",
    "- `num_classes`: the number of the classes for the output layer\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: a dictionary mapping from layer index (e.g. 0 for the embedding matrix) to the corresponding weight matrix initialised with small random numbers (hint: use numpy.random.uniform with from -0.1 to 0.1)\n",
    "\n",
    "Make sure that the dimensionality of each weight matrix is compatible with the previous and next weight matrix, otherwise you won't be able to perform forward and backward passes. Consider also using np.float32 precision to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:41:20.918617Z",
     "start_time": "2020-04-02T15:41:20.915597Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''This function returns the dictionary of randomly initialized weights \n",
    "   for a neural network with an input layer, one or more hidden layers, and an output layer. \n",
    "   The I/P layer 'vocab_size' represents the number of unique words \n",
    "   or ngrams in the vocabulary. The O/P layer 'num_classes' represents the number of classes which will be classified \n",
    "   by a neural network. If hidden_dim is an empty list, the neural network will only have an i/p layer and an o/p layer.'''\n",
    "def network_weights(vocab_size=2000, embedding_dim=300, \n",
    "                    hidden_dim=[], num_classes=3, init_val = 0.1):\n",
    "    layers_dim=[vocab_size]+[embedding_dim]+hidden_dim+[num_classes]\n",
    "    W={}\n",
    "    for input_layer in range(len(layers_dim)-1):\n",
    "        W[input_layer]= np.random.uniform(-init_val, init_val, (layers_dim[input_layer],layers_dim[input_layer+1])).astype(np.float32)\n",
    "\n",
    "    return W\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:48.636732Z",
     "start_time": "2020-04-02T14:26:48.634122Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([[-0.02637493, -0.09433637, -0.06779089, ..., -0.02193159,\n",
       "         -0.09183982, -0.08483312],\n",
       "        [-0.01634685,  0.01913789,  0.09112325, ..., -0.07781994,\n",
       "         -0.01977547,  0.08452381],\n",
       "        [ 0.04183768,  0.0679256 ,  0.07946683, ..., -0.05193486,\n",
       "          0.01428006, -0.03478409],\n",
       "        ...,\n",
       "        [-0.00028373,  0.03143086,  0.0131515 , ...,  0.01424241,\n",
       "          0.06348188,  0.00103876],\n",
       "        [-0.08830725,  0.01619583, -0.08653948, ..., -0.00863065,\n",
       "          0.06186633,  0.09729445],\n",
       "        [ 0.01061703, -0.02167895,  0.09023168, ...,  0.04236311,\n",
       "          0.01756194, -0.06356017]], dtype=float32),\n",
       " 1: array([[ 1.34228431e-02,  9.28744152e-02, -3.62130105e-02],\n",
       "        [-2.29819063e-02, -6.07093684e-02, -1.72710381e-02],\n",
       "        [-5.30987605e-02,  7.23892003e-02, -5.91134429e-02],\n",
       "        [ 9.31851193e-02,  9.32889357e-02, -4.41807285e-02],\n",
       "        [-5.05105741e-02, -4.53830976e-03, -7.49312043e-02],\n",
       "        [-8.45093355e-02,  4.45363373e-02,  2.63765696e-02],\n",
       "        [-6.33421019e-02, -6.46630749e-02,  9.11137164e-02],\n",
       "        [ 6.89098388e-02, -1.15626054e-02,  9.39684212e-02],\n",
       "        [-9.17792320e-02,  6.07857220e-02, -1.15125831e-02],\n",
       "        [ 5.51716313e-02, -9.06812549e-02,  9.69284028e-02],\n",
       "        [-5.95776848e-02,  6.10478856e-02,  7.38038346e-02],\n",
       "        [ 3.56249548e-02,  2.40198374e-02, -4.93938960e-02],\n",
       "        [ 3.15227509e-02,  1.75795592e-02,  7.13626146e-02],\n",
       "        [-7.93489739e-02, -5.76978885e-02,  5.62206358e-02],\n",
       "        [-6.69474304e-02,  8.56581181e-02, -5.30647002e-02],\n",
       "        [-7.46655986e-02,  6.62082657e-02,  6.40015379e-02],\n",
       "        [-9.03576687e-02,  5.44257089e-02,  1.93693992e-02],\n",
       "        [ 5.67222759e-02, -6.49368167e-02,  8.32749978e-02],\n",
       "        [-2.93667819e-02,  7.48486146e-02,  4.73491102e-02],\n",
       "        [ 4.83857468e-02,  9.94643383e-03, -5.02133556e-02],\n",
       "        [-5.57235293e-02, -4.72281948e-02, -1.62997041e-02],\n",
       "        [-8.71232897e-02,  8.72503147e-02,  8.54658894e-03],\n",
       "        [ 1.10379206e-02, -4.19370318e-03,  5.87792769e-02],\n",
       "        [-4.04871553e-02,  6.33595288e-02, -8.11736956e-02],\n",
       "        [ 1.41949151e-02,  4.58693057e-02, -6.77957833e-02],\n",
       "        [-3.81209003e-03,  2.12023072e-02,  8.33641216e-02],\n",
       "        [-8.86257514e-02,  8.58030096e-02,  8.73687398e-03],\n",
       "        [-9.47101414e-02,  2.65586451e-02,  1.31116007e-02],\n",
       "        [-8.80337283e-02,  3.42733301e-02,  4.99349870e-02],\n",
       "        [ 1.01631852e-02,  8.86174478e-03,  5.77235520e-02],\n",
       "        [-1.42931202e-02, -3.45530584e-02,  4.18761447e-02],\n",
       "        [-9.96882021e-02,  1.33409323e-02, -1.89518705e-02],\n",
       "        [-7.15282047e-03,  7.44213089e-02,  4.36828472e-02],\n",
       "        [-7.11252466e-02, -3.08646797e-03,  1.57353897e-02],\n",
       "        [ 6.62093237e-02,  6.28013089e-02, -1.85575541e-02],\n",
       "        [-2.47266050e-02, -1.54257268e-02, -4.93898168e-02],\n",
       "        [-7.62545243e-02, -1.84150357e-02,  2.42955219e-02],\n",
       "        [-5.13589159e-02,  3.35605331e-02, -5.03095165e-02],\n",
       "        [ 3.51390243e-02, -9.99142751e-02,  2.65707332e-03],\n",
       "        [ 7.20597580e-02, -1.89122334e-02, -9.22055691e-02],\n",
       "        [-5.53015545e-02, -7.61452019e-02, -7.45129362e-02],\n",
       "        [-6.11834154e-02,  2.45385133e-02, -3.57486717e-02],\n",
       "        [ 4.51321676e-02,  1.44264465e-02,  8.91896188e-02],\n",
       "        [-3.45994793e-02, -1.33454734e-02, -1.87490825e-02],\n",
       "        [-2.27271654e-02,  9.67052728e-02,  9.49425772e-02],\n",
       "        [-2.16231998e-02,  3.73887159e-02, -9.02743936e-02],\n",
       "        [ 4.09858450e-02, -3.41000743e-02,  2.22990811e-02],\n",
       "        [ 2.62436084e-02, -9.81832594e-02,  1.99112040e-03],\n",
       "        [ 2.56141983e-02, -3.14858928e-02, -3.44174402e-03],\n",
       "        [ 1.82751771e-02, -6.12101331e-02,  7.74389356e-02],\n",
       "        [-2.45214216e-02,  1.67135429e-02,  7.71199241e-02],\n",
       "        [ 9.43213329e-02,  7.38727078e-02,  9.13689584e-02],\n",
       "        [ 5.72718233e-02, -2.63596103e-02, -1.49225127e-02],\n",
       "        [ 6.33092364e-04,  2.92931441e-02, -2.79669324e-03],\n",
       "        [-7.24151544e-03, -3.68151404e-02, -1.00882212e-02],\n",
       "        [ 9.84000266e-02,  6.00762516e-02,  2.74083260e-02],\n",
       "        [ 3.35392095e-02, -8.10893402e-02,  5.61277829e-02],\n",
       "        [ 1.44960834e-02, -5.31603731e-02, -5.19390777e-03],\n",
       "        [-6.12203702e-02,  8.69579315e-02,  2.39997972e-02],\n",
       "        [-5.99870197e-02,  1.00108562e-02, -7.12993219e-02],\n",
       "        [ 9.75368842e-02, -4.42769863e-02,  9.98428389e-02],\n",
       "        [ 2.03255173e-02, -1.30556757e-04, -5.39045930e-02],\n",
       "        [ 9.70259085e-02,  3.55828442e-02,  7.05545349e-03],\n",
       "        [ 3.11045386e-02,  8.83663595e-02, -9.89744067e-02],\n",
       "        [ 7.83478171e-02,  2.44909152e-03,  5.81478933e-03],\n",
       "        [-4.31464538e-02, -9.01764631e-02, -9.65704396e-02],\n",
       "        [ 6.77644834e-03,  8.48938972e-02,  9.28706154e-02],\n",
       "        [ 7.52345994e-02,  1.54507486e-02, -3.98679040e-02],\n",
       "        [ 6.52848855e-02,  2.19366066e-02,  8.18784162e-02],\n",
       "        [ 3.58273797e-02, -9.15639326e-02,  8.08592588e-02],\n",
       "        [ 1.93504933e-02, -7.86408484e-02,  2.11769175e-02],\n",
       "        [-4.08656187e-02,  6.18269481e-03, -5.08729406e-02],\n",
       "        [ 9.47782844e-02,  6.50642142e-02,  1.50793083e-02],\n",
       "        [ 7.75088891e-02,  4.21245433e-02,  2.52690148e-02],\n",
       "        [-9.98589844e-02,  1.13789728e-02, -4.77115214e-02],\n",
       "        [-4.66002151e-02,  3.68392952e-02,  1.20947380e-02],\n",
       "        [ 6.95494115e-02, -7.43407607e-02,  4.79199439e-02],\n",
       "        [ 5.58310486e-02, -5.94407804e-02,  9.37093720e-02],\n",
       "        [-5.83396852e-02, -4.63795923e-02,  6.60039783e-02],\n",
       "        [ 8.80562067e-02, -7.47811943e-02, -8.51149037e-02],\n",
       "        [ 4.04327624e-02,  4.63285623e-03, -4.17484343e-02],\n",
       "        [-5.64469323e-02,  1.00027826e-02,  9.29442048e-02],\n",
       "        [ 6.00989610e-02,  6.33108094e-02,  7.67501742e-02],\n",
       "        [-1.31524978e-02,  8.31204206e-02,  6.47427561e-03],\n",
       "        [ 6.17115535e-02,  6.22760924e-03, -4.57305014e-02],\n",
       "        [-5.78232110e-02,  2.43496965e-03, -9.30803269e-02],\n",
       "        [ 4.11338583e-02,  3.80901657e-02,  2.67552827e-02],\n",
       "        [-9.16116759e-02,  8.13428909e-02, -6.85566962e-02],\n",
       "        [-3.67093347e-02,  6.57840632e-03,  8.64247754e-02],\n",
       "        [ 4.10719998e-02,  3.60335074e-02, -4.26547527e-02],\n",
       "        [ 5.22642508e-02,  3.30045111e-02, -4.84992303e-02],\n",
       "        [-5.30457161e-02, -9.54050124e-02, -3.45926434e-02],\n",
       "        [ 5.95230460e-02,  9.54605415e-02, -4.58163843e-02],\n",
       "        [ 8.35455060e-02, -6.60510436e-02, -8.51711184e-02],\n",
       "        [-9.72229987e-02,  9.14235935e-02,  1.17769623e-05],\n",
       "        [-7.28854463e-02, -3.01751494e-02, -2.21036170e-02],\n",
       "        [ 2.97771636e-02,  1.29236132e-02,  5.08070365e-02],\n",
       "        [ 1.20276576e-02,  7.97069818e-02,  1.64238084e-02],\n",
       "        [-1.55940382e-02, -9.93562266e-02, -2.31544375e-02],\n",
       "        [-6.84568211e-02, -2.38854848e-02,  9.85804871e-02],\n",
       "        [-5.22933751e-02, -4.81728688e-02, -4.87646181e-03],\n",
       "        [ 8.62193555e-02, -2.53362730e-02, -9.54410899e-03],\n",
       "        [-1.63453426e-02, -9.34906974e-02, -2.51494758e-02],\n",
       "        [ 3.73056792e-02,  9.68528464e-02,  8.35907459e-02],\n",
       "        [ 3.17043364e-02, -8.69076848e-02, -3.58160213e-02],\n",
       "        [-7.13499337e-02,  6.79340884e-02,  5.22549041e-02],\n",
       "        [-9.51724872e-02,  7.41623193e-02, -3.65652181e-02],\n",
       "        [ 5.39833382e-02,  9.87110212e-02,  7.82206189e-03],\n",
       "        [-4.63771112e-02, -1.70895606e-02,  8.16007107e-02],\n",
       "        [-7.07630590e-02,  1.30127091e-02, -1.06880171e-02],\n",
       "        [-8.18509422e-03, -3.79811134e-03, -8.30176659e-03],\n",
       "        [ 7.30351135e-02, -3.57205234e-02, -7.67210647e-02],\n",
       "        [-6.54106140e-02, -9.98884887e-02,  3.37630846e-02],\n",
       "        [ 8.33358243e-02, -3.01976949e-02,  7.49243349e-02],\n",
       "        [-6.27343208e-02, -3.90287377e-02,  7.43984878e-02],\n",
       "        [-7.48520494e-02,  2.61846911e-02, -5.34006618e-02],\n",
       "        [ 2.79436782e-02, -8.59727189e-02, -1.84515137e-02],\n",
       "        [-9.26451236e-02,  4.91228588e-02, -6.26649102e-03],\n",
       "        [ 8.75558034e-02, -2.26634322e-03, -5.89173138e-02],\n",
       "        [ 6.15711436e-02,  1.72744580e-02,  3.31439897e-02],\n",
       "        [ 2.54041199e-02,  4.23226645e-03,  4.39430922e-02],\n",
       "        [-4.04086486e-02, -1.38934478e-02,  5.38578443e-03],\n",
       "        [-2.81328727e-02,  6.85022548e-02, -3.97958606e-02],\n",
       "        [ 7.51553327e-02,  8.16582516e-02, -8.08021650e-02],\n",
       "        [ 5.70490547e-02,  1.85690187e-02,  8.25239345e-02],\n",
       "        [-8.75513554e-02,  8.37699324e-02,  3.68052125e-02],\n",
       "        [-6.11840822e-02, -1.54735707e-02, -5.48489951e-02],\n",
       "        [-6.07096078e-03, -8.46624523e-02, -9.38567594e-02],\n",
       "        [ 3.49972807e-02,  4.17604484e-02,  3.96309458e-02],\n",
       "        [-2.35412661e-02,  8.22786707e-03,  5.06696217e-02],\n",
       "        [-3.82363843e-03, -6.47659078e-02,  2.28694035e-03],\n",
       "        [ 1.05680488e-02,  2.37865979e-03, -9.61883962e-02],\n",
       "        [ 6.67946190e-02,  6.08656369e-02,  9.58393048e-03],\n",
       "        [-6.07002527e-03, -5.66794761e-02,  8.58110487e-02],\n",
       "        [-4.03395258e-02, -3.07254288e-02,  9.83414724e-02],\n",
       "        [ 5.68264574e-02, -8.63900781e-02, -2.33242121e-02],\n",
       "        [ 7.71912979e-03, -1.70679074e-02, -8.79098848e-02],\n",
       "        [ 2.27954537e-02, -8.75972286e-02,  6.27584523e-03],\n",
       "        [ 7.80713856e-02, -9.17350724e-02,  9.21787024e-02],\n",
       "        [ 5.34409620e-02,  7.21192285e-02,  4.31466196e-03],\n",
       "        [ 2.51631942e-02,  2.59151664e-02,  1.58171337e-02],\n",
       "        [-2.12427415e-03,  8.39825496e-02,  2.57134531e-02],\n",
       "        [-5.31022511e-02, -5.65348268e-02, -1.56875998e-02],\n",
       "        [-3.86869013e-02,  5.37976734e-02,  7.14179575e-02],\n",
       "        [ 5.37984632e-02, -9.47228298e-02,  1.46085387e-02],\n",
       "        [ 2.83869449e-02, -2.04946939e-02, -2.08926369e-02],\n",
       "        [ 1.61074381e-02, -5.45662306e-02,  4.27457765e-02],\n",
       "        [-1.38957100e-02, -8.53190646e-02,  5.15212901e-02],\n",
       "        [-9.31798364e-04, -1.91762708e-02, -5.85137680e-02],\n",
       "        [-8.85037053e-03, -9.32271704e-02, -4.95502315e-02],\n",
       "        [ 7.22364485e-02, -1.15783112e-02, -4.59032506e-02],\n",
       "        [-1.81416329e-02,  4.72645946e-02, -4.48701084e-02],\n",
       "        [ 9.27953720e-02, -8.16782489e-02,  4.93522137e-02],\n",
       "        [-7.80265704e-02,  3.03213801e-02,  3.05654407e-02],\n",
       "        [ 9.56569910e-02,  2.31329650e-02,  6.22237921e-02],\n",
       "        [-9.10717715e-03, -2.60398611e-02,  6.70131370e-02],\n",
       "        [ 6.42554834e-02,  5.37227914e-02,  5.40225916e-02],\n",
       "        [-5.10511780e-03,  3.98476794e-02,  9.09155384e-02],\n",
       "        [ 1.46789178e-02, -2.94216629e-02,  2.89083887e-02],\n",
       "        [ 1.62045024e-02,  5.31434976e-02, -6.01440184e-02],\n",
       "        [-1.54455965e-02, -5.96035831e-02, -3.41892764e-02],\n",
       "        [-5.34296874e-03, -5.87974582e-03, -2.47759297e-02],\n",
       "        [ 8.45443457e-02,  9.85627323e-02, -4.65272851e-02],\n",
       "        [ 4.34590541e-02, -1.93869025e-02, -6.39239624e-02],\n",
       "        [-7.20127597e-02, -3.16540711e-02, -6.70707002e-02],\n",
       "        [-3.42993066e-02,  1.61496355e-04,  9.68949422e-02],\n",
       "        [-5.29724248e-02,  7.88580477e-02, -1.46851758e-03],\n",
       "        [-8.65789354e-02, -3.76089811e-02, -9.80823338e-02],\n",
       "        [ 1.84324663e-02,  5.64649440e-02, -5.19445725e-02],\n",
       "        [ 6.88024089e-02,  2.86389813e-02,  7.72195011e-02],\n",
       "        [ 8.53215083e-02,  9.17680562e-02,  6.60324544e-02],\n",
       "        [ 4.31769677e-02,  9.22377408e-02,  8.28169584e-02],\n",
       "        [ 3.18827704e-02,  4.47944663e-02,  7.61642233e-02],\n",
       "        [ 7.15416819e-02, -8.45175609e-02, -8.09759423e-02],\n",
       "        [-9.00712311e-02, -7.38157332e-02,  7.60056917e-03],\n",
       "        [ 7.35996990e-03, -5.03439866e-02, -2.48013213e-02],\n",
       "        [ 1.24280080e-02,  4.78672087e-02,  3.09841391e-02],\n",
       "        [-6.61449805e-02,  1.26330154e-02,  5.73227070e-02],\n",
       "        [ 7.01072216e-02, -6.61113160e-03,  6.05770089e-02],\n",
       "        [-5.25830872e-02, -8.39191452e-02, -3.29554006e-02],\n",
       "        [-9.57222208e-02, -4.12133969e-02, -2.09250245e-02],\n",
       "        [ 8.77739936e-02, -6.14245906e-02, -5.07884063e-02],\n",
       "        [-9.24634412e-02, -1.48592340e-02,  6.30847644e-03],\n",
       "        [-1.82057172e-02, -7.59163871e-02,  1.01704914e-02],\n",
       "        [-5.36751375e-02, -7.10608885e-02,  2.98198964e-03],\n",
       "        [ 3.14396508e-02,  8.21219683e-02, -6.02085739e-02],\n",
       "        [-7.51630291e-02,  9.86944884e-02,  6.53779740e-03],\n",
       "        [-8.25501978e-02, -4.43944000e-02, -8.12607759e-04],\n",
       "        [ 8.32439587e-02, -7.41322562e-02, -6.85036555e-02],\n",
       "        [-7.83599988e-02, -8.60530436e-02, -8.61732885e-02],\n",
       "        [ 4.05583046e-02, -6.35929182e-02,  7.34520629e-02],\n",
       "        [-8.96247476e-02, -8.24547559e-02,  8.74789879e-02],\n",
       "        [ 9.97291878e-02,  5.77432960e-02,  8.93690661e-02],\n",
       "        [ 2.84518767e-02, -7.77026191e-02, -8.88288170e-02],\n",
       "        [ 3.50588039e-02, -1.40797095e-02, -4.21645381e-02],\n",
       "        [ 2.68225484e-02, -9.13444608e-02,  8.62977654e-02],\n",
       "        [-2.44871210e-02,  9.49440598e-02, -8.99025574e-02],\n",
       "        [ 2.69150976e-02, -7.63838813e-02, -3.85117009e-02],\n",
       "        [-2.00430416e-02,  7.11081475e-02, -6.70667589e-02],\n",
       "        [ 9.86413136e-02,  7.96757266e-02, -4.28716131e-02],\n",
       "        [ 6.58734366e-02,  4.22145724e-02, -7.16942623e-02],\n",
       "        [-2.65544257e-03,  6.54949397e-02, -8.79150704e-02],\n",
       "        [ 6.76101521e-02, -1.73710454e-02,  3.58883254e-02],\n",
       "        [ 7.88425561e-03,  2.66317874e-02,  7.45117962e-02],\n",
       "        [-1.36107265e-03, -8.74098018e-02, -8.67979825e-02],\n",
       "        [ 8.44821110e-02, -4.01768945e-02,  7.49536604e-02],\n",
       "        [ 5.30618131e-02, -9.09461156e-02, -2.62742452e-02],\n",
       "        [-2.62872912e-02, -9.99097005e-02, -8.27424526e-02],\n",
       "        [-9.78374556e-02, -5.59550077e-02, -7.98778161e-02],\n",
       "        [ 4.89222519e-02, -3.86501220e-03, -6.83566928e-02],\n",
       "        [ 9.18832868e-02,  6.62421286e-02, -8.81815180e-02],\n",
       "        [-4.62247543e-02, -1.31007694e-02,  2.95272935e-02],\n",
       "        [ 2.19657663e-02, -7.67279118e-02,  4.25595716e-02],\n",
       "        [-4.78262939e-02,  4.31692898e-02, -7.25668743e-02],\n",
       "        [-1.28423814e-02, -7.04637021e-02, -3.78000103e-02],\n",
       "        [ 2.78634788e-03,  3.09194364e-02, -3.52030098e-02],\n",
       "        [ 4.79077883e-02, -5.22873662e-02,  5.83256185e-02],\n",
       "        [-2.08557919e-02,  8.67267400e-02,  2.76678912e-02],\n",
       "        [ 6.07588678e-04, -6.92554116e-02, -8.31757206e-03],\n",
       "        [-7.01190755e-02,  1.46846869e-04, -3.05699799e-02],\n",
       "        [ 1.36063574e-02, -2.53809639e-03, -5.83304688e-02],\n",
       "        [-3.21381865e-03, -7.08871186e-02, -1.90365333e-02],\n",
       "        [ 4.77090701e-02,  2.11503915e-02,  2.33571306e-02],\n",
       "        [-3.11912652e-02, -6.84125572e-02,  2.98621263e-02],\n",
       "        [-5.24199232e-02, -1.54952854e-02, -5.84754162e-02],\n",
       "        [-3.39693986e-02, -7.04045445e-02,  9.22912061e-02],\n",
       "        [ 6.25380129e-02,  2.33272277e-02, -5.22467494e-02],\n",
       "        [ 9.01562795e-02,  2.30439603e-02, -2.65314560e-02],\n",
       "        [-5.19088767e-02,  5.12909107e-02, -3.12688127e-02],\n",
       "        [ 8.00619051e-02, -5.34027629e-02, -3.49442735e-02],\n",
       "        [ 7.30637163e-02,  5.80355823e-02, -2.19823662e-02],\n",
       "        [-1.94801984e-03, -1.91200543e-02,  2.21136380e-02],\n",
       "        [ 6.10836297e-02,  2.91578136e-02,  8.34174827e-02],\n",
       "        [-8.48553404e-02,  3.03827729e-02,  1.28409583e-02],\n",
       "        [-5.51414862e-02,  2.84188818e-02, -4.88333553e-02],\n",
       "        [ 6.66561276e-02, -8.10018033e-02, -9.60622281e-02],\n",
       "        [ 5.87540455e-02, -9.12585035e-02,  3.22558098e-02],\n",
       "        [-5.56938164e-02, -4.23238464e-02, -4.19776142e-02],\n",
       "        [-7.65140876e-02,  3.02146785e-02, -6.96224570e-02],\n",
       "        [ 2.23567430e-02, -1.65510681e-02, -2.59116776e-02],\n",
       "        [ 6.43545091e-02,  1.29420380e-03,  2.14570835e-02],\n",
       "        [-9.94837508e-02, -9.05190855e-02,  6.16564369e-03],\n",
       "        [ 5.59479892e-02, -9.53289121e-02,  6.73358217e-02],\n",
       "        [ 6.41523153e-02,  2.33732294e-02, -9.72717851e-02],\n",
       "        [-8.11878070e-02, -7.93600753e-02, -3.41210514e-02],\n",
       "        [ 4.52972163e-04,  9.06635821e-02, -4.94683906e-03],\n",
       "        [-9.73633975e-02,  4.06589620e-02, -3.30895185e-02],\n",
       "        [-4.24220189e-02,  8.97283256e-02,  5.12028709e-02],\n",
       "        [ 2.10484322e-02, -4.15117890e-02, -6.45188540e-02],\n",
       "        [-4.31376547e-02, -8.27845186e-02,  9.45775807e-02],\n",
       "        [ 9.69663486e-02,  6.25912547e-02,  6.51953891e-02],\n",
       "        [-4.48062755e-02, -9.36184824e-02, -2.28073169e-02],\n",
       "        [-4.93704863e-02,  5.80324195e-02,  8.41296837e-02],\n",
       "        [ 5.37092239e-02, -1.04407920e-02,  8.44131261e-02],\n",
       "        [-5.54825626e-02, -5.68822883e-02, -9.07045528e-02],\n",
       "        [ 4.69145067e-02,  9.49693918e-02, -4.79381308e-02],\n",
       "        [-9.50057060e-02, -2.21554898e-02, -1.44297890e-02],\n",
       "        [-4.08831937e-03, -5.44504113e-02, -7.00387359e-03],\n",
       "        [-8.02114755e-02,  4.49274592e-02, -9.37496796e-02],\n",
       "        [ 3.59098539e-02, -6.31869733e-02, -5.60478158e-02],\n",
       "        [-7.18741119e-02,  4.84359488e-02,  4.35985737e-02],\n",
       "        [ 7.88820013e-02,  2.37679072e-02,  3.64003703e-02],\n",
       "        [ 3.23179290e-02,  6.83497265e-02,  3.98739204e-02],\n",
       "        [-9.77114066e-02,  3.66623066e-02,  2.29011662e-03],\n",
       "        [ 6.21445384e-03, -6.52632788e-02, -2.57027149e-02],\n",
       "        [-3.61579657e-03, -3.72683108e-02,  7.25040659e-02],\n",
       "        [-4.45152121e-03, -7.04507083e-02,  3.38391811e-02],\n",
       "        [ 5.82087003e-02,  4.91453037e-02, -6.57577142e-02],\n",
       "        [-4.56171436e-03,  7.26038292e-02,  8.60223621e-02],\n",
       "        [ 2.84645371e-02, -8.55400860e-02, -6.65475205e-02],\n",
       "        [-8.05475488e-02,  6.90538585e-02, -5.81053197e-02],\n",
       "        [ 7.01553002e-02,  3.79306562e-02,  7.80127198e-02],\n",
       "        [-3.57504748e-02, -5.84296836e-03,  7.24705728e-03],\n",
       "        [ 5.95550798e-02, -5.28683066e-02, -2.79794168e-02],\n",
       "        [-7.25187212e-02,  7.47622699e-02, -2.31213570e-02],\n",
       "        [-8.15643668e-02, -9.22232792e-02,  7.39383399e-02],\n",
       "        [ 7.64816776e-02, -2.47404375e-03, -6.73717335e-02],\n",
       "        [ 1.78395733e-02,  6.25576228e-02, -9.35900509e-02],\n",
       "        [-9.56180021e-02,  6.49213865e-02,  7.56144673e-02],\n",
       "        [ 8.96311626e-02, -7.89199322e-02,  1.42754661e-02],\n",
       "        [-4.73660678e-02, -8.31899792e-02, -7.30372220e-03],\n",
       "        [-7.47382045e-02,  9.17681828e-02,  8.24770927e-02],\n",
       "        [-5.58790844e-03, -6.82766885e-02, -7.24251494e-02],\n",
       "        [-1.98135581e-02,  4.46772426e-02, -7.97074810e-02],\n",
       "        [ 2.18460206e-02,  5.75099587e-02, -6.25110790e-02],\n",
       "        [ 2.95051560e-02,  9.97900069e-02,  1.20789502e-02],\n",
       "        [-1.60441583e-03,  3.33223902e-02, -5.66113964e-02],\n",
       "        [ 8.24301913e-02, -4.51689214e-02,  4.75982353e-02],\n",
       "        [-3.78868096e-02, -2.73189321e-02,  2.34615579e-02],\n",
       "        [ 8.15326422e-02,  2.09506061e-02,  6.65546507e-02],\n",
       "        [-7.74852857e-02, -4.30321246e-02,  7.17577636e-02],\n",
       "        [ 7.27943098e-03,  7.90842623e-02,  1.92134138e-02],\n",
       "        [ 6.03040792e-02,  8.23174138e-03,  6.01875409e-02],\n",
       "        [-1.62876695e-02,  9.86228809e-02, -5.27914874e-02],\n",
       "        [-2.92032566e-02,  6.33860826e-02,  7.06076249e-02],\n",
       "        [ 7.76672661e-02, -7.54658207e-02,  5.52913286e-02],\n",
       "        [-8.79756734e-02, -1.02478275e-02, -5.18852919e-02],\n",
       "        [ 9.35356244e-02, -3.22960317e-02, -4.50956374e-02],\n",
       "        [-2.34685186e-02, -4.50538769e-02, -2.99740434e-02],\n",
       "        [-4.91272584e-02,  1.70481801e-02,  8.12431872e-02]], dtype=float32)}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = network_weights(vocab_size=2000,embedding_dim=300,hidden_dim=[], num_classes=3)\n",
    "W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:31:57.970152Z",
     "start_time": "2020-04-01T10:31:57.966123Z"
    }
   },
   "source": [
    "Then you need to develop a `softmax` function (same as in Assignment 1) to be used in the output layer. \n",
    "\n",
    "It takes as input `z` (array of real numbers) and returns `sig` (the softmax of `z`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Softmax fn takes a numpy array z which represents the output of the last layer of a neural network, and returns \n",
    "   the softmax of z. Softmax is a commonly used activation function in the output layer of multi-class classification.\n",
    "   sig returns the probabilities of each class'''\n",
    "def softmax(z):\n",
    "    Numerator = np.exp(z)\n",
    "    Denominator = np.sum(np.exp(z))\n",
    "    sig = Numerator / Denominator   \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig= softmax([5,6,7])\n",
    "sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the categorical cross entropy loss by slightly modifying the function from Assignment 1 to depend only on the true label `y` and the class probabilities vector `y_preds`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:51.360838Z",
     "start_time": "2020-04-02T14:26:51.356935Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''This function computes the cross-entropy loss for a single sample by comparing the predicted probabilities (y_preds) \n",
    "   with the true label (y).'''\n",
    "def categorical_loss(y, y_preds):\n",
    "    l=-np.log(y_preds[y])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5108256237659907"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=categorical_loss(1,[0.3,0.6,0.1])\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:02:56.149535Z",
     "start_time": "2020-03-31T15:02:56.145738Z"
    }
   },
   "source": [
    "Then, implement the `relu` function to introduce non-linearity after each hidden layer of your network \n",
    "(during the forward pass): \n",
    "\n",
    "$$relu(z_i)= max(z_i,0)$$\n",
    "\n",
    "and the `relu_derivative` function to compute its derivative (used in the backward pass):\n",
    "\n",
    "  \n",
    "  relu_derivative($z_i$)=0, if $z_i$<=0, 1 otherwise.\n",
    "  \n",
    "\n",
    "\n",
    "Note that both functions take as input a vector $z$ \n",
    "\n",
    "Hint use .copy() to avoid in place changes in array z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:52.665236Z",
     "start_time": "2020-04-02T14:26:52.661519Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' This function takes numpy array z as input and applies relu function to it element-wise'''\n",
    "def relu(z):\n",
    "    z = z.copy()\n",
    "    a = np.maximum(0, z)    \n",
    "    return a\n",
    "    ''' This function computes the derivative of relu activation fn given an i/p 'z'. Returns an array of same shape as z \n",
    "    where the elements are 0 if the corresponding element in z is negative or 0 and 1 if it is positive.'''\n",
    "def relu_derivative(z):\n",
    "    dz=np.array(z)\n",
    "    dz = dz.copy()\n",
    "    dz[dz <= 0] = 0\n",
    "    dz[dz > 0] = 1  \n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz= relu_derivative([1,-2,0])\n",
    "dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training you should also apply a dropout mask element-wise after the activation function (i.e. vector of ones with a random percentage set to zero). The `dropout_mask` function takes as input:\n",
    "\n",
    "- `size`: the size of the vector that we want to apply dropout\n",
    "- `dropout_rate`: the percentage of elements that will be randomly set to zeros\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `dropout_vec`: a vector with binary values (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:53.429192Z",
     "start_time": "2020-04-02T14:26:53.425301Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' Returns a vector of the same size as the input size with values either 1 or 0. The probability of a value \n",
    "    being 0 is determined by the dropout_rate input.'''\n",
    "def dropout_mask(size, dropout_rate): \n",
    "    '''Initialized the dropout_vec as a vector of ones with the same size as 'size' '''    \n",
    "    dropout_vec = np.ones(size)\n",
    "    dropout_vec[:round(size*dropout_rate)] = 0.0\n",
    "    ''' Randomly shuffles the order of the elements in the mask vector and returns the resulting vector.'''    \n",
    "    np.random.shuffle(dropout_vec)\n",
    "    return dropout_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:53.853632Z",
     "start_time": "2020-04-02T14:26:53.849944Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
      "[0. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(dropout_mask(10, 0.2))\n",
    "print(dropout_mask(10, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the `forward_pass` function that passes the input x through the network up to the output layer for computing the probability for each class using the weight matrices in `W`. The ReLU activation function should be applied on each hidden layer. \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `dropout_rate`: the dropout rate that is used to generate a random dropout mask vector applied after each hidden layer for regularisation.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `out_vals`: a dictionary of output values from each layer: h (the vector before the activation function), a (the resulting vector after passing h from the activation function), its dropout mask vector; and the prediction vector (probability for each class) from the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:54.761268Z",
     "start_time": "2020-04-02T14:26:54.753402Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, W, dropout_rate=0.2):\n",
    "    '''Initializing empty lists to store the outputs for each layer'''\n",
    "    out_vals = {}\n",
    "    h_vecs = []\n",
    "    a_vecs = []\n",
    "    dropout_vecs = []\n",
    "    \n",
    "    # Input layer\n",
    "    ''' Computes the output by taking the mean of the corresponding rows of W[0](i.e. for each word in input x)'''\n",
    "    h = np.mean(W[0][x,:],axis=0)\n",
    "    h_vecs.append(h)\n",
    "    ''' Apply ReLU activation to the input layer'''\n",
    "    a = relu(h)\n",
    "    a_vecs.append(a)\n",
    "    ''' Create a dropout mask vector''' \n",
    "    dropout = dropout_mask(a.shape[0],dropout_rate)\n",
    "    dropout_vecs.append(dropout)\n",
    "    ''' Applyting dropout to the input layer'''\n",
    "    a= a*dropout\n",
    "    \n",
    "    layer=0\n",
    "    # Hidden layers\n",
    "    for i in range(1,len(W)-1):\n",
    "        layer=layer+1\n",
    "        ''' Computes the dot product between a of previous layer and the weight matrix of current layer'''\n",
    "        h = np.dot(a, W[i])\n",
    "        h_vecs.append(h)\n",
    "        ''' Applying ReLU activation'''\n",
    "        a = relu(h)  \n",
    "        a_vecs.append(a)\n",
    "        ''' Create a dropout mask vector for current layer'''\n",
    "        dropout = dropout_mask(a.shape[0],dropout_rate)\n",
    "        ''' Applying dropout'''\n",
    "        a = a * dropout\n",
    "        dropout_vecs.append(dropout)\n",
    "    \n",
    "    # Output layer\n",
    "    ''' Computes the dot product between a of previous layer and the weight matrix of output layer'''\n",
    "    h = np.dot(a, W[layer+1])\n",
    "    h_vecs.append(h)\n",
    "    ''' Apply softmax function to the output layer to obtain predicted probabilities'''\n",
    "    probs = softmax(h)\n",
    "    out_vals['h'] = h_vecs\n",
    "    out_vals['a'] = a_vecs\n",
    "    out_vals['dropout'] = dropout_vecs\n",
    "    out_vals['pred'] = probs\n",
    "    \n",
    "    ''' This will return a dictionary of output values for each layer'''\n",
    "    return out_vals \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': [array([-0.0223637 , -0.04894667, -0.00422523,  0.04108326,  0.04111568,\n",
       "         -0.08519527, -0.01621249, -0.01957027, -0.00877306, -0.02863121,\n",
       "         -0.03802076, -0.03569879,  0.00385366, -0.02166606,  0.01365391,\n",
       "         -0.01951261,  0.01780181, -0.04224939,  0.03091075,  0.03924821,\n",
       "         -0.06660871,  0.0611973 ,  0.00661314, -0.00837829,  0.03067666,\n",
       "          0.00637556,  0.05237712,  0.04689235, -0.029748  , -0.01662533,\n",
       "          0.04356892, -0.05999134,  0.0649419 , -0.04722518,  0.00531162,\n",
       "         -0.04572587, -0.0245885 ,  0.01480933, -0.01493692, -0.02855929,\n",
       "          0.00625965, -0.04368814,  0.05423101, -0.06132715, -0.00591042,\n",
       "          0.06726888,  0.01382804,  0.0500506 , -0.03482917, -0.0386178 ,\n",
       "          0.09320569,  0.0244156 ,  0.001651  , -0.02699426, -0.06154031,\n",
       "         -0.02276647, -0.08975885, -0.05054047,  0.01461044,  0.00742705,\n",
       "         -0.00538513, -0.05102628,  0.03336635, -0.02433196,  0.0137298 ,\n",
       "          0.01951248,  0.00478553, -0.0320155 ,  0.01637666, -0.00457188,\n",
       "         -0.01287899,  0.0018141 , -0.02831641, -0.04574124,  0.03310939,\n",
       "          0.00343392,  0.01361406,  0.01205721,  0.00172198, -0.08939984,\n",
       "          0.02868801, -0.03294651,  0.06048858, -0.02894333,  0.00172826,\n",
       "          0.00229301,  0.00283575, -0.01482811, -0.0221698 , -0.00702386,\n",
       "          0.01983427,  0.03392756, -0.03845998,  0.03951858,  0.05847461,\n",
       "         -0.05722643, -0.05394221, -0.05326515,  0.00918243, -0.02901277,\n",
       "         -0.03079971,  0.07854132,  0.00067015,  0.01984537, -0.00298584,\n",
       "         -0.00970514,  0.06306285,  0.07861635, -0.02406014, -0.04915043,\n",
       "          0.00869478,  0.08690049,  0.01606263,  0.03097238,  0.0625217 ,\n",
       "         -0.00776177, -0.02321691,  0.04404841,  0.02486318, -0.05969398,\n",
       "         -0.0493994 , -0.00700049, -0.05463456,  0.0453976 ,  0.00607297,\n",
       "         -0.02644579,  0.04929168, -0.04153956, -0.00021684, -0.01014874,\n",
       "          0.03262972, -0.03403185, -0.06607202,  0.05281495, -0.0335038 ,\n",
       "          0.01619848,  0.04111638, -0.04355956,  0.07827535,  0.05447714,\n",
       "         -0.00457082,  0.02275857,  0.02991019,  0.03839208, -0.02519323,\n",
       "          0.07680641,  0.06924842,  0.07217732,  0.07193177, -0.04607941,\n",
       "          0.00056345, -0.02440589,  0.04043984,  0.01346286, -0.04334769,\n",
       "         -0.04036719,  0.00890901,  0.06347458,  0.0025134 , -0.0439952 ,\n",
       "         -0.03842164,  0.0523678 ,  0.02258847,  0.07939167,  0.03246429,\n",
       "          0.08267636,  0.00961101,  0.01665723, -0.07520331, -0.05998871,\n",
       "          0.02726911, -0.00595153,  0.0029283 ,  0.00350065, -0.01557809,\n",
       "          0.09324114,  0.02313993, -0.0164237 , -0.02328612,  0.01800758,\n",
       "         -0.02731312,  0.02088873,  0.06963561,  0.03461649, -0.02679426,\n",
       "         -0.00227623,  0.01816938, -0.00051671,  0.00575182, -0.06162169,\n",
       "         -0.01753953, -0.01897074,  0.02959699, -0.00954203,  0.00018839,\n",
       "         -0.02488082, -0.05303522,  0.04432091, -0.01553436, -0.07689802,\n",
       "          0.07121698,  0.05609271, -0.00586511,  0.08251788,  0.01837941,\n",
       "         -0.05350538, -0.05811491, -0.00793229,  0.03947975, -0.01706797,\n",
       "         -0.02770054,  0.02665163,  0.05559738, -0.0295232 , -0.01337189,\n",
       "         -0.01517363, -0.04761156,  0.03815208, -0.09322377, -0.0656939 ,\n",
       "         -0.01018037,  0.00073307, -0.04117993, -0.01344661, -0.02626381,\n",
       "          0.075769  ,  0.06393493, -0.00777471, -0.01500084, -0.0802152 ,\n",
       "         -0.00025629, -0.01294466,  0.03271434, -0.00161255,  0.03405747,\n",
       "         -0.05483479, -0.07385866, -0.0192213 ,  0.00640537,  0.00320075,\n",
       "         -0.02201433, -0.00522853, -0.0581711 , -0.0225635 ,  0.06231023,\n",
       "          0.03557699, -0.00900497, -0.07703617,  0.06065511, -0.04799479,\n",
       "         -0.01776665,  0.02748633, -0.03058667, -0.04538775, -0.04383407,\n",
       "          0.04729652,  0.01661737,  0.02133071, -0.08005922, -0.03786622,\n",
       "          0.03050955,  0.00637628, -0.0127834 ,  0.03287164,  0.00419935,\n",
       "          0.00527622,  0.04343242,  0.01146472, -0.03332254, -0.02689277,\n",
       "         -0.04207199, -0.07897355,  0.02117447,  0.01838462,  0.00562044,\n",
       "          0.01825135, -0.04838157,  0.03831635, -0.03249719,  0.01459184,\n",
       "         -0.00442958, -0.02152653, -0.05334602,  0.01767219,  0.09456877,\n",
       "         -0.02146791,  0.02081174,  0.0516549 , -0.0224294 , -0.0163219 ,\n",
       "         -0.0258477 ,  0.03226505, -0.06142793, -0.01931428, -0.0097762 ,\n",
       "          0.01035988, -0.05631524, -0.04428693, -0.06301408, -0.01709035],\n",
       "        dtype=float32),\n",
       "  array([0.00097239, 0.03607308, 0.00670176])],\n",
       " 'a': [array([0.        , 0.        , 0.        , 0.04108326, 0.04111568,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.00385366, 0.        , 0.01365391,\n",
       "         0.        , 0.01780181, 0.        , 0.03091075, 0.03924821,\n",
       "         0.        , 0.0611973 , 0.00661314, 0.        , 0.03067666,\n",
       "         0.00637556, 0.05237712, 0.04689235, 0.        , 0.        ,\n",
       "         0.04356892, 0.        , 0.0649419 , 0.        , 0.00531162,\n",
       "         0.        , 0.        , 0.01480933, 0.        , 0.        ,\n",
       "         0.00625965, 0.        , 0.05423101, 0.        , 0.        ,\n",
       "         0.06726888, 0.01382804, 0.0500506 , 0.        , 0.        ,\n",
       "         0.09320569, 0.0244156 , 0.001651  , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.01461044, 0.00742705,\n",
       "         0.        , 0.        , 0.03336635, 0.        , 0.0137298 ,\n",
       "         0.01951248, 0.00478553, 0.        , 0.01637666, 0.        ,\n",
       "         0.        , 0.0018141 , 0.        , 0.        , 0.03310939,\n",
       "         0.00343392, 0.01361406, 0.01205721, 0.00172198, 0.        ,\n",
       "         0.02868801, 0.        , 0.06048858, 0.        , 0.00172826,\n",
       "         0.00229301, 0.00283575, 0.        , 0.        , 0.        ,\n",
       "         0.01983427, 0.03392756, 0.        , 0.03951858, 0.05847461,\n",
       "         0.        , 0.        , 0.        , 0.00918243, 0.        ,\n",
       "         0.        , 0.07854132, 0.00067015, 0.01984537, 0.        ,\n",
       "         0.        , 0.06306285, 0.07861635, 0.        , 0.        ,\n",
       "         0.00869478, 0.08690049, 0.01606263, 0.03097238, 0.0625217 ,\n",
       "         0.        , 0.        , 0.04404841, 0.02486318, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.0453976 , 0.00607297,\n",
       "         0.        , 0.04929168, 0.        , 0.        , 0.        ,\n",
       "         0.03262972, 0.        , 0.        , 0.05281495, 0.        ,\n",
       "         0.01619848, 0.04111638, 0.        , 0.07827535, 0.05447714,\n",
       "         0.        , 0.02275857, 0.02991019, 0.03839208, 0.        ,\n",
       "         0.07680641, 0.06924842, 0.07217732, 0.07193177, 0.        ,\n",
       "         0.00056345, 0.        , 0.04043984, 0.01346286, 0.        ,\n",
       "         0.        , 0.00890901, 0.06347458, 0.0025134 , 0.        ,\n",
       "         0.        , 0.0523678 , 0.02258847, 0.07939167, 0.03246429,\n",
       "         0.08267636, 0.00961101, 0.01665723, 0.        , 0.        ,\n",
       "         0.02726911, 0.        , 0.0029283 , 0.00350065, 0.        ,\n",
       "         0.09324114, 0.02313993, 0.        , 0.        , 0.01800758,\n",
       "         0.        , 0.02088873, 0.06963561, 0.03461649, 0.        ,\n",
       "         0.        , 0.01816938, 0.        , 0.00575182, 0.        ,\n",
       "         0.        , 0.        , 0.02959699, 0.        , 0.00018839,\n",
       "         0.        , 0.        , 0.04432091, 0.        , 0.        ,\n",
       "         0.07121698, 0.05609271, 0.        , 0.08251788, 0.01837941,\n",
       "         0.        , 0.        , 0.        , 0.03947975, 0.        ,\n",
       "         0.        , 0.02665163, 0.05559738, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.03815208, 0.        , 0.        ,\n",
       "         0.        , 0.00073307, 0.        , 0.        , 0.        ,\n",
       "         0.075769  , 0.06393493, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.03271434, 0.        , 0.03405747,\n",
       "         0.        , 0.        , 0.        , 0.00640537, 0.00320075,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.06231023,\n",
       "         0.03557699, 0.        , 0.        , 0.06065511, 0.        ,\n",
       "         0.        , 0.02748633, 0.        , 0.        , 0.        ,\n",
       "         0.04729652, 0.01661737, 0.02133071, 0.        , 0.        ,\n",
       "         0.03050955, 0.00637628, 0.        , 0.03287164, 0.00419935,\n",
       "         0.00527622, 0.04343242, 0.01146472, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.02117447, 0.01838462, 0.00562044,\n",
       "         0.01825135, 0.        , 0.03831635, 0.        , 0.01459184,\n",
       "         0.        , 0.        , 0.        , 0.01767219, 0.09456877,\n",
       "         0.        , 0.02081174, 0.0516549 , 0.        , 0.        ,\n",
       "         0.        , 0.03226505, 0.        , 0.        , 0.        ,\n",
       "         0.01035988, 0.        , 0.        , 0.        , 0.        ],\n",
       "        dtype=float32)],\n",
       " 'dropout': [array([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.])],\n",
       " 'pred': array([0.3287884 , 0.34053404, 0.33067756])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_vals=forward_pass([0,0,0,1,1],W , dropout_rate=0.2)\n",
    "out_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward_pass` function computes the gradients and updates the weights for each matrix in the network from the output to the input. It takes as input \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `y`: the true label\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `out_vals`: a dictionary of output values from a forward pass.\n",
    "- `learning_rate`: the learning rate for updating the weights.\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: the updated weights of the network.\n",
    "\n",
    "Hint: the gradients on the output layer are similar to the multiclass logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This function updates the weights of a neural network to minimize the loss function'''\n",
    "def backward_pass(x, y, W, out_vals, lr=0.01, freeze_emb=False):\n",
    "    \n",
    "    # Initialize gradients\n",
    "    '''Initializes a list of numpy arrays with the same shape as the weight matrices W. \n",
    "       The list grad will be used to store the gradients of the loss function w.r.t the weights during backpropagation'''\n",
    "    grad = [np.zeros_like(w) for w in W]\n",
    "    \n",
    "    ''' Compute error at output layer by subtracting 1 from the predicted prob of the correct class'''\n",
    "    output_error = out_vals['pred']\n",
    "    output_error[y] = output_error[y] - 1\n",
    "    \n",
    "    # Backpropagating error through output layer of a neural network\n",
    "    ''' The grad variable is a list that stores the gradients of the weights for each layer in the network. Here grad[-1]\n",
    "    corresponds to the output layer'''\n",
    "    grad[-1] = np.outer(out_vals['a'][-1], output_error)\n",
    "    delta = output_error\n",
    "    # Backpropagating error through hidden layers of a neural network\n",
    "    for i in range(len(W) - 2, 0, -1):\n",
    "        dropout = out_vals['dropout'][i]\n",
    "        delta = np.dot(W[i+1], delta) * relu_derivative(out_vals['h'][i])\n",
    "        delta = delta * dropout\n",
    "        # Compute gradients for current layer\n",
    "        grad[i] = np.outer(out_vals['a'][i], delta)\n",
    "    \n",
    "    # Backpropagating error through input layer of a neural network\n",
    "    delta = np.dot(W[1], delta) * relu_derivative(out_vals['h'][0])\n",
    "    grad[0] = np.zeros_like(W[0])\n",
    "    for i in x:\n",
    "        grad[0][i,:] = grad[0][i,:] + delta\n",
    "    ''' If true, the gradients for the embedding layer are set to zero. '''\n",
    "    if freeze_emb== True:\n",
    "        grad[0] = np.zeros_like(W[0])\n",
    "    \n",
    "    '''Updates weights using the computed gradients'''\n",
    "    for i in range(len(W)):\n",
    "        W[i] = W[i] - (lr * grad[i])\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([[-0.02637493, -0.09433637, -0.06779089, ..., -0.02193159,\n",
       "         -0.09183982, -0.08483312],\n",
       "        [-0.01634685,  0.01913789,  0.09112325, ..., -0.07781994,\n",
       "         -0.01977547,  0.08452381],\n",
       "        [ 0.04183768,  0.0679256 ,  0.07946683, ..., -0.05193486,\n",
       "          0.01428006, -0.03478409],\n",
       "        ...,\n",
       "        [-0.00028373,  0.03143086,  0.0131515 , ...,  0.01424241,\n",
       "          0.06348188,  0.00103876],\n",
       "        [-0.08830725,  0.01619583, -0.08653948, ..., -0.00863065,\n",
       "          0.06186633,  0.09729445],\n",
       "        [ 0.01061703, -0.02167895,  0.09023168, ...,  0.04236311,\n",
       "          0.01756194, -0.06356017]], dtype=float32),\n",
       " 1: array([[ 1.34228431e-02,  9.28744152e-02, -3.62130105e-02],\n",
       "        [-2.29819063e-02, -6.07093684e-02, -1.72710381e-02],\n",
       "        [-5.30987605e-02,  7.23892003e-02, -5.91134429e-02],\n",
       "        [ 9.31716116e-02,  9.33160287e-02, -4.41943138e-02],\n",
       "        [-5.05240925e-02, -4.51119537e-03, -7.49448004e-02],\n",
       "        [-8.45093355e-02,  4.45363373e-02,  2.63765696e-02],\n",
       "        [-6.33421019e-02, -6.46630749e-02,  9.11137164e-02],\n",
       "        [ 6.89098388e-02, -1.15626054e-02,  9.39684212e-02],\n",
       "        [-9.17792320e-02,  6.07857220e-02, -1.15125831e-02],\n",
       "        [ 5.51716313e-02, -9.06812549e-02,  9.69284028e-02],\n",
       "        [-5.95776848e-02,  6.10478856e-02,  7.38038346e-02],\n",
       "        [ 3.56249548e-02,  2.40198374e-02, -4.93938960e-02],\n",
       "        [ 3.15214838e-02,  1.75821006e-02,  7.13613403e-02],\n",
       "        [-7.93489739e-02, -5.76978885e-02,  5.62206358e-02],\n",
       "        [-6.69519196e-02,  8.56671224e-02, -5.30692153e-02],\n",
       "        [-7.46655986e-02,  6.62082657e-02,  6.40015379e-02],\n",
       "        [-9.03635217e-02,  5.44374486e-02,  1.93635125e-02],\n",
       "        [ 5.67222759e-02, -6.49368167e-02,  8.32749978e-02],\n",
       "        [-2.93769450e-02,  7.48689992e-02,  4.73388888e-02],\n",
       "        [ 4.83728424e-02,  9.97231669e-03, -5.02263341e-02],\n",
       "        [-5.57235293e-02, -4.72281948e-02, -1.62997041e-02],\n",
       "        [-8.71434107e-02,  8.72906722e-02,  8.52635237e-03],\n",
       "        [ 1.10357463e-02, -4.18934203e-03,  5.87770901e-02],\n",
       "        [-4.04871553e-02,  6.33595288e-02, -8.11736956e-02],\n",
       "        [ 1.41848289e-02,  4.58895359e-02, -6.78059274e-02],\n",
       "        [-3.81418624e-03,  2.12065117e-02,  8.33620134e-02],\n",
       "        [-8.86429724e-02,  8.58375505e-02,  8.71955404e-03],\n",
       "        [-9.47255591e-02,  2.65895690e-02,  1.30960944e-02],\n",
       "        [-8.80337283e-02,  3.42733301e-02,  4.99349870e-02],\n",
       "        [ 1.01631852e-02,  8.86174478e-03,  5.77235520e-02],\n",
       "        [-1.43074452e-02, -3.45243262e-02,  4.18617374e-02],\n",
       "        [-9.96882021e-02,  1.33409323e-02, -1.89518705e-02],\n",
       "        [-7.17417261e-03,  7.44641359e-02,  4.36613723e-02],\n",
       "        [-7.11252466e-02, -3.08646797e-03,  1.57353897e-02],\n",
       "        [ 6.62075773e-02,  6.28048118e-02, -1.85593105e-02],\n",
       "        [-2.47266050e-02, -1.54257268e-02, -4.93898168e-02],\n",
       "        [-7.62545243e-02, -1.84150357e-02,  2.42955219e-02],\n",
       "        [-5.13637850e-02,  3.35702993e-02, -5.03144136e-02],\n",
       "        [ 3.51390243e-02, -9.99142751e-02,  2.65707332e-03],\n",
       "        [ 7.20597580e-02, -1.89122334e-02, -9.22055691e-02],\n",
       "        [-5.53036126e-02, -7.61410739e-02, -7.45150061e-02],\n",
       "        [-6.11834154e-02,  2.45385133e-02, -3.57486717e-02],\n",
       "        [ 4.51143371e-02,  1.44622100e-02,  8.91716858e-02],\n",
       "        [-3.45994793e-02, -1.33454734e-02, -1.87490825e-02],\n",
       "        [-2.27271654e-02,  9.67052728e-02,  9.49425772e-02],\n",
       "        [-2.16453170e-02,  3.74330774e-02, -9.02966379e-02],\n",
       "        [ 4.09812985e-02, -3.40909552e-02,  2.22945085e-02],\n",
       "        [ 2.62271524e-02, -9.81502527e-02,  1.97456979e-03],\n",
       "        [ 2.56141983e-02, -3.14858928e-02, -3.44174402e-03],\n",
       "        [ 1.82751771e-02, -6.12101331e-02,  7.74389356e-02],\n",
       "        [-2.45520666e-02,  1.67750088e-02,  7.70891031e-02],\n",
       "        [ 9.43133053e-02,  7.38888090e-02,  9.13608847e-02],\n",
       "        [ 5.72712805e-02, -2.63585215e-02, -1.49230586e-02],\n",
       "        [ 6.33092364e-04,  2.92931441e-02, -2.79669324e-03],\n",
       "        [-7.24151544e-03, -3.68151404e-02, -1.00882212e-02],\n",
       "        [ 9.84000266e-02,  6.00762516e-02,  2.74083260e-02],\n",
       "        [ 3.35392095e-02, -8.10893402e-02,  5.61277829e-02],\n",
       "        [ 1.44960834e-02, -5.31603731e-02, -5.19390777e-03],\n",
       "        [-6.12251740e-02,  8.69675666e-02,  2.39949658e-02],\n",
       "        [-5.99894617e-02,  1.00157541e-02, -7.13017779e-02],\n",
       "        [ 9.75368842e-02, -4.42769863e-02,  9.98428389e-02],\n",
       "        [ 2.03255173e-02, -1.30556757e-04, -5.39045930e-02],\n",
       "        [ 9.70149381e-02,  3.56048481e-02,  7.04441999e-03],\n",
       "        [ 3.11045386e-02,  8.83663595e-02, -9.89744067e-02],\n",
       "        [ 7.83433029e-02,  2.45814586e-03,  5.81024919e-03],\n",
       "        [-4.31528693e-02, -9.01635953e-02, -9.65768920e-02],\n",
       "        [ 6.77487491e-03,  8.48970531e-02,  9.28690330e-02],\n",
       "        [ 7.52345994e-02,  1.54507486e-02, -3.98679040e-02],\n",
       "        [ 6.52795010e-02,  2.19474065e-02,  8.18730008e-02],\n",
       "        [ 3.58273797e-02, -9.15639326e-02,  8.08592588e-02],\n",
       "        [ 1.93504933e-02, -7.86408484e-02,  2.11769175e-02],\n",
       "        [-4.08662152e-02,  6.18389114e-03, -5.08735405e-02],\n",
       "        [ 9.47782844e-02,  6.50642142e-02,  1.50793083e-02],\n",
       "        [ 7.75088891e-02,  4.21245433e-02,  2.52690148e-02],\n",
       "        [-9.98698703e-02,  1.14008073e-02, -4.77224699e-02],\n",
       "        [-4.66013442e-02,  3.68415597e-02,  1.20936024e-02],\n",
       "        [ 6.95449354e-02, -7.43317827e-02,  4.79154421e-02],\n",
       "        [ 5.58270844e-02, -5.94328291e-02,  9.37053849e-02],\n",
       "        [-5.83402514e-02, -4.63784567e-02,  6.60034088e-02],\n",
       "        [ 8.80562067e-02, -7.47811943e-02, -8.51149037e-02],\n",
       "        [ 4.04233301e-02,  4.65177500e-03, -4.17579208e-02],\n",
       "        [-5.64469323e-02,  1.00027826e-02,  9.29442048e-02],\n",
       "        [ 6.00790731e-02,  6.33506996e-02,  7.67301719e-02],\n",
       "        [-1.31524978e-02,  8.31204206e-02,  6.47427561e-03],\n",
       "        [ 6.17109853e-02,  6.22874897e-03, -4.57310729e-02],\n",
       "        [-5.78239649e-02,  2.43648181e-03, -9.30810852e-02],\n",
       "        [ 4.11329259e-02,  3.80920358e-02,  2.67543449e-02],\n",
       "        [-9.16116759e-02,  8.13428909e-02, -6.85566962e-02],\n",
       "        [-3.67093347e-02,  6.57840632e-03,  8.64247754e-02],\n",
       "        [ 4.10719998e-02,  3.60335074e-02, -4.26547527e-02],\n",
       "        [ 5.22577295e-02,  3.30175912e-02, -4.85057890e-02],\n",
       "        [-5.30568711e-02, -9.53826383e-02, -3.46038625e-02],\n",
       "        [ 5.95230460e-02,  9.54605415e-02, -4.58163843e-02],\n",
       "        [ 8.35325127e-02, -6.60249824e-02, -8.51841863e-02],\n",
       "        [-9.72422245e-02,  9.14621555e-02, -7.55927953e-06],\n",
       "        [-7.28854463e-02, -3.01751494e-02, -2.21036170e-02],\n",
       "        [ 2.97771636e-02,  1.29236132e-02,  5.08070365e-02],\n",
       "        [ 1.20276576e-02,  7.97069818e-02,  1.64238084e-02],\n",
       "        [-1.55970573e-02, -9.93501711e-02, -2.31574740e-02],\n",
       "        [-6.84568211e-02, -2.38854848e-02,  9.85804871e-02],\n",
       "        [-5.22933751e-02, -4.81728688e-02, -4.87646181e-03],\n",
       "        [ 8.61935320e-02, -2.52844777e-02, -9.57008085e-03],\n",
       "        [-1.63455630e-02, -9.34902555e-02, -2.51496974e-02],\n",
       "        [ 3.72991542e-02,  9.68659338e-02,  8.35841835e-02],\n",
       "        [ 3.17043364e-02, -8.69076848e-02, -3.58160213e-02],\n",
       "        [-7.13499337e-02,  6.79340884e-02,  5.22549041e-02],\n",
       "        [-9.51932215e-02,  7.42039071e-02, -3.65860716e-02],\n",
       "        [ 5.39574900e-02,  9.87628661e-02,  7.79606523e-03],\n",
       "        [-4.63771112e-02, -1.70895606e-02,  8.16007107e-02],\n",
       "        [-7.07630590e-02,  1.30127091e-02, -1.06880171e-02],\n",
       "        [-8.18795297e-03, -3.79237743e-03, -8.30464176e-03],\n",
       "        [ 7.30065416e-02, -3.56632155e-02, -7.67498008e-02],\n",
       "        [-6.54158952e-02, -9.98778959e-02,  3.37577730e-02],\n",
       "        [ 8.33256410e-02, -3.01772697e-02,  7.49140930e-02],\n",
       "        [-6.27548772e-02, -3.89875068e-02,  7.43778133e-02],\n",
       "        [-7.48520494e-02,  2.61846911e-02, -5.34006618e-02],\n",
       "        [ 2.79436782e-02, -8.59727189e-02, -1.84515137e-02],\n",
       "        [-9.26596062e-02,  4.91519072e-02, -6.28105684e-03],\n",
       "        [ 8.75476286e-02, -2.24994680e-03, -5.89255355e-02],\n",
       "        [ 6.15711436e-02,  1.72744580e-02,  3.31439897e-02],\n",
       "        [ 2.54041199e-02,  4.23226645e-03,  4.39430922e-02],\n",
       "        [-4.04086486e-02, -1.38934478e-02,  5.38578443e-03],\n",
       "        [-2.81328727e-02,  6.85022548e-02, -3.97958606e-02],\n",
       "        [ 7.51404065e-02,  8.16881898e-02, -8.08171769e-02],\n",
       "        [ 5.70470579e-02,  1.85730236e-02,  8.25219263e-02],\n",
       "        [-8.75513554e-02,  8.37699324e-02,  3.68052125e-02],\n",
       "        [-6.12002887e-02, -1.54410645e-02, -5.48652947e-02],\n",
       "        [-6.07096078e-03, -8.46624523e-02, -9.38567594e-02],\n",
       "        [ 3.49972807e-02,  4.17604484e-02,  3.96309458e-02],\n",
       "        [-2.35412661e-02,  8.22786707e-03,  5.06696217e-02],\n",
       "        [-3.83436670e-03, -6.47443896e-02,  2.27615043e-03],\n",
       "        [ 1.05680488e-02,  2.37865979e-03, -9.61883962e-02],\n",
       "        [ 6.67946190e-02,  6.08656369e-02,  9.58393048e-03],\n",
       "        [-6.08739021e-03, -5.66446464e-02,  8.57935840e-02],\n",
       "        [-4.03395258e-02, -3.07254288e-02,  9.83414724e-02],\n",
       "        [ 5.68211315e-02, -8.63793957e-02, -2.33295685e-02],\n",
       "        [ 7.70561120e-03, -1.70407925e-02, -8.79234810e-02],\n",
       "        [ 2.27954537e-02, -8.75972286e-02,  6.27584523e-03],\n",
       "        [ 7.80456496e-02, -9.16834525e-02,  9.21528185e-02],\n",
       "        [ 5.34230505e-02,  7.21551544e-02,  4.29664759e-03],\n",
       "        [ 2.51631942e-02,  2.59151664e-02,  1.58171337e-02],\n",
       "        [-2.13175690e-03,  8.39975581e-02,  2.57059273e-02],\n",
       "        [-5.31120853e-02, -5.65151020e-02, -1.56974904e-02],\n",
       "        [-3.86995242e-02,  5.38229917e-02,  7.14052621e-02],\n",
       "        [ 5.37984632e-02, -9.47228298e-02,  1.46085387e-02],\n",
       "        [ 2.83616918e-02, -2.04440427e-02, -2.09180350e-02],\n",
       "        [ 1.60846701e-02, -5.45205637e-02,  4.27228776e-02],\n",
       "        [-1.39194411e-02, -8.52714661e-02,  5.14974227e-02],\n",
       "        [-9.55448696e-04, -1.91288343e-02, -5.85375542e-02],\n",
       "        [-8.85037053e-03, -9.32271704e-02, -4.95502315e-02],\n",
       "        [ 7.22362633e-02, -1.15779396e-02, -4.59034369e-02],\n",
       "        [-1.81416329e-02,  4.72645946e-02, -4.48701084e-02],\n",
       "        [ 9.27820759e-02, -8.16515802e-02,  4.93388412e-02],\n",
       "        [-7.80309968e-02,  3.03302584e-02,  3.05609888e-02],\n",
       "        [ 9.56569910e-02,  2.31329650e-02,  6.22237921e-02],\n",
       "        [-9.10717715e-03, -2.60398611e-02,  6.70131370e-02],\n",
       "        [ 6.42525543e-02,  5.37286666e-02,  5.40196456e-02],\n",
       "        [-5.12598751e-03,  3.98895388e-02,  9.08945488e-02],\n",
       "        [ 1.46780914e-02, -2.94200054e-02,  2.89075576e-02],\n",
       "        [ 1.62045024e-02,  5.31434976e-02, -6.01440184e-02],\n",
       "        [-1.54455965e-02, -5.96035831e-02, -3.41892764e-02],\n",
       "        [-5.36018666e-03, -5.84521104e-03, -2.47932466e-02],\n",
       "        [ 8.45369189e-02,  9.85776287e-02, -4.65347546e-02],\n",
       "        [ 4.34329510e-02, -1.93345464e-02, -6.39502155e-02],\n",
       "        [-7.20234336e-02, -3.16326620e-02, -6.70814354e-02],\n",
       "        [-3.43264896e-02,  2.16018599e-04,  9.68676030e-02],\n",
       "        [-5.29755848e-02,  7.88643859e-02, -1.47169573e-03],\n",
       "        [-8.65844121e-02, -3.75979963e-02, -9.80878420e-02],\n",
       "        [ 1.84324663e-02,  5.64649440e-02, -5.19445725e-02],\n",
       "        [ 6.88024089e-02,  2.86389813e-02,  7.72195011e-02],\n",
       "        [ 8.53125426e-02,  9.17860392e-02,  6.60234371e-02],\n",
       "        [ 4.31769677e-02,  9.22377408e-02,  8.28169584e-02],\n",
       "        [ 3.18818076e-02,  4.47963975e-02,  7.61632549e-02],\n",
       "        [ 7.15405309e-02, -8.45152523e-02, -8.09770999e-02],\n",
       "        [-9.00712311e-02, -7.38157332e-02,  7.60056917e-03],\n",
       "        [ 7.32931329e-03, -5.02824972e-02, -2.48321541e-02],\n",
       "        [ 1.24203999e-02,  4.78824687e-02,  3.09764872e-02],\n",
       "        [-6.61449805e-02,  1.26330154e-02,  5.73227070e-02],\n",
       "        [ 7.01072216e-02, -6.61113160e-03,  6.05770089e-02],\n",
       "        [-5.25890079e-02, -8.39072698e-02, -3.29613553e-02],\n",
       "        [-9.57222208e-02, -4.12133969e-02, -2.09250245e-02],\n",
       "        [ 8.77671256e-02, -6.14108152e-02, -5.07953137e-02],\n",
       "        [-9.24863366e-02, -1.48133117e-02,  6.28544951e-03],\n",
       "        [-1.82170987e-02, -7.58935587e-02,  1.01590445e-02],\n",
       "        [-5.36751375e-02, -7.10608885e-02,  2.98198964e-03],\n",
       "        [ 3.14396508e-02,  8.21219683e-02, -6.02085739e-02],\n",
       "        [-7.51690030e-02,  9.87064705e-02,  6.53178919e-03],\n",
       "        [-8.25501978e-02, -4.43944000e-02, -8.12607759e-04],\n",
       "        [ 8.32420675e-02, -7.41284631e-02, -6.85055575e-02],\n",
       "        [-7.83599988e-02, -8.60530436e-02, -8.61732885e-02],\n",
       "        [ 4.05583046e-02, -6.35929182e-02,  7.34520629e-02],\n",
       "        [-8.96247476e-02, -8.24547559e-02,  8.74789879e-02],\n",
       "        [ 9.97194566e-02,  5.77628142e-02,  8.93592790e-02],\n",
       "        [ 2.84518767e-02, -7.77026191e-02, -8.88288170e-02],\n",
       "        [ 3.50587419e-02, -1.40795853e-02, -4.21646004e-02],\n",
       "        [ 2.68225484e-02, -9.13444608e-02,  8.62977654e-02],\n",
       "        [-2.44871210e-02,  9.49440598e-02, -8.99025574e-02],\n",
       "        [ 2.69005254e-02, -7.63546531e-02, -3.85263569e-02],\n",
       "        [-2.00430416e-02,  7.11081475e-02, -6.70667589e-02],\n",
       "        [ 9.86413136e-02,  7.96757266e-02, -4.28716131e-02],\n",
       "        [ 6.58500213e-02,  4.22615376e-02, -7.17178122e-02],\n",
       "        [-2.67388520e-03,  6.55319309e-02, -8.79336190e-02],\n",
       "        [ 6.76101521e-02, -1.73710454e-02,  3.58883254e-02],\n",
       "        [ 7.85712469e-03,  2.66862052e-02,  7.44845094e-02],\n",
       "        [-1.36711558e-03, -8.73976812e-02, -8.68040601e-02],\n",
       "        [ 8.44821110e-02, -4.01768945e-02,  7.49536604e-02],\n",
       "        [ 5.30618131e-02, -9.09461156e-02, -2.62742452e-02],\n",
       "        [-2.62872912e-02, -9.99097005e-02, -8.27424526e-02],\n",
       "        [-9.78504361e-02, -5.59289721e-02, -7.98908712e-02],\n",
       "        [ 4.89222519e-02, -3.86501220e-03, -6.83566928e-02],\n",
       "        [ 9.18832868e-02,  6.62421286e-02, -8.81815180e-02],\n",
       "        [-4.62335171e-02, -1.30831935e-02,  2.95184804e-02],\n",
       "        [ 2.19474866e-02, -7.66912473e-02,  4.25411868e-02],\n",
       "        [-4.78262939e-02,  4.31692898e-02, -7.25668743e-02],\n",
       "        [-1.28423814e-02, -7.04637021e-02, -3.78000103e-02],\n",
       "        [ 2.78634788e-03,  3.09194364e-02, -3.52030098e-02],\n",
       "        [ 4.79077883e-02, -5.22873662e-02,  5.83256185e-02],\n",
       "        [-2.08683358e-02,  8.67519000e-02,  2.76552752e-02],\n",
       "        [ 6.07588678e-04, -6.92554116e-02, -8.31757206e-03],\n",
       "        [-7.01190755e-02,  1.46846869e-04, -3.05699799e-02],\n",
       "        [ 1.36063574e-02, -2.53809639e-03, -5.83304688e-02],\n",
       "        [-3.21405968e-03, -7.08866351e-02, -1.90367757e-02],\n",
       "        [ 4.77090701e-02,  2.11503915e-02,  2.33571306e-02],\n",
       "        [-3.11912652e-02, -6.84125572e-02,  2.98621263e-02],\n",
       "        [-5.24199232e-02, -1.54952854e-02, -5.84754162e-02],\n",
       "        [-3.39943106e-02, -7.03545774e-02,  9.22661510e-02],\n",
       "        [ 6.25169918e-02,  2.33693906e-02, -5.22678912e-02],\n",
       "        [ 9.01562795e-02,  2.30439603e-02, -2.65314560e-02],\n",
       "        [-5.19088767e-02,  5.12909107e-02, -3.12688127e-02],\n",
       "        [ 8.00619051e-02, -5.34027629e-02, -3.49442735e-02],\n",
       "        [ 7.30637163e-02,  5.80355823e-02, -2.19823662e-02],\n",
       "        [-1.94801984e-03, -1.91200543e-02,  2.21136380e-02],\n",
       "        [ 6.10728736e-02,  2.91793876e-02,  8.34066648e-02],\n",
       "        [-8.48553404e-02,  3.03827729e-02,  1.28409583e-02],\n",
       "        [-5.51526839e-02,  2.84413416e-02, -4.88446173e-02],\n",
       "        [ 6.66561276e-02, -8.10018033e-02, -9.60622281e-02],\n",
       "        [ 5.87540455e-02, -9.12585035e-02,  3.22558098e-02],\n",
       "        [-5.56938164e-02, -4.23238464e-02, -4.19776142e-02],\n",
       "        [-7.65161936e-02,  3.02189026e-02, -6.96245751e-02],\n",
       "        [ 2.23556906e-02, -1.65489573e-02, -2.59127360e-02],\n",
       "        [ 6.43545091e-02,  1.29420380e-03,  2.14570835e-02],\n",
       "        [-9.94837508e-02, -9.05190855e-02,  6.16564369e-03],\n",
       "        [ 5.59479892e-02, -9.53289121e-02,  6.73358217e-02],\n",
       "        [ 6.41523153e-02,  2.33732294e-02, -9.72717851e-02],\n",
       "        [-8.12082939e-02, -7.93189838e-02, -3.41416560e-02],\n",
       "        [ 4.41274862e-04,  9.06870439e-02, -4.95860358e-03],\n",
       "        [-9.73633975e-02,  4.06589620e-02, -3.30895185e-02],\n",
       "        [-4.24220189e-02,  8.97283256e-02,  5.12028709e-02],\n",
       "        [ 2.10284895e-02, -4.14717890e-02, -6.45389113e-02],\n",
       "        [-4.31376547e-02, -8.27845186e-02,  9.45775807e-02],\n",
       "        [ 9.69663486e-02,  6.25912547e-02,  6.51953891e-02],\n",
       "        [-4.48153127e-02, -9.36003561e-02, -2.28164060e-02],\n",
       "        [-4.93704863e-02,  5.80324195e-02,  8.41296837e-02],\n",
       "        [ 5.37092239e-02, -1.04407920e-02,  8.44131261e-02],\n",
       "        [-5.54825626e-02, -5.68822883e-02, -9.07045528e-02],\n",
       "        [ 4.68989562e-02,  9.50005823e-02, -4.79537707e-02],\n",
       "        [-9.50111696e-02, -2.21445312e-02, -1.44352840e-02],\n",
       "        [-4.09533266e-03, -5.44363445e-02, -7.01092717e-03],\n",
       "        [-8.02114755e-02,  4.49274592e-02, -9.37496796e-02],\n",
       "        [ 3.59098539e-02, -6.31869733e-02, -5.60478158e-02],\n",
       "        [-7.18841431e-02,  4.84560688e-02,  4.35884848e-02],\n",
       "        [ 7.88799049e-02,  2.37721121e-02,  3.63982618e-02],\n",
       "        [ 3.23179290e-02,  6.83497265e-02,  3.98739204e-02],\n",
       "        [-9.77222145e-02,  3.66839844e-02,  2.27924671e-03],\n",
       "        [ 6.21307314e-03, -6.52605095e-02, -2.57041035e-02],\n",
       "        [-3.61753133e-03, -3.72648313e-02,  7.25023212e-02],\n",
       "        [-4.46580128e-03, -7.04220661e-02,  3.38248189e-02],\n",
       "        [ 5.82049308e-02,  4.91528643e-02, -6.57615053e-02],\n",
       "        [-4.56171436e-03,  7.26038292e-02,  8.60223621e-02],\n",
       "        [ 2.84645371e-02, -8.55400860e-02, -6.65475205e-02],\n",
       "        [-8.05475488e-02,  6.90538585e-02, -5.81053197e-02],\n",
       "        [ 7.01553002e-02,  3.79306562e-02,  7.80127198e-02],\n",
       "        [-3.57574367e-02, -5.82900452e-03,  7.24005536e-03],\n",
       "        [ 5.95490351e-02, -5.28561826e-02, -2.79854962e-02],\n",
       "        [-7.25205692e-02,  7.47659763e-02, -2.31232155e-02],\n",
       "        [-8.15703676e-02, -9.22112430e-02,  7.39323046e-02],\n",
       "        [ 7.64816776e-02, -2.47404375e-03, -6.73717335e-02],\n",
       "        [ 1.78269754e-02,  6.25828911e-02, -9.36027213e-02],\n",
       "        [-9.56180021e-02,  6.49213865e-02,  7.56144673e-02],\n",
       "        [ 8.96263650e-02, -7.89103094e-02,  1.42706409e-02],\n",
       "        [-4.73660678e-02, -8.31899792e-02, -7.30372220e-03],\n",
       "        [-7.47382045e-02,  9.17681828e-02,  8.24770927e-02],\n",
       "        [-5.58790844e-03, -6.82766885e-02, -7.24251494e-02],\n",
       "        [-1.98193685e-02,  4.46888968e-02, -7.97133248e-02],\n",
       "        [ 2.18149275e-02,  5.75723236e-02, -6.25423508e-02],\n",
       "        [ 2.95051560e-02,  9.97900069e-02,  1.20789502e-02],\n",
       "        [-1.61125848e-03,  3.33361148e-02, -5.66182783e-02],\n",
       "        [ 8.24132078e-02, -4.51348567e-02,  4.75811542e-02],\n",
       "        [-3.78868096e-02, -2.73189321e-02,  2.34615579e-02],\n",
       "        [ 8.15326422e-02,  2.09506061e-02,  6.65546507e-02],\n",
       "        [-7.74852857e-02, -4.30321246e-02,  7.17577636e-02],\n",
       "        [ 7.26882261e-03,  7.91055400e-02,  1.92027445e-02],\n",
       "        [ 6.03040792e-02,  8.23174138e-03,  6.01875409e-02],\n",
       "        [-1.62876695e-02,  9.86228809e-02, -5.27914874e-02],\n",
       "        [-2.92032566e-02,  6.33860826e-02,  7.06076249e-02],\n",
       "        [ 7.76638599e-02, -7.54589887e-02,  5.52879028e-02],\n",
       "        [-8.79756734e-02, -1.02478275e-02, -5.18852919e-02],\n",
       "        [ 9.35356244e-02, -3.22960317e-02, -4.50956374e-02],\n",
       "        [-2.34685186e-02, -4.50538769e-02, -2.99740434e-02],\n",
       "        [-4.91272584e-02,  1.70481801e-02,  8.12431872e-02]])}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backward_pass([0,0,0,1,1], 1, W, out_vals, lr=0.001, freeze_emb=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:08:59.937442Z",
     "start_time": "2020-02-15T14:08:59.932221Z"
    }
   },
   "source": [
    "Finally you need to modify SGD to support back-propagation by using the `forward_pass` and `backward_pass` functions.\n",
    "\n",
    "The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `W`: the weights of the network (dictionary)\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `dropout`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated (to be used by the backward pass function).\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:19.021428Z",
     "start_time": "2020-04-02T15:09:19.017835Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, W, X_dev=[], Y_dev=[], lr=0.001, \n",
    "        dropout=0.2, epochs=5, tolerance=0.001, freeze_emb=False, \n",
    "        print_progress=True):\n",
    "    \n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    ''' Initially set to infinity. Later, it will be used to track the previous validation loss'''\n",
    "    prev_val_loss = float(\"inf\")\n",
    "    best_weights = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        '''iterates over each example in the training dataset'''\n",
    "        for i in range(len(X_tr)):\n",
    "            x = X_tr[i]\n",
    "            y = Y_tr[i]\n",
    "            \n",
    "            # Forward pass\n",
    "            out_vals = forward_pass(x, W, dropout_rate=dropout)\n",
    "            \n",
    "            ''' Compute loss and gradients using backward pass'''\n",
    "            W = backward_pass(x, y, W, out_vals, lr=lr, freeze_emb=freeze_emb)\n",
    "        \n",
    "                \n",
    "        '''Compute and store training loss'''\n",
    "        tr_loss = 0\n",
    "        for i in range(len(X_tr)):\n",
    "            x = X_tr[i]\n",
    "            y = Y_tr[i]\n",
    "            out_vals = forward_pass(x, W, dropout_rate=0)\n",
    "            tr_loss = tr_loss + categorical_loss(y, out_vals['pred'])\n",
    "        tr_loss = tr_loss / len(X_tr)\n",
    "        training_loss_history.append(tr_loss)\n",
    "        \n",
    "        '''Compute and store validation loss'''\n",
    "        val_loss = 0\n",
    "        for i in range(len(X_dev)):\n",
    "            x = X_dev[i]\n",
    "            y = Y_dev[i]\n",
    "        \n",
    "            out_vals = forward_pass(x, W, dropout_rate=0)\n",
    "            val_loss =  val_loss +categorical_loss(y, out_vals['pred'])\n",
    "        val_loss = val_loss / len(X_dev)\n",
    "        validation_loss_history.append(val_loss)\n",
    "        \n",
    "        '''Checking if validation loss has increased'''\n",
    "        \n",
    "        if val_loss < prev_val_loss:\n",
    "            prev_val_loss = val_loss\n",
    "            best_weights = W\n",
    "            '''If true, model's performance is not improving significantly and it will break'''\n",
    "        elif val_loss >= prev_val_loss + tolerance:\n",
    "            break\n",
    "            \n",
    "        # Print progress\n",
    "        if print_progress:\n",
    "            print(f\"Epoch {epoch+1}: train loss {tr_loss:.4f}, val loss {val_loss:.4f}\") \n",
    "    \n",
    "    return W, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:10:15.772383Z",
     "start_time": "2020-02-15T14:10:15.767855Z"
    }
   },
   "source": [
    "Now you are ready to train and evaluate your neural net. First, you need to define your network using the `network_weights` function followed by SGD with backprop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:33.643515Z",
     "start_time": "2020-04-02T15:09:33.640943Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (2000, 300)\n",
      "Shape W1 (300, 3)\n",
      "Epoch 1: train loss 1.0938, val loss 1.0963\n",
      "Epoch 2: train loss 1.0894, val loss 1.0936\n",
      "Epoch 3: train loss 1.0847, val loss 1.0906\n",
      "Epoch 4: train loss 1.0794, val loss 1.0873\n",
      "Epoch 5: train loss 1.0733, val loss 1.0835\n",
      "Epoch 6: train loss 1.0662, val loss 1.0791\n",
      "Epoch 7: train loss 1.0580, val loss 1.0739\n",
      "Epoch 8: train loss 1.0482, val loss 1.0678\n",
      "Epoch 9: train loss 1.0367, val loss 1.0606\n",
      "Epoch 10: train loss 1.0233, val loss 1.0520\n",
      "Epoch 11: train loss 1.0076, val loss 1.0420\n",
      "Epoch 12: train loss 0.9895, val loss 1.0302\n",
      "Epoch 13: train loss 0.9688, val loss 1.0166\n",
      "Epoch 14: train loss 0.9457, val loss 1.0010\n",
      "Epoch 15: train loss 0.9201, val loss 0.9832\n",
      "Epoch 16: train loss 0.8925, val loss 0.9634\n",
      "Epoch 17: train loss 0.8631, val loss 0.9416\n",
      "Epoch 18: train loss 0.8325, val loss 0.9179\n",
      "Epoch 19: train loss 0.8012, val loss 0.8925\n",
      "Epoch 20: train loss 0.7697, val loss 0.8656\n",
      "Epoch 21: train loss 0.7383, val loss 0.8376\n",
      "Epoch 22: train loss 0.7073, val loss 0.8086\n",
      "Epoch 23: train loss 0.6772, val loss 0.7791\n",
      "Epoch 24: train loss 0.6479, val loss 0.7494\n",
      "Epoch 25: train loss 0.6196, val loss 0.7196\n",
      "Epoch 26: train loss 0.5924, val loss 0.6902\n",
      "Epoch 27: train loss 0.5664, val loss 0.6617\n",
      "Epoch 28: train loss 0.5416, val loss 0.6342\n",
      "Epoch 29: train loss 0.5181, val loss 0.6078\n",
      "Epoch 30: train loss 0.4960, val loss 0.5829\n",
      "Epoch 31: train loss 0.4752, val loss 0.5595\n",
      "Epoch 32: train loss 0.4558, val loss 0.5376\n",
      "Epoch 33: train loss 0.4377, val loss 0.5174\n",
      "Epoch 34: train loss 0.4210, val loss 0.4987\n",
      "Epoch 35: train loss 0.4056, val loss 0.4815\n",
      "Epoch 36: train loss 0.3912, val loss 0.4658\n",
      "Epoch 37: train loss 0.3779, val loss 0.4512\n",
      "Epoch 38: train loss 0.3655, val loss 0.4378\n",
      "Epoch 39: train loss 0.3540, val loss 0.4256\n",
      "Epoch 40: train loss 0.3432, val loss 0.4144\n",
      "Epoch 41: train loss 0.3332, val loss 0.4042\n",
      "Epoch 42: train loss 0.3236, val loss 0.3946\n",
      "Epoch 43: train loss 0.3146, val loss 0.3857\n",
      "Epoch 44: train loss 0.3062, val loss 0.3775\n",
      "Epoch 45: train loss 0.2982, val loss 0.3700\n",
      "Epoch 46: train loss 0.2906, val loss 0.3630\n",
      "Epoch 47: train loss 0.2833, val loss 0.3565\n",
      "Epoch 48: train loss 0.2764, val loss 0.3507\n",
      "Epoch 49: train loss 0.2697, val loss 0.3453\n",
      "Epoch 50: train loss 0.2634, val loss 0.3400\n",
      "Epoch 51: train loss 0.2572, val loss 0.3354\n",
      "Epoch 52: train loss 0.2514, val loss 0.3310\n",
      "Epoch 53: train loss 0.2457, val loss 0.3267\n",
      "Epoch 54: train loss 0.2403, val loss 0.3229\n",
      "Epoch 55: train loss 0.2351, val loss 0.3192\n",
      "Epoch 56: train loss 0.2301, val loss 0.3159\n",
      "Epoch 57: train loss 0.2252, val loss 0.3129\n",
      "Epoch 58: train loss 0.2204, val loss 0.3099\n",
      "Epoch 59: train loss 0.2158, val loss 0.3073\n",
      "Epoch 60: train loss 0.2115, val loss 0.3046\n",
      "Epoch 61: train loss 0.2071, val loss 0.3022\n",
      "Epoch 62: train loss 0.2030, val loss 0.3002\n",
      "Epoch 63: train loss 0.1990, val loss 0.2982\n",
      "Epoch 64: train loss 0.1952, val loss 0.2962\n",
      "Epoch 65: train loss 0.1915, val loss 0.2943\n",
      "Epoch 66: train loss 0.1878, val loss 0.2926\n",
      "Epoch 67: train loss 0.1842, val loss 0.2910\n",
      "Epoch 68: train loss 0.1807, val loss 0.2898\n",
      "Epoch 69: train loss 0.1773, val loss 0.2885\n",
      "Epoch 70: train loss 0.1741, val loss 0.2872\n",
      "Epoch 71: train loss 0.1710, val loss 0.2860\n",
      "Epoch 72: train loss 0.1679, val loss 0.2850\n",
      "Epoch 73: train loss 0.1650, val loss 0.2840\n",
      "Epoch 74: train loss 0.1619, val loss 0.2834\n",
      "Epoch 75: train loss 0.1591, val loss 0.2823\n",
      "Epoch 76: train loss 0.1564, val loss 0.2813\n",
      "Epoch 77: train loss 0.1537, val loss 0.2805\n",
      "Epoch 78: train loss 0.1512, val loss 0.2796\n",
      "Epoch 79: train loss 0.1487, val loss 0.2790\n",
      "Epoch 80: train loss 0.1462, val loss 0.2784\n",
      "Epoch 81: train loss 0.1437, val loss 0.2779\n",
      "Epoch 82: train loss 0.1413, val loss 0.2776\n",
      "Epoch 83: train loss 0.1390, val loss 0.2773\n",
      "Epoch 84: train loss 0.1366, val loss 0.2771\n",
      "Epoch 85: train loss 0.1346, val loss 0.2761\n",
      "Epoch 86: train loss 0.1324, val loss 0.2756\n",
      "Epoch 87: train loss 0.1302, val loss 0.2755\n",
      "Epoch 88: train loss 0.1282, val loss 0.2750\n",
      "Epoch 89: train loss 0.1262, val loss 0.2747\n",
      "Epoch 90: train loss 0.1242, val loss 0.2745\n",
      "Epoch 91: train loss 0.1223, val loss 0.2741\n",
      "Epoch 92: train loss 0.1203, val loss 0.2740\n",
      "Epoch 93: train loss 0.1185, val loss 0.2736\n",
      "Epoch 94: train loss 0.1167, val loss 0.2734\n",
      "Epoch 95: train loss 0.1149, val loss 0.2730\n",
      "Epoch 96: train loss 0.1132, val loss 0.2729\n",
      "Epoch 97: train loss 0.1114, val loss 0.2727\n",
      "Epoch 98: train loss 0.1098, val loss 0.2728\n",
      "Epoch 99: train loss 0.1082, val loss 0.2729\n",
      "Epoch 100: train loss 0.1066, val loss 0.2727\n"
     ]
    }
   ],
   "source": [
    "X_tr=train_articles_ids\n",
    "Y_tr=train_label\n",
    "X_dev=dev_articles_ids\n",
    "Y_dev=dev_label\n",
    "W = network_weights(vocab_size=len(vocab),embedding_dim=300,\n",
    "                    hidden_dim=[], num_classes=3)\n",
    "\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "\n",
    "W, loss_tr, dev_loss = SGD(X_tr, Y_tr,\n",
    "                            W,\n",
    "                            X_dev=X_dev, \n",
    "                            Y_dev=Y_dev,\n",
    "                            lr=0.001, \n",
    "                            dropout=0.2,\n",
    "                            freeze_emb=False,\n",
    "                            tolerance=0.01,\n",
    "                            epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:15.716497Z",
     "start_time": "2020-04-02T14:27:15.612736Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_tr, color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss History')\n",
    "\n",
    "# plot validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(dev_loss, color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss History')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:10:11.037495Z",
     "start_time": "2020-04-02T15:10:11.034999Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8533333333333334\n",
      "Precision: 0.8558498866533634\n",
      "Recall: 0.8533333333333334\n",
      "F1-Score: 0.8532238631062531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "X_te= test_articles_ids\n",
    "Y_te= test_label\n",
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['pred']) \n",
    "            for x,y in zip(X_te,Y_te)]\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 1.0874, val loss 1.0899\n",
      "Epoch 2: train loss 1.0703, val loss 1.0786\n",
      "Epoch 3: train loss 1.0527, val loss 1.0670\n",
      "Epoch 4: train loss 1.0341, val loss 1.0548\n",
      "Epoch 5: train loss 1.0142, val loss 1.0417\n",
      "Epoch 6: train loss 0.9927, val loss 1.0273\n",
      "Epoch 7: train loss 0.9696, val loss 1.0117\n",
      "Epoch 8: train loss 0.9450, val loss 0.9948\n",
      "Epoch 9: train loss 0.9188, val loss 0.9765\n",
      "Epoch 10: train loss 0.8913, val loss 0.9567\n",
      "Epoch 11: train loss 0.8627, val loss 0.9355\n",
      "Epoch 12: train loss 0.8331, val loss 0.9128\n",
      "Epoch 13: train loss 0.8029, val loss 0.8885\n",
      "Epoch 14: train loss 0.7725, val loss 0.8630\n",
      "Epoch 15: train loss 0.7421, val loss 0.8364\n",
      "Epoch 16: train loss 0.7120, val loss 0.8090\n",
      "Epoch 17: train loss 0.6824, val loss 0.7809\n",
      "Epoch 18: train loss 0.6538, val loss 0.7526\n",
      "Epoch 19: train loss 0.6261, val loss 0.7240\n",
      "Epoch 20: train loss 0.5996, val loss 0.6957\n",
      "Epoch 21: train loss 0.5742, val loss 0.6680\n",
      "Epoch 22: train loss 0.5503, val loss 0.6416\n",
      "Epoch 23: train loss 0.5274, val loss 0.6161\n",
      "Epoch 24: train loss 0.5060, val loss 0.5922\n",
      "Epoch 25: train loss 0.4858, val loss 0.5692\n",
      "Epoch 26: train loss 0.4669, val loss 0.5479\n",
      "Epoch 27: train loss 0.4494, val loss 0.5279\n",
      "Epoch 28: train loss 0.4329, val loss 0.5092\n",
      "Epoch 29: train loss 0.4174, val loss 0.4921\n",
      "Epoch 30: train loss 0.4031, val loss 0.4762\n",
      "Epoch 31: train loss 0.3896, val loss 0.4612\n",
      "Epoch 32: train loss 0.3772, val loss 0.4480\n",
      "Epoch 33: train loss 0.3656, val loss 0.4356\n",
      "Epoch 34: train loss 0.3546, val loss 0.4238\n",
      "Epoch 35: train loss 0.3445, val loss 0.4128\n",
      "Epoch 36: train loss 0.3347, val loss 0.4029\n",
      "Epoch 37: train loss 0.3256, val loss 0.3931\n",
      "Epoch 38: train loss 0.3170, val loss 0.3842\n",
      "Epoch 39: train loss 0.3089, val loss 0.3754\n",
      "Epoch 40: train loss 0.3010, val loss 0.3676\n",
      "Epoch 41: train loss 0.2935, val loss 0.3603\n",
      "Epoch 42: train loss 0.2863, val loss 0.3538\n",
      "Epoch 43: train loss 0.2795, val loss 0.3478\n",
      "Epoch 44: train loss 0.2730, val loss 0.3421\n",
      "Epoch 45: train loss 0.2669, val loss 0.3362\n",
      "Epoch 46: train loss 0.2607, val loss 0.3309\n",
      "Epoch 47: train loss 0.2548, val loss 0.3262\n",
      "Epoch 48: train loss 0.2492, val loss 0.3214\n",
      "Epoch 49: train loss 0.2436, val loss 0.3173\n",
      "Epoch 50: train loss 0.2385, val loss 0.3133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 1.0906, val loss 1.0921\n",
      "Epoch 2: train loss 1.0766, val loss 1.0834\n",
      "Epoch 3: train loss 1.0620, val loss 1.0742\n",
      "Epoch 4: train loss 1.0466, val loss 1.0644\n",
      "Epoch 5: train loss 1.0297, val loss 1.0536\n",
      "Epoch 6: train loss 1.0110, val loss 1.0417\n",
      "Epoch 7: train loss 0.9903, val loss 1.0285\n",
      "Epoch 8: train loss 0.9674, val loss 1.0138\n",
      "Epoch 9: train loss 0.9421, val loss 0.9973\n",
      "Epoch 10: train loss 0.9146, val loss 0.9790\n",
      "Epoch 11: train loss 0.8850, val loss 0.9587\n",
      "Epoch 12: train loss 0.8540, val loss 0.9367\n",
      "Epoch 13: train loss 0.8220, val loss 0.9129\n",
      "Epoch 14: train loss 0.7895, val loss 0.8875\n",
      "Epoch 15: train loss 0.7571, val loss 0.8606\n",
      "Epoch 16: train loss 0.7251, val loss 0.8325\n",
      "Epoch 17: train loss 0.6937, val loss 0.8034\n",
      "Epoch 18: train loss 0.6632, val loss 0.7736\n",
      "Epoch 19: train loss 0.6336, val loss 0.7433\n",
      "Epoch 20: train loss 0.6055, val loss 0.7133\n",
      "Epoch 21: train loss 0.5784, val loss 0.6836\n",
      "Epoch 22: train loss 0.5522, val loss 0.6542\n",
      "Epoch 23: train loss 0.5272, val loss 0.6256\n",
      "Epoch 24: train loss 0.5039, val loss 0.5982\n",
      "Epoch 25: train loss 0.4824, val loss 0.5723\n",
      "Epoch 26: train loss 0.4625, val loss 0.5478\n",
      "Epoch 27: train loss 0.4433, val loss 0.5246\n",
      "Epoch 28: train loss 0.4266, val loss 0.5034\n",
      "Epoch 29: train loss 0.4103, val loss 0.4841\n",
      "Epoch 30: train loss 0.3960, val loss 0.4660\n",
      "Epoch 31: train loss 0.3826, val loss 0.4498\n",
      "Epoch 32: train loss 0.3703, val loss 0.4350\n",
      "Epoch 33: train loss 0.3595, val loss 0.4209\n",
      "Epoch 34: train loss 0.3490, val loss 0.4082\n",
      "Epoch 35: train loss 0.3394, val loss 0.3966\n",
      "Epoch 36: train loss 0.3304, val loss 0.3867\n",
      "Epoch 37: train loss 0.3223, val loss 0.3766\n",
      "Epoch 38: train loss 0.3147, val loss 0.3673\n",
      "Epoch 39: train loss 0.3074, val loss 0.3592\n",
      "Epoch 40: train loss 0.3003, val loss 0.3518\n",
      "Epoch 41: train loss 0.2936, val loss 0.3450\n",
      "Epoch 42: train loss 0.2874, val loss 0.3390\n",
      "Epoch 43: train loss 0.2816, val loss 0.3340\n",
      "Epoch 44: train loss 0.2754, val loss 0.3294\n",
      "Epoch 45: train loss 0.2705, val loss 0.3241\n",
      "Epoch 46: train loss 0.2657, val loss 0.3186\n",
      "Epoch 47: train loss 0.2605, val loss 0.3152\n",
      "Epoch 48: train loss 0.2550, val loss 0.3111\n",
      "Epoch 49: train loss 0.2505, val loss 0.3079\n",
      "Epoch 50: train loss 0.2452, val loss 0.3053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 1.0509, val loss 1.0716\n",
      "Epoch 2: train loss 0.9923, val loss 1.0313\n",
      "Epoch 3: train loss 0.9380, val loss 0.9932\n",
      "Epoch 4: train loss 0.8869, val loss 0.9565\n",
      "Epoch 5: train loss 0.8388, val loss 0.9209\n",
      "Epoch 6: train loss 0.7935, val loss 0.8860\n",
      "Epoch 7: train loss 0.7509, val loss 0.8519\n",
      "Epoch 8: train loss 0.7108, val loss 0.8185\n",
      "Epoch 9: train loss 0.6733, val loss 0.7862\n",
      "Epoch 10: train loss 0.6384, val loss 0.7553\n",
      "Epoch 11: train loss 0.6061, val loss 0.7258\n",
      "Epoch 12: train loss 0.5758, val loss 0.6976\n",
      "Epoch 13: train loss 0.5479, val loss 0.6710\n",
      "Epoch 14: train loss 0.5223, val loss 0.6462\n",
      "Epoch 15: train loss 0.4985, val loss 0.6230\n",
      "Epoch 16: train loss 0.4765, val loss 0.6014\n",
      "Epoch 17: train loss 0.4564, val loss 0.5815\n",
      "Epoch 18: train loss 0.4378, val loss 0.5630\n",
      "Epoch 19: train loss 0.4206, val loss 0.5462\n",
      "Epoch 20: train loss 0.4047, val loss 0.5303\n",
      "Epoch 21: train loss 0.3898, val loss 0.5157\n",
      "Epoch 22: train loss 0.3762, val loss 0.5020\n",
      "Epoch 23: train loss 0.3633, val loss 0.4897\n",
      "Epoch 24: train loss 0.3513, val loss 0.4782\n",
      "Epoch 25: train loss 0.3401, val loss 0.4674\n",
      "Epoch 26: train loss 0.3295, val loss 0.4573\n",
      "Epoch 27: train loss 0.3198, val loss 0.4476\n",
      "Epoch 28: train loss 0.3103, val loss 0.4388\n",
      "Epoch 29: train loss 0.3013, val loss 0.4309\n",
      "Epoch 30: train loss 0.2926, val loss 0.4231\n",
      "Epoch 31: train loss 0.2844, val loss 0.4162\n",
      "Epoch 32: train loss 0.2767, val loss 0.4097\n",
      "Epoch 33: train loss 0.2694, val loss 0.4034\n",
      "Epoch 34: train loss 0.2623, val loss 0.3972\n",
      "Epoch 35: train loss 0.2556, val loss 0.3912\n",
      "Epoch 36: train loss 0.2492, val loss 0.3861\n",
      "Epoch 37: train loss 0.2429, val loss 0.3819\n",
      "Epoch 38: train loss 0.2369, val loss 0.3772\n",
      "Epoch 39: train loss 0.2310, val loss 0.3734\n",
      "Epoch 40: train loss 0.2254, val loss 0.3694\n",
      "Epoch 41: train loss 0.2202, val loss 0.3652\n",
      "Epoch 42: train loss 0.2151, val loss 0.3617\n",
      "Epoch 43: train loss 0.2102, val loss 0.3579\n",
      "Epoch 44: train loss 0.2053, val loss 0.3548\n",
      "Epoch 45: train loss 0.2008, val loss 0.3513\n",
      "Epoch 46: train loss 0.1962, val loss 0.3489\n",
      "Epoch 47: train loss 0.1918, val loss 0.3463\n",
      "Epoch 48: train loss 0.1875, val loss 0.3441\n",
      "Epoch 49: train loss 0.1834, val loss 0.3418\n",
      "Epoch 50: train loss 0.1794, val loss 0.3398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 1.0755, val loss 1.1224\n",
      "Epoch 2: train loss 1.0126, val loss 1.0802\n",
      "Epoch 3: train loss 0.9547, val loss 1.0402\n",
      "Epoch 4: train loss 0.9000, val loss 1.0011\n",
      "Epoch 5: train loss 0.8484, val loss 0.9628\n",
      "Epoch 6: train loss 0.7991, val loss 0.9246\n",
      "Epoch 7: train loss 0.7524, val loss 0.8869\n",
      "Epoch 8: train loss 0.7080, val loss 0.8493\n",
      "Epoch 9: train loss 0.6661, val loss 0.8120\n",
      "Epoch 10: train loss 0.6270, val loss 0.7757\n",
      "Epoch 11: train loss 0.5908, val loss 0.7406\n",
      "Epoch 12: train loss 0.5572, val loss 0.7069\n",
      "Epoch 13: train loss 0.5267, val loss 0.6751\n",
      "Epoch 14: train loss 0.4983, val loss 0.6443\n",
      "Epoch 15: train loss 0.4727, val loss 0.6159\n",
      "Epoch 16: train loss 0.4497, val loss 0.5902\n",
      "Epoch 17: train loss 0.4287, val loss 0.5660\n",
      "Epoch 18: train loss 0.4100, val loss 0.5438\n",
      "Epoch 19: train loss 0.3929, val loss 0.5239\n",
      "Epoch 20: train loss 0.3771, val loss 0.5053\n",
      "Epoch 21: train loss 0.3629, val loss 0.4885\n",
      "Epoch 22: train loss 0.3498, val loss 0.4728\n",
      "Epoch 23: train loss 0.3381, val loss 0.4586\n",
      "Epoch 24: train loss 0.3270, val loss 0.4457\n",
      "Epoch 25: train loss 0.3166, val loss 0.4338\n",
      "Epoch 26: train loss 0.3067, val loss 0.4235\n",
      "Epoch 27: train loss 0.2975, val loss 0.4140\n",
      "Epoch 28: train loss 0.2890, val loss 0.4056\n",
      "Epoch 29: train loss 0.2807, val loss 0.3977\n",
      "Epoch 30: train loss 0.2731, val loss 0.3899\n",
      "Epoch 31: train loss 0.2658, val loss 0.3832\n",
      "Epoch 32: train loss 0.2588, val loss 0.3767\n",
      "Epoch 33: train loss 0.2520, val loss 0.3715\n",
      "Epoch 34: train loss 0.2458, val loss 0.3658\n",
      "Epoch 35: train loss 0.2397, val loss 0.3612\n",
      "Epoch 36: train loss 0.2336, val loss 0.3570\n",
      "Epoch 37: train loss 0.2277, val loss 0.3535\n",
      "Epoch 38: train loss 0.2221, val loss 0.3497\n",
      "Epoch 39: train loss 0.2169, val loss 0.3461\n",
      "Epoch 40: train loss 0.2115, val loss 0.3432\n",
      "Epoch 41: train loss 0.2064, val loss 0.3403\n",
      "Epoch 42: train loss 0.2016, val loss 0.3373\n",
      "Epoch 43: train loss 0.1970, val loss 0.3347\n",
      "Epoch 44: train loss 0.1924, val loss 0.3329\n",
      "Epoch 45: train loss 0.1879, val loss 0.3310\n",
      "Epoch 46: train loss 0.1836, val loss 0.3292\n",
      "Epoch 47: train loss 0.1797, val loss 0.3274\n",
      "Epoch 48: train loss 0.1755, val loss 0.3256\n",
      "Epoch 49: train loss 0.1717, val loss 0.3240\n",
      "Epoch 50: train loss 0.1679, val loss 0.3226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.8991, val loss 0.9717\n",
      "Epoch 2: train loss 0.7116, val loss 0.7975\n",
      "Epoch 3: train loss 0.5622, val loss 0.6079\n",
      "Epoch 4: train loss 0.4384, val loss 0.4594\n",
      "Epoch 5: train loss 0.3504, val loss 0.3718\n",
      "Epoch 6: train loss 0.2889, val loss 0.3198\n",
      "Epoch 7: train loss 0.2533, val loss 0.2946\n",
      "Epoch 8: train loss 0.2217, val loss 0.2838\n",
      "Epoch 9: train loss 0.1885, val loss 0.2640\n",
      "Epoch 10: train loss 0.1601, val loss 0.2600\n",
      "Epoch 11: train loss 0.1394, val loss 0.2526\n",
      "Epoch 12: train loss 0.1241, val loss 0.2550\n",
      "Epoch 13: train loss 0.1091, val loss 0.2600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.8681, val loss 0.9480\n",
      "Epoch 2: train loss 0.6991, val loss 0.7700\n",
      "Epoch 3: train loss 0.5540, val loss 0.5511\n",
      "Epoch 4: train loss 0.4585, val loss 0.4197\n",
      "Epoch 5: train loss 0.3931, val loss 0.3616\n",
      "Epoch 6: train loss 0.3194, val loss 0.3026\n",
      "Epoch 7: train loss 0.2792, val loss 0.2944\n",
      "Epoch 8: train loss 0.2551, val loss 0.2919\n",
      "Epoch 9: train loss 0.2195, val loss 0.2910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.8700, val loss 0.9600\n",
      "Epoch 2: train loss 0.6302, val loss 0.7014\n",
      "Epoch 3: train loss 0.4679, val loss 0.5188\n",
      "Epoch 4: train loss 0.3612, val loss 0.4111\n",
      "Epoch 5: train loss 0.2909, val loss 0.3500\n",
      "Epoch 6: train loss 0.2371, val loss 0.3090\n",
      "Epoch 7: train loss 0.1955, val loss 0.2832\n",
      "Epoch 8: train loss 0.1681, val loss 0.2674\n",
      "Epoch 9: train loss 0.1420, val loss 0.2565\n",
      "Epoch 10: train loss 0.1212, val loss 0.2487\n",
      "Epoch 11: train loss 0.1028, val loss 0.2439\n",
      "Epoch 12: train loss 0.0879, val loss 0.2423\n",
      "Epoch 13: train loss 0.0748, val loss 0.2453\n",
      "Epoch 14: train loss 0.0661, val loss 0.2446\n",
      "Epoch 15: train loss 0.0572, val loss 0.2438\n",
      "Epoch 16: train loss 0.0495, val loss 0.2437\n",
      "Epoch 17: train loss 0.0435, val loss 0.2484\n",
      "Epoch 18: train loss 0.0374, val loss 0.2496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.8805, val loss 0.9556\n",
      "Epoch 2: train loss 0.6711, val loss 0.7235\n",
      "Epoch 3: train loss 0.4964, val loss 0.5124\n",
      "Epoch 4: train loss 0.3888, val loss 0.4013\n",
      "Epoch 5: train loss 0.3109, val loss 0.3320\n",
      "Epoch 6: train loss 0.2601, val loss 0.3007\n",
      "Epoch 7: train loss 0.2187, val loss 0.2835\n",
      "Epoch 8: train loss 0.1742, val loss 0.2722\n",
      "Epoch 9: train loss 0.1546, val loss 0.2720\n",
      "Epoch 10: train loss 0.1313, val loss 0.2703\n",
      "Epoch 11: train loss 0.1147, val loss 0.2697\n",
      "Epoch 12: train loss 0.0958, val loss 0.2711\n",
      "Epoch 13: train loss 0.0811, val loss 0.2732\n",
      "Epoch 14: train loss 0.0659, val loss 0.2726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.001, 0.01]\n",
    "Emb_size = [50, 200]\n",
    "dropout_rates = [0.2, 0.4]\n",
    "best_accuracy = 0.0\n",
    "best_hyperparams = {}\n",
    "para=[]\n",
    "matrix=[]\n",
    "\n",
    "for lr in learning_rate:\n",
    "    for dim in Emb_size:\n",
    "        for dr in dropout_rates:\n",
    "            W = network_weights(vocab_size=len(vocab), \n",
    "                    embedding_dim=dim, \n",
    "                    hidden_dim=[], \n",
    "                    num_classes=3, \n",
    "                    init_val = 0.5)\n",
    "            W, loss_tr, dev_loss = SGD(X_tr, Y_tr,\n",
    "                            W,\n",
    "                            X_dev=X_dev, \n",
    "                            Y_dev=Y_dev,\n",
    "                            lr=lr, \n",
    "                            dropout=dr,\n",
    "                            freeze_emb=False,\n",
    "                            tolerance=0.01,\n",
    "                            epochs=50)\n",
    "            preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['pred']) for x,y in zip(X_te,Y_te)]\n",
    "            Accuracy=accuracy_score(Y_te,preds_te)\n",
    "            Precision=precision_score(Y_te,preds_te,average='macro')\n",
    "            Recall=recall_score(Y_te,preds_te,average='macro')\n",
    "            F1_Score=f1_score(Y_te,preds_te,average='macro')\n",
    "            \n",
    "            para.append([lr,dim,dr])\n",
    "            matrix.append([Accuracy, Precision, Recall, F1_Score])\n",
    "\n",
    "            if Accuracy > best_accuracy:\n",
    "                best_accuracy = Accuracy\n",
    "                best_hyperparams = {'learning_rate': lr, 'Emb_size': dim, 'dropout_rate': dr}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0.001, 50, 0.2],\n",
       "  [0.001, 50, 0.4],\n",
       "  [0.001, 200, 0.2],\n",
       "  [0.001, 200, 0.4],\n",
       "  [0.01, 50, 0.2],\n",
       "  [0.01, 50, 0.4],\n",
       "  [0.01, 200, 0.2],\n",
       "  [0.01, 200, 0.4]],\n",
       " [[0.8533333333333334,\n",
       "   0.8542207908391483,\n",
       "   0.8533333333333334,\n",
       "   0.8528418803418804],\n",
       "  [0.8555555555555555,\n",
       "   0.8566716653964975,\n",
       "   0.8555555555555555,\n",
       "   0.8548395798030913],\n",
       "  [0.8477777777777777,\n",
       "   0.8488879105322186,\n",
       "   0.8477777777777776,\n",
       "   0.8472228283072066],\n",
       "  [0.8466666666666667,\n",
       "   0.8486429802446981,\n",
       "   0.8466666666666667,\n",
       "   0.846434888885462],\n",
       "  [0.8488888888888889,\n",
       "   0.8481046841232672,\n",
       "   0.8488888888888888,\n",
       "   0.8480658726361582],\n",
       "  [0.85, 0.8499354879648039, 0.85, 0.8482053599993447],\n",
       "  [0.8644444444444445,\n",
       "   0.8650421030829346,\n",
       "   0.8644444444444445,\n",
       "   0.8642423243828703],\n",
       "  [0.8588888888888889,\n",
       "   0.8585518653376978,\n",
       "   0.8588888888888889,\n",
       "   0.8582687453253878]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para, matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Learning rate, Embedding size and dropout rates hyperparameters significantly affects the accuracy of a neural network. I'm performing a grid search above over the specified hyperparameter values and keeping track of the best performing set of hyperparameters based on the accuracy metric. \n",
    "I have started with a small learning rate i.e. 0.001 and gradually adjusting it based on the training progress and validation performance. Dropout is a technique to drop selected neurons at any time during training.\n",
    "For learning rate : 0.001, Embedding size: 50,drop out rate: 0.4, the accuracy is: 0.8555555555555555 \n",
    "which is the highest for 0.001 lr.\n",
    "When the learning rate is 0.01, Embedding dimension is 200 and dropout rate is 0.2:\n",
    "The Accuracy for this combination is highest amongst the other combinations.\n",
    "For this model, accuracy is good when learning rate is low and embedding dimension is high.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pre-trained Embeddings\n",
    "\n",
    "Now re-train the network using GloVe pre-trained embeddings. You need to modify the `backward_pass` function above to stop computing gradients and updating weights of the embedding matrix.\n",
    "\n",
    "Use the function below to obtain the embedding martix for your vocabulary. Generally, that should work without any problem. If you get errors, you can modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:32.020697Z",
     "start_time": "2020-04-02T14:27:32.015733Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_glove_embeddings(f_zip, f_txt, word_to_id, emb_size=300):\n",
    "    \n",
    "    w_emb = np.zeros((len(word_to_id), emb_size))\n",
    "    \n",
    "    with zipfile.ZipFile(f_zip) as z:\n",
    "        with z.open(f_txt) as f:\n",
    "            for line in f:\n",
    "                line = line.decode('utf-8')\n",
    "                word = line.split()[0]\n",
    "                     \n",
    "                if word in vocab:\n",
    "                    emb = np.array(line.strip('\\n').split()[1:]).astype(np.float32)\n",
    "                    w_emb[word_to_id[word]] +=emb\n",
    "    return w_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:28:54.548613Z",
     "start_time": "2020-04-02T14:27:32.780248Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.073143  ,  0.15172   , -0.17648999, ..., -0.57871997,\n",
       "         0.27922001, -0.078284  ],\n",
       "       [-0.14117   , -0.11568   , -0.57611001, ..., -0.18863   ,\n",
       "         0.27243   ,  0.12526   ],\n",
       "       [ 0.72355998, -0.20265   ,  0.39272001, ...,  0.016114  ,\n",
       "        -0.23998   , -0.14477   ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.57735002,  0.53346997,  0.25926   , ..., -0.19133   ,\n",
       "        -0.14083   ,  0.13174   ],\n",
       "       [-0.13158   ,  0.11049   ,  0.64915001, ..., -0.17715999,\n",
       "         0.26337999, -0.38286   ]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_glove = get_glove_embeddings(\"glove.840B.300d.zip\",\"glove.840B.300d.txt\",word_to_id)\n",
    "w_glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialise the weights of your network using the `network_weights` function. Second, replace the weigths of the embedding matrix with `w_glove`. Finally, train the network by freezing the embedding weights: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:30:11.121198Z",
     "start_time": "2020-04-02T14:29:24.946124Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (2000, 300)\n",
      "Shape W1 (300, 3)\n",
      "Epoch 1: train loss 1.4074, val loss 1.4718\n",
      "Epoch 2: train loss 1.3591, val loss 1.4383\n",
      "Epoch 3: train loss 1.2380, val loss 1.3186\n",
      "Epoch 4: train loss 1.1478, val loss 1.2290\n",
      "Epoch 5: train loss 1.0629, val loss 1.1424\n",
      "Epoch 6: train loss 0.9908, val loss 1.0675\n",
      "Epoch 7: train loss 0.9185, val loss 0.9895\n",
      "Epoch 8: train loss 0.8777, val loss 0.9467\n",
      "Epoch 9: train loss 0.8229, val loss 0.8855\n",
      "Epoch 10: train loss 0.7810, val loss 0.8386\n",
      "Epoch 11: train loss 0.7506, val loss 0.8043\n",
      "Epoch 12: train loss 0.7308, val loss 0.7819\n",
      "Epoch 13: train loss 0.6925, val loss 0.7366\n",
      "Epoch 14: train loss 0.6674, val loss 0.7065\n",
      "Epoch 15: train loss 0.6431, val loss 0.6772\n",
      "Epoch 16: train loss 0.6253, val loss 0.6553\n",
      "Epoch 17: train loss 0.6076, val loss 0.6338\n",
      "Epoch 18: train loss 0.6061, val loss 0.6317\n",
      "Epoch 19: train loss 0.5882, val loss 0.6096\n",
      "Epoch 20: train loss 0.5702, val loss 0.5868\n",
      "Epoch 21: train loss 0.5628, val loss 0.5771\n",
      "Epoch 22: train loss 0.5631, val loss 0.5762\n",
      "Epoch 23: train loss 0.5512, val loss 0.5606\n",
      "Epoch 24: train loss 0.5388, val loss 0.5454\n",
      "Epoch 25: train loss 0.5382, val loss 0.5433\n",
      "Epoch 26: train loss 0.5168, val loss 0.5148\n",
      "Epoch 27: train loss 0.5146, val loss 0.5115\n",
      "Epoch 28: train loss 0.5037, val loss 0.4970\n",
      "Epoch 29: train loss 0.4941, val loss 0.4833\n",
      "Epoch 30: train loss 0.5005, val loss 0.4914\n",
      "Epoch 31: train loss 0.4862, val loss 0.4721\n",
      "Epoch 32: train loss 0.4886, val loss 0.4749\n",
      "Epoch 33: train loss 0.4821, val loss 0.4658\n",
      "Epoch 34: train loss 0.4777, val loss 0.4590\n",
      "Epoch 35: train loss 0.4757, val loss 0.4554\n",
      "Epoch 36: train loss 0.4797, val loss 0.4593\n",
      "Epoch 37: train loss 0.4676, val loss 0.4444\n",
      "Epoch 38: train loss 0.4666, val loss 0.4414\n",
      "Epoch 39: train loss 0.4582, val loss 0.4299\n",
      "Epoch 40: train loss 0.4581, val loss 0.4294\n",
      "Epoch 41: train loss 0.4555, val loss 0.4248\n",
      "Epoch 42: train loss 0.4465, val loss 0.4126\n",
      "Epoch 43: train loss 0.4488, val loss 0.4149\n",
      "Epoch 44: train loss 0.4404, val loss 0.4036\n",
      "Epoch 45: train loss 0.4426, val loss 0.4054\n",
      "Epoch 46: train loss 0.4420, val loss 0.4046\n",
      "Epoch 47: train loss 0.4438, val loss 0.4059\n",
      "Epoch 48: train loss 0.4437, val loss 0.4062\n",
      "Epoch 49: train loss 0.4421, val loss 0.4030\n",
      "Epoch 50: train loss 0.4403, val loss 0.3995\n",
      "Epoch 51: train loss 0.4310, val loss 0.3878\n",
      "Epoch 52: train loss 0.4329, val loss 0.3886\n",
      "Epoch 53: train loss 0.4296, val loss 0.3829\n",
      "Epoch 54: train loss 0.4283, val loss 0.3815\n",
      "Epoch 55: train loss 0.4220, val loss 0.3726\n",
      "Epoch 56: train loss 0.4252, val loss 0.3745\n",
      "Epoch 57: train loss 0.4271, val loss 0.3777\n",
      "Epoch 58: train loss 0.4301, val loss 0.3800\n",
      "Epoch 59: train loss 0.4210, val loss 0.3693\n",
      "Epoch 60: train loss 0.4205, val loss 0.3666\n",
      "Epoch 61: train loss 0.4209, val loss 0.3670\n",
      "Epoch 62: train loss 0.4230, val loss 0.3698\n",
      "Epoch 63: train loss 0.4267, val loss 0.3727\n",
      "Epoch 64: train loss 0.4198, val loss 0.3641\n",
      "Epoch 65: train loss 0.4166, val loss 0.3600\n",
      "Epoch 66: train loss 0.4161, val loss 0.3593\n",
      "Epoch 67: train loss 0.4166, val loss 0.3589\n",
      "Epoch 68: train loss 0.4163, val loss 0.3590\n",
      "Epoch 69: train loss 0.4168, val loss 0.3587\n",
      "Epoch 70: train loss 0.4126, val loss 0.3516\n",
      "Epoch 71: train loss 0.4082, val loss 0.3458\n",
      "Epoch 72: train loss 0.4099, val loss 0.3466\n",
      "Epoch 73: train loss 0.4088, val loss 0.3470\n",
      "Epoch 74: train loss 0.4044, val loss 0.3389\n",
      "Epoch 75: train loss 0.4084, val loss 0.3433\n",
      "Epoch 76: train loss 0.4082, val loss 0.3433\n",
      "Epoch 77: train loss 0.4103, val loss 0.3447\n",
      "Epoch 78: train loss 0.4090, val loss 0.3434\n",
      "Epoch 79: train loss 0.4055, val loss 0.3384\n",
      "Epoch 80: train loss 0.4063, val loss 0.3394\n",
      "Epoch 81: train loss 0.4051, val loss 0.3373\n",
      "Epoch 82: train loss 0.4035, val loss 0.3349\n",
      "Epoch 83: train loss 0.4105, val loss 0.3443\n",
      "Epoch 84: train loss 0.4069, val loss 0.3386\n",
      "Epoch 85: train loss 0.4060, val loss 0.3371\n",
      "Epoch 86: train loss 0.4065, val loss 0.3366\n",
      "Epoch 87: train loss 0.4084, val loss 0.3389\n",
      "Epoch 88: train loss 0.4012, val loss 0.3287\n",
      "Epoch 89: train loss 0.4022, val loss 0.3308\n",
      "Epoch 90: train loss 0.4024, val loss 0.3319\n",
      "Epoch 91: train loss 0.3978, val loss 0.3253\n",
      "Epoch 92: train loss 0.4003, val loss 0.3277\n",
      "Epoch 93: train loss 0.3999, val loss 0.3257\n",
      "Epoch 94: train loss 0.4032, val loss 0.3310\n",
      "Epoch 95: train loss 0.4028, val loss 0.3290\n",
      "Epoch 96: train loss 0.4002, val loss 0.3253\n",
      "Epoch 97: train loss 0.3963, val loss 0.3198\n",
      "Epoch 98: train loss 0.3932, val loss 0.3160\n",
      "Epoch 99: train loss 0.3948, val loss 0.3159\n",
      "Epoch 100: train loss 0.3965, val loss 0.3180\n"
     ]
    }
   ],
   "source": [
    "# Initialise the weights of network \n",
    "W = network_weights(vocab_size=len(vocab),embedding_dim=300,hidden_dim=[], num_classes=3, init_val = 0.1)\n",
    "\n",
    "#Replace the weights of the embedding matrix with w_glove\n",
    "W[0] = w_glove\n",
    "\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "\n",
    "W, loss_tr, dev_loss = SGD(X_tr,Y_tr,W,X_dev=X_dev, Y_dev=Y_dev,lr=0.001,\n",
    "                                       dropout = 0.2,freeze_emb = True,tolerance=0.01,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:12:00.815184Z",
     "start_time": "2020-04-02T15:12:00.812563Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8844444444444445\n",
      "Precision: 0.8856056418245473\n",
      "Recall: 0.8844444444444445\n",
      "F1-Score: 0.8831928424546054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['pred']) \n",
    "            for x,y in zip(X_te,Y_te)]\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 1.0857, val loss 1.0995\n",
      "Epoch 2: train loss 1.0660, val loss 1.0880\n",
      "Epoch 3: train loss 1.0456, val loss 1.0763\n",
      "Epoch 4: train loss 1.0240, val loss 1.0638\n",
      "Epoch 5: train loss 1.0011, val loss 1.0504\n",
      "Epoch 6: train loss 0.9767, val loss 1.0358\n",
      "Epoch 7: train loss 0.9506, val loss 1.0200\n",
      "Epoch 8: train loss 0.9231, val loss 1.0028\n",
      "Epoch 9: train loss 0.8943, val loss 0.9841\n",
      "Epoch 10: train loss 0.8647, val loss 0.9639\n",
      "Epoch 11: train loss 0.8343, val loss 0.9421\n",
      "Epoch 12: train loss 0.8039, val loss 0.9192\n",
      "Epoch 13: train loss 0.7737, val loss 0.8951\n",
      "Epoch 14: train loss 0.7437, val loss 0.8698\n",
      "Epoch 15: train loss 0.7142, val loss 0.8437\n",
      "Epoch 16: train loss 0.6856, val loss 0.8171\n",
      "Epoch 17: train loss 0.6580, val loss 0.7904\n",
      "Epoch 18: train loss 0.6311, val loss 0.7637\n",
      "Epoch 19: train loss 0.6053, val loss 0.7373\n",
      "Epoch 20: train loss 0.5804, val loss 0.7112\n",
      "Epoch 21: train loss 0.5568, val loss 0.6859\n",
      "Epoch 22: train loss 0.5345, val loss 0.6614\n",
      "Epoch 23: train loss 0.5134, val loss 0.6380\n",
      "Epoch 24: train loss 0.4935, val loss 0.6159\n",
      "Epoch 25: train loss 0.4750, val loss 0.5952\n",
      "Epoch 26: train loss 0.4575, val loss 0.5755\n",
      "Epoch 27: train loss 0.4411, val loss 0.5570\n",
      "Epoch 28: train loss 0.4259, val loss 0.5396\n",
      "Epoch 29: train loss 0.4116, val loss 0.5237\n",
      "Epoch 30: train loss 0.3979, val loss 0.5087\n",
      "Epoch 31: train loss 0.3853, val loss 0.4943\n",
      "Epoch 32: train loss 0.3735, val loss 0.4817\n",
      "Epoch 33: train loss 0.3624, val loss 0.4694\n",
      "Epoch 34: train loss 0.3518, val loss 0.4579\n",
      "Epoch 35: train loss 0.3421, val loss 0.4472\n",
      "Epoch 36: train loss 0.3328, val loss 0.4374\n",
      "Epoch 37: train loss 0.3238, val loss 0.4289\n",
      "Epoch 38: train loss 0.3151, val loss 0.4206\n",
      "Epoch 39: train loss 0.3071, val loss 0.4125\n",
      "Epoch 40: train loss 0.2996, val loss 0.4049\n",
      "Epoch 41: train loss 0.2926, val loss 0.3980\n",
      "Epoch 42: train loss 0.2856, val loss 0.3923\n",
      "Epoch 43: train loss 0.2791, val loss 0.3855\n",
      "Epoch 44: train loss 0.2726, val loss 0.3799\n",
      "Epoch 45: train loss 0.2663, val loss 0.3749\n",
      "Epoch 46: train loss 0.2604, val loss 0.3701\n",
      "Epoch 47: train loss 0.2547, val loss 0.3652\n",
      "Epoch 48: train loss 0.2490, val loss 0.3606\n",
      "Epoch 49: train loss 0.2438, val loss 0.3559\n",
      "Epoch 50: train loss 0.2385, val loss 0.3522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 1.0901, val loss 1.1036\n",
      "Epoch 2: train loss 1.0725, val loss 1.0923\n",
      "Epoch 3: train loss 1.0544, val loss 1.0807\n",
      "Epoch 4: train loss 1.0349, val loss 1.0683\n",
      "Epoch 5: train loss 1.0138, val loss 1.0550\n",
      "Epoch 6: train loss 0.9903, val loss 1.0402\n",
      "Epoch 7: train loss 0.9643, val loss 1.0237\n",
      "Epoch 8: train loss 0.9359, val loss 1.0051\n",
      "Epoch 9: train loss 0.9050, val loss 0.9843\n",
      "Epoch 10: train loss 0.8721, val loss 0.9612\n",
      "Epoch 11: train loss 0.8381, val loss 0.9358\n",
      "Epoch 12: train loss 0.8037, val loss 0.9085\n",
      "Epoch 13: train loss 0.7695, val loss 0.8795\n",
      "Epoch 14: train loss 0.7364, val loss 0.8496\n",
      "Epoch 15: train loss 0.7049, val loss 0.8188\n",
      "Epoch 16: train loss 0.6747, val loss 0.7877\n",
      "Epoch 17: train loss 0.6461, val loss 0.7564\n",
      "Epoch 18: train loss 0.6188, val loss 0.7251\n",
      "Epoch 19: train loss 0.5926, val loss 0.6940\n",
      "Epoch 20: train loss 0.5673, val loss 0.6631\n",
      "Epoch 21: train loss 0.5430, val loss 0.6333\n",
      "Epoch 22: train loss 0.5197, val loss 0.6046\n",
      "Epoch 23: train loss 0.4970, val loss 0.5765\n",
      "Epoch 24: train loss 0.4761, val loss 0.5506\n",
      "Epoch 25: train loss 0.4559, val loss 0.5261\n",
      "Epoch 26: train loss 0.4368, val loss 0.5028\n",
      "Epoch 27: train loss 0.4195, val loss 0.4817\n",
      "Epoch 28: train loss 0.4038, val loss 0.4625\n",
      "Epoch 29: train loss 0.3895, val loss 0.4453\n",
      "Epoch 30: train loss 0.3765, val loss 0.4292\n",
      "Epoch 31: train loss 0.3642, val loss 0.4144\n",
      "Epoch 32: train loss 0.3532, val loss 0.4010\n",
      "Epoch 33: train loss 0.3425, val loss 0.3889\n",
      "Epoch 34: train loss 0.3329, val loss 0.3779\n",
      "Epoch 35: train loss 0.3245, val loss 0.3674\n",
      "Epoch 36: train loss 0.3160, val loss 0.3588\n",
      "Epoch 37: train loss 0.3085, val loss 0.3503\n",
      "Epoch 38: train loss 0.3012, val loss 0.3426\n",
      "Epoch 39: train loss 0.2940, val loss 0.3361\n",
      "Epoch 40: train loss 0.2882, val loss 0.3293\n",
      "Epoch 41: train loss 0.2821, val loss 0.3239\n",
      "Epoch 42: train loss 0.2760, val loss 0.3188\n",
      "Epoch 43: train loss 0.2698, val loss 0.3142\n",
      "Epoch 44: train loss 0.2643, val loss 0.3108\n",
      "Epoch 45: train loss 0.2599, val loss 0.3063\n",
      "Epoch 46: train loss 0.2547, val loss 0.3031\n",
      "Epoch 47: train loss 0.2503, val loss 0.3002\n",
      "Epoch 48: train loss 0.2452, val loss 0.2967\n",
      "Epoch 49: train loss 0.2403, val loss 0.2948\n",
      "Epoch 50: train loss 0.2359, val loss 0.2934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 1.0432, val loss 1.0767\n",
      "Epoch 2: train loss 0.9859, val loss 1.0378\n",
      "Epoch 3: train loss 0.9329, val loss 1.0011\n",
      "Epoch 4: train loss 0.8833, val loss 0.9656\n",
      "Epoch 5: train loss 0.8366, val loss 0.9316\n",
      "Epoch 6: train loss 0.7924, val loss 0.8984\n",
      "Epoch 7: train loss 0.7507, val loss 0.8662\n",
      "Epoch 8: train loss 0.7116, val loss 0.8348\n",
      "Epoch 9: train loss 0.6748, val loss 0.8044\n",
      "Epoch 10: train loss 0.6402, val loss 0.7748\n",
      "Epoch 11: train loss 0.6079, val loss 0.7464\n",
      "Epoch 12: train loss 0.5779, val loss 0.7193\n",
      "Epoch 13: train loss 0.5500, val loss 0.6934\n",
      "Epoch 14: train loss 0.5241, val loss 0.6688\n",
      "Epoch 15: train loss 0.5003, val loss 0.6457\n",
      "Epoch 16: train loss 0.4784, val loss 0.6240\n",
      "Epoch 17: train loss 0.4583, val loss 0.6039\n",
      "Epoch 18: train loss 0.4396, val loss 0.5849\n",
      "Epoch 19: train loss 0.4221, val loss 0.5672\n",
      "Epoch 20: train loss 0.4060, val loss 0.5508\n",
      "Epoch 21: train loss 0.3910, val loss 0.5355\n",
      "Epoch 22: train loss 0.3772, val loss 0.5213\n",
      "Epoch 23: train loss 0.3643, val loss 0.5081\n",
      "Epoch 24: train loss 0.3521, val loss 0.4955\n",
      "Epoch 25: train loss 0.3409, val loss 0.4840\n",
      "Epoch 26: train loss 0.3303, val loss 0.4732\n",
      "Epoch 27: train loss 0.3202, val loss 0.4631\n",
      "Epoch 28: train loss 0.3108, val loss 0.4541\n",
      "Epoch 29: train loss 0.3018, val loss 0.4455\n",
      "Epoch 30: train loss 0.2932, val loss 0.4376\n",
      "Epoch 31: train loss 0.2851, val loss 0.4300\n",
      "Epoch 32: train loss 0.2775, val loss 0.4228\n",
      "Epoch 33: train loss 0.2701, val loss 0.4160\n",
      "Epoch 34: train loss 0.2632, val loss 0.4100\n",
      "Epoch 35: train loss 0.2565, val loss 0.4042\n",
      "Epoch 36: train loss 0.2499, val loss 0.3987\n",
      "Epoch 37: train loss 0.2438, val loss 0.3936\n",
      "Epoch 38: train loss 0.2377, val loss 0.3888\n",
      "Epoch 39: train loss 0.2320, val loss 0.3843\n",
      "Epoch 40: train loss 0.2264, val loss 0.3800\n",
      "Epoch 41: train loss 0.2211, val loss 0.3757\n",
      "Epoch 42: train loss 0.2158, val loss 0.3720\n",
      "Epoch 43: train loss 0.2109, val loss 0.3684\n",
      "Epoch 44: train loss 0.2060, val loss 0.3651\n",
      "Epoch 45: train loss 0.2013, val loss 0.3616\n",
      "Epoch 46: train loss 0.1969, val loss 0.3584\n",
      "Epoch 47: train loss 0.1926, val loss 0.3555\n",
      "Epoch 48: train loss 0.1882, val loss 0.3529\n",
      "Epoch 49: train loss 0.1842, val loss 0.3503\n",
      "Epoch 50: train loss 0.1801, val loss 0.3480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 1.0334, val loss 1.0649\n",
      "Epoch 2: train loss 0.9701, val loss 1.0237\n",
      "Epoch 3: train loss 0.9122, val loss 0.9843\n",
      "Epoch 4: train loss 0.8575, val loss 0.9458\n",
      "Epoch 5: train loss 0.8060, val loss 0.9078\n",
      "Epoch 6: train loss 0.7577, val loss 0.8703\n",
      "Epoch 7: train loss 0.7122, val loss 0.8329\n",
      "Epoch 8: train loss 0.6698, val loss 0.7960\n",
      "Epoch 9: train loss 0.6306, val loss 0.7597\n",
      "Epoch 10: train loss 0.5944, val loss 0.7245\n",
      "Epoch 11: train loss 0.5612, val loss 0.6906\n",
      "Epoch 12: train loss 0.5305, val loss 0.6577\n",
      "Epoch 13: train loss 0.5027, val loss 0.6266\n",
      "Epoch 14: train loss 0.4775, val loss 0.5974\n",
      "Epoch 15: train loss 0.4545, val loss 0.5701\n",
      "Epoch 16: train loss 0.4335, val loss 0.5444\n",
      "Epoch 17: train loss 0.4146, val loss 0.5211\n",
      "Epoch 18: train loss 0.3971, val loss 0.4992\n",
      "Epoch 19: train loss 0.3814, val loss 0.4795\n",
      "Epoch 20: train loss 0.3672, val loss 0.4617\n",
      "Epoch 21: train loss 0.3544, val loss 0.4453\n",
      "Epoch 22: train loss 0.3423, val loss 0.4299\n",
      "Epoch 23: train loss 0.3310, val loss 0.4153\n",
      "Epoch 24: train loss 0.3210, val loss 0.4028\n",
      "Epoch 25: train loss 0.3112, val loss 0.3908\n",
      "Epoch 26: train loss 0.3022, val loss 0.3802\n",
      "Epoch 27: train loss 0.2938, val loss 0.3697\n",
      "Epoch 28: train loss 0.2859, val loss 0.3605\n",
      "Epoch 29: train loss 0.2783, val loss 0.3520\n",
      "Epoch 30: train loss 0.2710, val loss 0.3446\n",
      "Epoch 31: train loss 0.2642, val loss 0.3375\n",
      "Epoch 32: train loss 0.2578, val loss 0.3305\n",
      "Epoch 33: train loss 0.2513, val loss 0.3245\n",
      "Epoch 34: train loss 0.2452, val loss 0.3198\n",
      "Epoch 35: train loss 0.2395, val loss 0.3141\n",
      "Epoch 36: train loss 0.2333, val loss 0.3102\n",
      "Epoch 37: train loss 0.2276, val loss 0.3060\n",
      "Epoch 38: train loss 0.2225, val loss 0.3016\n",
      "Epoch 39: train loss 0.2170, val loss 0.2987\n",
      "Epoch 40: train loss 0.2119, val loss 0.2952\n",
      "Epoch 41: train loss 0.2071, val loss 0.2920\n",
      "Epoch 42: train loss 0.2019, val loss 0.2897\n",
      "Epoch 43: train loss 0.1971, val loss 0.2872\n",
      "Epoch 44: train loss 0.1926, val loss 0.2843\n",
      "Epoch 45: train loss 0.1881, val loss 0.2829\n",
      "Epoch 46: train loss 0.1839, val loss 0.2803\n",
      "Epoch 47: train loss 0.1795, val loss 0.2787\n",
      "Epoch 48: train loss 0.1756, val loss 0.2760\n",
      "Epoch 49: train loss 0.1713, val loss 0.2746\n",
      "Epoch 50: train loss 0.1676, val loss 0.2738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.8959, val loss 0.9544\n",
      "Epoch 2: train loss 0.6711, val loss 0.7457\n",
      "Epoch 3: train loss 0.5135, val loss 0.5499\n",
      "Epoch 4: train loss 0.4021, val loss 0.4225\n",
      "Epoch 5: train loss 0.3277, val loss 0.3531\n",
      "Epoch 6: train loss 0.2756, val loss 0.3163\n",
      "Epoch 7: train loss 0.2356, val loss 0.2985\n",
      "Epoch 8: train loss 0.2019, val loss 0.2795\n",
      "Epoch 9: train loss 0.1770, val loss 0.2775\n",
      "Epoch 10: train loss 0.1592, val loss 0.2870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.8989, val loss 0.9564\n",
      "Epoch 2: train loss 0.7134, val loss 0.7874\n",
      "Epoch 3: train loss 0.5786, val loss 0.5774\n",
      "Epoch 4: train loss 0.4836, val loss 0.4421\n",
      "Epoch 5: train loss 0.3980, val loss 0.3439\n",
      "Epoch 6: train loss 0.3471, val loss 0.3088\n",
      "Epoch 7: train loss 0.3052, val loss 0.3020\n",
      "Epoch 8: train loss 0.2681, val loss 0.2885\n",
      "Epoch 9: train loss 0.2415, val loss 0.2941\n",
      "Epoch 10: train loss 0.2264, val loss 0.2956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.8782, val loss 0.9514\n",
      "Epoch 2: train loss 0.6429, val loss 0.6999\n",
      "Epoch 3: train loss 0.4797, val loss 0.5187\n",
      "Epoch 4: train loss 0.3718, val loss 0.4113\n",
      "Epoch 5: train loss 0.2996, val loss 0.3485\n",
      "Epoch 6: train loss 0.2450, val loss 0.3100\n",
      "Epoch 7: train loss 0.2065, val loss 0.2900\n",
      "Epoch 8: train loss 0.1714, val loss 0.2736\n",
      "Epoch 9: train loss 0.1470, val loss 0.2653\n",
      "Epoch 10: train loss 0.1256, val loss 0.2601\n",
      "Epoch 11: train loss 0.1045, val loss 0.2562\n",
      "Epoch 12: train loss 0.0896, val loss 0.2588\n",
      "Epoch 13: train loss 0.0766, val loss 0.2587\n",
      "Epoch 14: train loss 0.0683, val loss 0.2595\n",
      "Epoch 15: train loss 0.0597, val loss 0.2633\n",
      "Epoch 16: train loss 0.0506, val loss 0.2637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.8495, val loss 0.9374\n",
      "Epoch 2: train loss 0.6609, val loss 0.7097\n",
      "Epoch 3: train loss 0.4953, val loss 0.5103\n",
      "Epoch 4: train loss 0.3873, val loss 0.4000\n",
      "Epoch 5: train loss 0.3139, val loss 0.3453\n",
      "Epoch 6: train loss 0.2636, val loss 0.3168\n",
      "Epoch 7: train loss 0.2170, val loss 0.2945\n",
      "Epoch 8: train loss 0.1841, val loss 0.2854\n",
      "Epoch 9: train loss 0.1548, val loss 0.2803\n",
      "Epoch 10: train loss 0.1355, val loss 0.2821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abolivi/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.001, 0.01]\n",
    "Emb_size = [50, 200]\n",
    "dropout_rates = [0.2, 0.4]\n",
    "best_accuracy = 0.0\n",
    "best_hyperparams = {}\n",
    "para=[]\n",
    "matrix=[]\n",
    "\n",
    "for lr in learning_rate:\n",
    "    for dim in Emb_size:\n",
    "        for dr in dropout_rates:\n",
    "            W = network_weights(vocab_size=len(vocab), \n",
    "                    embedding_dim=dim, \n",
    "                    hidden_dim=[], \n",
    "                    num_classes=3, \n",
    "                    init_val = 0.5)\n",
    "            W, loss_tr, dev_loss = SGD(X_tr, Y_tr,\n",
    "                            W,\n",
    "                            X_dev=X_dev, \n",
    "                            Y_dev=Y_dev,\n",
    "                            lr=lr, \n",
    "                            dropout=dr,\n",
    "                            freeze_emb=False,\n",
    "                            tolerance=0.01,\n",
    "                            epochs=50)\n",
    "            preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['pred']) for x,y in zip(X_te,Y_te)]\n",
    "            Accuracy=accuracy_score(Y_te,preds_te)\n",
    "            Precision=precision_score(Y_te,preds_te,average='macro')\n",
    "            Recall=recall_score(Y_te,preds_te,average='macro')\n",
    "            F1_Score=f1_score(Y_te,preds_te,average='macro')\n",
    "            \n",
    "            para.append([lr,dim,dr])\n",
    "            matrix.append([Accuracy, Precision, Recall, F1_Score])\n",
    "\n",
    "            if Accuracy > best_accuracy:\n",
    "                best_accuracy = Accuracy\n",
    "                best_hyperparams = {'learning_rate': lr, 'Emb_size': dim, 'dropout_rate': dr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0.001, 50, 0.2],\n",
       "  [0.001, 50, 0.4],\n",
       "  [0.001, 200, 0.2],\n",
       "  [0.001, 200, 0.4],\n",
       "  [0.01, 50, 0.2],\n",
       "  [0.01, 50, 0.4],\n",
       "  [0.01, 200, 0.2],\n",
       "  [0.01, 200, 0.4]],\n",
       " [[0.8522222222222222,\n",
       "   0.8537529050571,\n",
       "   0.8522222222222222,\n",
       "   0.8515323721262419],\n",
       "  [0.8466666666666667,\n",
       "   0.8472423295359075,\n",
       "   0.8466666666666667,\n",
       "   0.8457826264852994],\n",
       "  [0.8511111111111112,\n",
       "   0.8523271627491577,\n",
       "   0.851111111111111,\n",
       "   0.8506419031712719],\n",
       "  [0.8455555555555555,\n",
       "   0.8460720798246153,\n",
       "   0.8455555555555555,\n",
       "   0.8449821468766249],\n",
       "  [0.8544444444444445,\n",
       "   0.8536851092896175,\n",
       "   0.8544444444444445,\n",
       "   0.8532255359923345],\n",
       "  [0.8466666666666667,\n",
       "   0.8476592696982502,\n",
       "   0.8466666666666667,\n",
       "   0.8450911260511101],\n",
       "  [0.8577777777777778,\n",
       "   0.8577608483833213,\n",
       "   0.8577777777777778,\n",
       "   0.8575287961098113],\n",
       "  [0.8566666666666667,\n",
       "   0.8559066653961599,\n",
       "   0.8566666666666668,\n",
       "   0.8555783285910454]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para, matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm trying for different combinations of lr, dr, and emb dim. When lr is 0.01, emb dim is 200 and dr is 0.2, the model performs well. Accuracy is much better compared to the other combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend to support deeper architectures \n",
    "\n",
    "Extend the network to support back-propagation for more hidden layers. You need to modify the `backward_pass` function above to compute gradients and update the weights between intermediate hidden layers. Finally, train and evaluate a network with a deeper architecture. Do deeper architectures increase performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:58:51.764619Z",
     "start_time": "2020-04-02T14:58:47.483690Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 1.2506, val loss 1.2728\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=len(vocab),embedding_dim=300,hidden_dim=[300],num_classes=3, init_val = 0.1)\n",
    "W[0] = w_glove\n",
    "    \n",
    "W, loss_tr, dev_loss = SGD(X_tr,Y_tr,W,X_dev=X_dev, Y_dev=Y_dev,lr= 0.001,dropout=0.2,freeze_emb=False,print_progress=True,\n",
    "tolerance=0.001,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:11:51.994986Z",
     "start_time": "2020-04-02T15:11:51.992563Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['pred']) \n",
    "            for x,y in zip(X_te,Y_te)]\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Results\n",
    "\n",
    "Add your final results here:\n",
    "\n",
    "| Model | Precision  | Recall  | F1-Score  | Accuracy\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "| Average Embedding  |0.8558498866533634|0.8533333333333334|0.8532238631062531|0.8533333333333334|\n",
    "| Average Embedding (Pre-trained)  |0.8856056418245473|0.8844444444444445|0.8831928424546054|0.8844444444444445|\n",
    "| Average Embedding (Pre-trained) + X hidden layers    |   |   |   |   |\n",
    "\n",
    "\n",
    "Please discuss why your best performing model is better than the rest and provide a bried error analaysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Average Embedding (Pre-trained) model is performing better in my case. To train and evaluate a network with a deeper \n",
    "architecture, I have already added a loop in a backward_pass function for the hidden layer. As we increase the number \n",
    "of hidden layers, the network has the potential to learn more complex representations of the data. \n",
    "Deeper architectures can capture intricate patterns and hierarchies in the data, allowing the model to achieve \n",
    "higher performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
